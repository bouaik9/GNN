,id,title,authorships,abstract,topic,subfield,Unnamed: 0
0,W1959608418,Auto-Encoding Variational Bayes,"Diederik P. Kingma, Max Welling","Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1,W177847060,Efficient Learning of Deep Boltzmann Machines,"Ruslan Salakhutdinov, Hugo Larochelle","We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM’s), a generative model with many layers of hidden variables. The algorithm learns a separate “recognition” model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM’s practical. Finally, we demonstrate that the DBM’s trained using the proposed approximate inference algorithm perform well compared to DBN’s and SVM’s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3,W2080829915,Sample-based non-uniform random variate generation,Luc Devroye,"A sample of n lid random variables with a given unknown density is given. We discuss several issues related to the problem or generating a new sample of lid random variables with almost the same density. In particular, we look at sample independence, consistency, sample indistinguishability, moment matching and generator efficiency. We also introduce the notion of a replacement number, the minimum number of observations in a given sample that have to be replaced to obtain a sample with a given density.",Bayesian Methods and Mixture Models,Artificial Intelligence,
4,W2097268041,Deep AutoRegressive Networks,"Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, Daan Wierstra","We introduce a deep, generative autoencoder capable of learning hierarchies of distributed representations from data. Successive deep stochastic hidden layers are equipped with autoregressive connections, which enable the model to be sampled from quickly and exactly via ancestral sampling. We derive an efficient approximate parameter estimation method based on the minimum description length (MDL) principle, which can be seen as maximising a variational lower bound on the log-likelihood, with a feedforward neural network implementing approximate inference. We demonstrate state-of-the-art generative performance on a number of classic data sets, including several UCI data sets, MNIST and Atari 2600 games.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
5,W2103633133,EM Algorithms for PCA and SPCA,Sam T. Roweis,I present an expectation-maximization (EM) algorithm for principal component analysis (PCA). The algorithm allows a few eigenvectors and eigenvalues to be extracted from large collections of high dimensional data. It is computationally very efficient in space and time. It also naturally accommodates missing information. I also introduce a new variant of PCA called sensible principal component analysis (SPCA) which defines a proper density model in the data space. Learning for SPCA is also done with an EM algorithm. I report results on synthetic and real data showing that these EM algorithms correctly and efficiently find the leading eigenvectors of the covariance of datasets in a few iterations using up to hundreds of thousands of datapoints in thousands of dimensions.,Gaussian Processes and Bayesian Inference,Artificial Intelligence,
6,W2119196781,Variational Bayesian Inference with Stochastic Search,"David M. Blei, Michael I. Jordan, John Paisley","Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.",Bayesian Methods and Mixture Models,Artificial Intelligence,
7,W2135346645,An Application of the Principle of Maximum Information Preservation to Linear Systems,Ralph Linsker,"This paper addresses the problem of determining the weights for a set of linear filters (model cells) so as to maximize the ensemble-averaged information that the cells' output values jointly convey about their input values, given the statistical properties of the ensemble of input vectors. The quantity that is maximized is the Shannon information rate, or equivalently the average mutual information between input and output. Several models for the role of processing noise are analyzed, and the biological motivation for considering them is described. For simple models in which nearby input signal values (in space or time) are correlated, the cells resulting from this optimization process include center-surround cells and cells sensitive to temporal variations in input signal.",Neural dynamics and brain function,Cognitive Neuroscience,
8,W2145094598,Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion,"Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol","We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,
9,W2163922914,Representation Learning: A Review and New Perspectives,"Yoshua Bengio, Aaron Courville, P. M. Durai Raj Vincent","The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",Face and Expression Recognition,Computer Vision and Pattern Recognition,
10,W2166851633,Stochastic variational inference,"Matthew D. Hoffman, David M. Blei, Chong Wang, John Paisley","We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.",Bayesian Methods and Mixture Models,Artificial Intelligence,
11,W2171490498,Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition,"Koray Kavukcuoglu, Marc’Aurelio Ranzato, Yann LeCun","Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.",Sparse and Compressive Sensing Techniques,Computational Mechanics,
12,W2405601855,Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.,"John C. Duchi, Elad Hazan, Yoram Singer","We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.",Stochastic Gradient Optimization Techniques,Artificial Intelligence,
13,W2951493172,Variational Bayesian Inference with Stochastic Search,"John Paisley, David M. Blei, Michael I. Jordan","Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.",Bayesian Methods and Mixture Models,Artificial Intelligence,
14,W2963173382,Black Box Variational Inference,"Rajesh Ranganath, Sean Gerrish, David M. Blei","Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significant model-specific analysis, and these efforts can hinder and deter us from quickly developing and exploring a variety of models for a problem at hand. In this paper, we present a ""black box"" variational inference algorithm, one that can be quickly applied to many models with little additional derivation. Our method is based on a stochastic optimization of the variational objective where the noisy gradient is computed from Monte Carlo samples from the variational distribution. We develop a number of methods to reduce the variance of the gradient, always maintaining the criterion that we want to avoid difficult model-based derivations. We evaluate our method against the corresponding black box sampling based methods. We find that our method reaches better predictive likelihoods much faster than sampling methods. Finally, we demonstrate that Black Box Variational Inference lets us easily explore a wide space of models by quickly constructing and evaluating several models of longitudinal healthcare data.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
15,W3104819538,Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression,"Tim Salimans, David A. Knowles","We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
16,W2099471712,GAN（Generative Adversarial Nets）,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio","We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
17,W2962793481,Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks,"Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros","Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
19,W2962770929,A Style-Based Generator Architecture for Generative Adversarial Networks,"Tero Karras, Samuli Laine, Timo Aila","We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
20,W1514535095,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio","Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
21,W4312933868,High-Resolution Image Synthesis with Latent Diffusion Models,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer","By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
23,W2963420272,Context Encoders: Feature Learning by Inpainting,"Deepak Pathak, Philipp Krähenbühl, Jeff Donahue, Trevor Darrell, Alexei A. Efros","We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders - a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
24,W2593414223,Least Squares Generative Adversarial Networks,"Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley","Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
27,W151672344,Deep Learning using Robust Interdependent Codes,"Hugo Larochelle, Dumitru Erhan, Pascal Vincent","We investigate a simple yet effective method to introduce inhibitory and excitatory interactions between units in the layers of a deep neural network classifier. The method is based on the greedy layer-wise procedure of deep learning algorithms and extends the denoising autoencoder (Vincent et al., 2008) by adding asymmetric lateral connections between its hidden coding units, in a manner that is much simpler and computationally more efficient than previously proposed approaches. We present experiments on two character recognition problems which show for the first time that lateral connections can significantly improve the classification performance of deep networks.",Neural Networks and Applications,Artificial Intelligence,
28,W189596042,Deep Boltzmann machines,"Ruslan Salakhutdinov, Geoffrey E. Hinton","We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
29,W1993845689,"The ""Wake-Sleep"" Algorithm for Unsupervised Neural Networks","Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, Radford M. Neal","An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up ""recognition"" connections convert the input into representations in successive hidden layers, and top-down ""generative"" connections reconstruct the representation in one layer from the representation in the layer above. In the ""wake"" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the ""sleep"" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.",Neural dynamics and brain function,Cognitive Neuroscience,
31,W2072128103,Learning Deep Architectures for AI,Yoshua Bengio,"Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional formulae re-using many sub-formulae. Each level of the architecture represents features at a different level of abstraction, defined as a composition of lower-level features. Searching the parameter space of deep architectures is a difficult task, but new algorithms have been discovered and a new sub-area has emerged in the machine learning community since 2006, following these discoveries. Learning algorithms such as those for Deep Belief Networks and other related unsupervised learning algorithms have recently been proposed to train deep architectures, yielding exciting results and beating the state-of-the-art in certain areas. Learning Deep Architectures for AI discusses the motivations for and principles of learning algorithms for deep architectures. By analyzing and comparing recent results with different learning algorithms for deep architectures, explanations for their success are proposed and discussed, highlighting challenges and suggesting avenues for future explorations in this area.",Machine Learning and Data Classification,Artificial Intelligence,
32,W2096192494,On the quantitative analysis of deep belief networks,"Ruslan Salakhutdinov, Iain Murray","Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test data. This is, to our knowledge, the first step towards obtaining quantitative results that would allow us to directly assess the performance of Deep Belief Networks as generative models of data.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
33,W2122565151,Learning and Evaluating Boltzmann Machines,Ruslan Salakhutdinov,"We provide a brief overview of the variational framework for obtaining deterministic approximations or upper bounds for the log-partition function. We also review some of the Monte Carlo based methods for estimating partition functions of arbitrary Markov Random Fields. We then develop an annealed importance sampling (AIS) procedure for estimating partition functions of rest ricted Boltzmann machines (RBM’s), semi-restricted Boltzmann machines (SRBM’s), an d Boltzmann machines (BM’s). Our empirical results indicate that the AIS procedu re provides much better estimates of the partition function than some of the popular variational-based methods. Finally, we develop a new learning algorithm for training general Boltzmann machines and show that it can be successfully applied to learning good generative models.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
34,W2130325614,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,"Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng","There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
35,W2134557905,Learning methods for generic object recognition with invariance to pose and lighting,"Yann LeCun, Fu Jie Huang, Léon Bottou","We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 azimuths, 9 elevations, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest neighbor methods, support vector machines, and convolutional networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13% for SVM and 7% for convolutional nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while convolutional nets yielded 16/7% error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,
36,W2136922672,A Fast Learning Algorithm for Deep Belief Nets,"Geoffrey E. Hinton, Simon Osindero, Yee‐Whye Teh","We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,
38,W2154956324,Small codes and large image databases for recognition,"Antonio Torralba, Rob Fergus, Yair Weiss","The Internet contains billions of images, freely available online. Methods for efficiently searching this incredibly rich resource are vital for a large number of applications. These include object recognition, computer graphics, personal photo collections, online image search tools. In this paper, our goal is to develop efficient image search and scene matching techniques that are not only fast, but also require very little memory, enabling their use on standard hardware or even on handheld devices. Our approach uses recently developed machine learning techniques to convert the Gist descriptor (a real valued vector that describes orientation energies at different scales and orientations within an image) to a compact binary code, with a few hundred bits per image. Using our scheme, it is possible to perform real-time searches with millions from the Internet using a single large PC and obtain recognition results comparable to the full descriptor. Using our codes on high quality labeled images from the LabelMe database gives surprisingly powerful recognition results using simple nearest neighbor techniques.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
39,W2157629899,OPTIMAL PERCEPTUAL INFERENCE,"Geoffrey E. Hinton, J. Sejnowski","When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with realnumbers, we usc a more dircct encoding in which thc probability  associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular nondeterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences.",Neural Networks and Applications,Artificial Intelligence,
41,W2161893161,3D Object Recognition with Deep Belief Nets,"Vinod Nair, Geoffrey E. Hinton","We introduce a new type of top-level model for Deep Belief Nets and evaluate it on a 3D object recognition task. The top-level model is a third-order Boltzmann machine, trained using a hybrid algorithm that combines both generative and discriminative gradients. Performance is evaluated on the NORB database (normalized-uniform version), which contains stereo-pair images of objects under different lighting conditions and viewpoints. Our model achieves 6.5% error on the test set, which is close to the best published result for NORB (5.9%) using a convolutional neural net that has built-in knowledge of translation invariance. It substantially outperforms shallow models such as SVMs (11.6%). DBNs are especially suited for semi-supervised learning, and to demonstrate this we consider a modified version of the NORB recognition task in which additional unlabeled images are created by applying small translations to the images in the database. With the extra unlabeled data (and the same amount of labeled data as before), our model achieves 5.2% error.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
42,W2172174689,Efficient Learning of Sparse Representations with an Energy-Based Model,"Marc’Aurelio Ranzato, Christopher S. Poultney, Sumit Chopra, Yann LeCun","We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces stroke detectors when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.",Music and Audio Processing,Signal Processing,
43,W3140854437,"Review of deep learning: concepts, CNN architectures, challenges, applications, future directions","Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Q. Al-Dujaili, Ye Duan, Omran Al-Shamma, José Santamaría, Mohammed A. Fadhel, Muthana Al‐Amidie, Laith Farhan","In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational approach in the field of ML, thus achieving outstanding results on several complex cognitive tasks, matching or even beating those provided by human performance. One of the benefits of DL is the ability to learn massive amounts of data. The DL field has grown fast in the last few years and it has been extensively used to successfully address a wide range of traditional applications. More importantly, DL has outperformed well-known ML techniques in many domains, e.g., cybersecurity, natural language processing, bioinformatics, robotics and control, and medical information processing, among many others. Despite it has been contributed several works reviewing the State-of-the-Art on DL, all of them only tackled one aspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this contribution, we propose using a more holistic approach in order to provide a more suitable starting point from which to develop a full understanding of DL. Specifically, this review attempts to provide a more comprehensive survey of the most important aspects of DL and including those enhancements recently added to the field. In particular, this paper outlines the importance of DL, presents the types of DL techniques and networks. It then presents convolutional neural networks (CNNs) which the most utilized DL network type and describes the development of CNNs architectures together with their main features, e.g., starting with the AlexNet network and closing with the High-Resolution network (HR.Net). Finally, we further present the challenges and suggested solutions to help researchers understand the existing research gaps. It is followed by a list of the major DL applications. Computational tools including FPGA, GPU, and CPU are summarized along with a description of their influence on DL. The paper ends with the evolution matrix, benchmark datasets, and summary and conclusion.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
44,W2951004968,Auto-Encoding Variational Bayes,"Diederik P. Kingma, Max Welling","How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
47,W2948978827,An Introduction to Variational Autoencoders,"Diederik P. Kingma, Max Welling","Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models.In this work, we provide an introduction to variational autoencoders and some important extensions.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
48,W2502949459,Deep learning for computational biology,"Christof Angermueller, Tanel Pärnamaa, Leopold Parts, Oliver Stegle","Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.",Cell Image Analysis Techniques,Biophysics,
49,W2910068345,Deep Learning for Anomaly Detection: A Survey,"Raghavendra Chalapathy, Sanjay Chawla","Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.",Anomaly Detection Techniques and Applications,Artificial Intelligence,
50,W4213113494,Efficient Learning Machines,"Mariette Awad, Rahul Khanna",Machine learning techniques provide cost-effective alternatives to traditional methods for extracting underlying relationships between information and data and for predicting future events by processi,Neural Networks and Applications,Artificial Intelligence,
52,W2025768430,Extracting and composing robust features with denoising autoencoders,"Pascal Vincent, Hugo Larochelle, Yoshua Bengio, Pierre-Antoine Manzagol","Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
53,W2100495367,Reducing the Dimensionality of Data with Neural Networks,"Geoffrey E. Hinton, Ruslan Salakhutdinov","High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.",Neural Networks and Applications,Artificial Intelligence,
54,W2108665656,Sparse Feature Learning for Deep Belief Networks,"Marc’Aurelio Ranzato, Y-Lan Boureau, Y. Le Cun","Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
59,W2139427956,Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition,"Marc’Aurelio Ranzato, Fu Jie Huang, Y-Lan Boureau, Yann LeCun","We present an unsupervised method for learning a hierarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extractor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each filter output within adjacent windows, and a point-wise sigmoid non-linearity. A second level of larger and more invariant features is obtained by training the same algorithm on patches of features from the first level. Training a supervised classifier on these features yields 0.64% error on MNIST, and 54% average recognition rate on Caltech 101 with 30 training samples per category. While the resulting architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely supervised learning procedures, and yields good performance with very few labeled training samples.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
78,W1994616650,A Stochastic Approximation Method,"Herbert Robbins, Sutton Monro","Let $M(x)$ denote the expected value at level $x$ of the response to a certain experiment. $M(x)$ is assumed to be a monotone function of $x$ but is unknown to the experimenter, and it is desired to find the solution $x = \theta$ of the equation $M(x) = \alpha$, where $\alpha$ is a given constant. We give a method for making successive experiments at levels $x_1,x_2,\cdots$ in such a way that $x_n$ will tend to $\theta$ in probability.",Optimal Experimental Design Methods,Management Science and Operations Research,
80,W2102409316,"Autoencoders, Minimum Description Length and Helmholtz Free Energy","Geoffrey E. Hinton, Richard S. Zemel","An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights define the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and that this approximation gives an upper bound on the description length. Even when this bound is poor, it can be used as a Lyapunov function for learning both the generative and the recognition weights. We demonstrate that this approach can be used to learn factorial codes.",Neural Networks and Applications,Artificial Intelligence,
82,W2116064496,Training Products of Experts by Minimizing Contrastive Divergence,Geoffrey E. Hinton,"It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual ""expert"" models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called ""contrastive divergence"" whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.",Neural Networks and Applications,Artificial Intelligence,
106,W2153791616,"Whatever next? Predictive brains, situated agents, and the future of cognitive science",Andy Clark,"Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this ""hierarchical prediction machine"" approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.",Embodied and Extended Cognition,Cognitive Neuroscience,
125,W1520448186,Mean Field Theory for Sigmoid Belief Networks,"L.K. Saul, Tommi Jaakkola, Michael I. Jordan",We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics. Our mean field theory provides a tractable approximation to the true probability distribution in these networks; it also yields a lower bound on the likelihood of evidence. We demonstrate the utility of this framework on a benchmark problem in statistical pattern recognition---the classification of handwritten digits.,Bayesian Modeling and Causal Inference,Artificial Intelligence,
128,W2250539671,Glove: Global Vectors for Word Representation,"Jeffrey Pennington, Richard Socher, Christopher D. Manning","Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",Topic Modeling,Artificial Intelligence,
135,W2064630666,Representational Power of Restricted Boltzmann Machines and Deep Belief Networks,"Nicolas Le Roux, Yoshua Bengio","Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
145,W2953318193,Pixel Recurrent Neural Networks,"Aäron van den Oord, Nal Kalchbrenner, Koray Kavukcuoglu","Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
146,W1583912456,NICE: Non-linear Independent Components Estimation,"Laurent Dinh, David Krueger, Yoshua Bengio","We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
148,W1850742715,DRAW: A Recurrent Neural Network For Image Generation,"Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra","This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
177,W2155904486,Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories,"Li Fei-Fei, Rob Fergus, Pietro Perona","Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum-likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
180,W2752782242,Squeeze-and-Excitation Networks,"Jie Hu, Li Shen, Gang Sun","Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the ""Squeeze-and-Excitation"" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
181,W2963420686,Squeeze-and-Excitation Networks,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu","The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the ""Squeeze-and-Excitation"" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of -25 percent. Models and code are available at https://github.com/hujie-frank/SENet.",Force Microscopy Techniques and Applications,"Atomic and Molecular Physics, and Optics",
182,W2173520492,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,"Alec Radford, Luke Metz, Soumith Chintala","In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
193,W2112796928,Gradient-based learning applied to document recognition,"Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner","Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.",Handwritten Text Recognition Techniques,Computer Vision and Pattern Recognition,
264,W2020999234,"Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images","Stuart Geman, Donald Geman","We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,
287,W2105464873,Sparse coding with an overcomplete basis set: A strategy employed by V1?,"Bruno A. Olshausen, David J. Field","The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.",Neural dynamics and brain function,Cognitive Neuroscience,
297,W1489081407,Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields,"Karol Gregor, Yann LeCun","We introduce a new neural architecture and an unsupervised algorithm for learning invariant representations from temporal sequence of images. The system uses two groups of complex cells whose outputs are combined multiplicatively: one that represents the content of the image, constrained to be constant over several consecutive frames, and one that represents the precise location of features, which is allowed to vary over time but constrained to be sparse. The architecture uses an encoder to extract features, and a decoder to reconstruct the input from the features. The method was applied to patches extracted from consecutive movie frames and produces orientation and frequency selective units analogous to the complex cells in V1. An extension of the method is proposed to train a network composed of units with local receptive field spread over a large image of arbitrary size. A layer of complex cells, subject to sparsity constraints, pool feature units over overlapping local neighborhoods, which causes the feature units to organize themselves into pinwheel patterns of orientation-selective receptive fields, similar to those observed in the mammalian visual cortex. A feed-forward encoder efficiently computes the feature representation of full images.",Neural dynamics and brain function,Cognitive Neuroscience,
311,W2962756421,node2vec,"Aditya Grover, Jure Leskovec","Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.",Bioinformatics and Genomic Networks,Molecular Biology,
312,W3104097132,DeepWalk,"Bryan Perozzi, Rami Al‐Rfou, Steven Skiena","We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs.",Topic Modeling,Artificial Intelligence,
314,W114517082,Large-Scale Machine Learning with Stochastic Gradient Descent,Léon Bottou,"During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.",Stochastic Gradient Optimization Techniques,Artificial Intelligence,
321,W1686810756,Very Deep Convolutional Networks for Large-Scale Image Recognition,"Karen Simonyan, Andrew Zisserman","In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",Advanced Vision and Imaging,Computer Vision and Pattern Recognition,
341,W1909320841,Stochastic Backpropagation and Approximate Inference in Deep Generative Models,"Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra","We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
345,W2108677974,Practical Variational Inference for Neural Networks,Alex Graves,"Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.",Neural Networks and Applications,Artificial Intelligence,
349,W1522301498,Adam: A Method for Stochastic Optimization,"Diederik P. Kingma, Jimmy Ba","We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",Stochastic Gradient Optimization Techniques,Artificial Intelligence,
353,W3036167779,Denoising Diffusion Probabilistic Models,"Jonathan Ho, Ajay N. Jain, Pieter Abbeel","We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
354,W3035060554,Bootstrap your own latent: A new approach to self-supervised Learning,"Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Ávila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Rémi Munos, Michal Vaľko","We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,
355,W582134693,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,"Yarin Gal, Zoubin Ghahramani","Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
358,W1511986666,Probabilistic graphical models : principles and techniques,"Daniel L. Koller, Nir Friedman","Most tasks require a person or an automated system to reason -- to reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
364,W1826234144,Variational dropout and the local reparameterization trick,"Diederik P. Kingma, Tim Salimans, Max Welling","We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the mini-batch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
367,W1893585201,Learning to generate chairs with convolutional neural networks,"Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox","We train a generative convolutional neural network which is able to generate images of objects given object type, viewpoint, and color. We train the network in a supervised manner on a dataset of rendered 3D chair models. Our experiments show that the network does not merely learn all images by heart, but rather finds a meaningful representation of a 3D chair model allowing it to assess the similarity of different chairs, interpolate between given viewpoints to generate the missing ones, or invent new chair styles by interpolating between chairs from the training set. We show that the network can be used to find correspondences between different chairs from the dataset, outperforming existing approaches on this task.",Face recognition and analysis,Computer Vision and Pattern Recognition,
368,W1906598733,Markov Chain Monte Carlo and Variational Inference: Bridging the Gap,"Tim Salimans, Diederik P. Kingma, Max Welling","Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.",Markov Chains and Monte Carlo Methods,Statistics and Probability,
385,W1503398984,Machine Learning : A Probabilistic Perspective,Kevin P. Murphy,"Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.",Computational Physics and Python Applications,Artificial Intelligence,
391,W1606347560,Theano: new features and speed improvements,"Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio","Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.",Parallel Computing and Optimization Techniques,Hardware and Architecture,
403,W2892341857,A Survey on Deep Learning,"Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria Presa Reyes, Mei‐Ling Shyu, Shu‐Ching Chen, S. S. Iyengar","The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain. Deep learning uses multiple layers to represent the abstractions of data to build computational models. Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing. However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective. The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level. Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth. This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications. It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.",Anomaly Detection Techniques and Applications,Artificial Intelligence,
422,W3135550350,Deep Learning for Anomaly Detection,"Guansong Pang, Chunhua Shen, Longbing Cao, Anton van den Hengel","Anomaly detection, a.k.a. outlier detection or novelty detection, has been a lasting yet active research area in various research communities for several decades. There are still some unique problem complexities and challenges that require advanced approaches. In recent years, deep learning enabled anomaly detection, i.e., deep anomaly detection , has emerged as a critical direction. This article surveys the research of deep anomaly detection with a comprehensive taxonomy, covering advancements in 3 high-level categories and 11 fine-grained categories of the methods. We review their key intuitions, objective functions, underlying assumptions, advantages, and disadvantages and discuss how they address the aforementioned challenges. We further discuss a set of possible future opportunities and new perspectives on addressing the challenges.",Anomaly Detection Techniques and Applications,Artificial Intelligence,
460,W1975116882,On the theory of mortality measurement,Ulf Grenander,"Abstract 2.1. Limitations of the parametric methods. In the previous sections we have studied the efficiency of various methods of estimating the force of mortality. The most efficient of these is, at least for large samples, the one given by the maximum likelihood method, and the rest of them have to be compared to this best estimate. However in this discussion the notion of efficiency is based on the assumption that the mortality intensity can be expressed by Makeham's formula. How realistic is this in practice?",Health and Conflict Studies,General Health Professions,
461,W1995883243,A Note on the $L_1$ Consistency of Variable Kernel Estimates,Luc Devroye,"A sample $X_1, \cdots, X_n$ of i.i.d. $R^d$-valued random vectors with common density $f$ is used to construct the density estimate $f_n(x) = (1/n) \sum^n_{i = 1} H^{-d}_{ni}K((x - X_i)/H_{ni}),$ where $K$ is a given density on $R^d$, and the $H_{ni}$'s are positive functions of $n, i$ and $X_1, \cdots, X_n$ (but not of $x$). The $H_{ni}$'s can be thought of as locally adapted smoothing parameters. We give sufficient conditions for the weak convergence to 0 of $\int |f_n - f|$ for all $f$. This is illustrated for the estimate of Breiman, Meisel and Purcell (1977).",Statistical Methods and Inference,Statistics and Probability,
462,W2006650073,A New Method of Interpolation and Smooth Curve Fitting Based on Local Procedures,Hiroshi Akima,"A new mathematical method is developed for interpolation from a given set of data points in a plane and for fitting a smooth curve to the points. This method is devised in such a way that the resultant curve will pass through the given points and will appear smooth and natural. It is based on a piecewise function composed of a set of polynomials, each of degree three, at most, and applicable to successive intervals of the given points. In this method, the slope of the curve is determined at each given point locally, and each polynomial representing a portion of the curve between a pair of given points is determined by the coordinates of and the slopes at the points. Comparison indicates that the curve obtained by this new method is closer to a manually drawn curve than those drawn by other mathematical methods.",Advanced Numerical Analysis Techniques,Computational Mechanics,
463,W2036742716,General random number generator [G5],Edgar L. Butler,"article Free Access Share on General random number generator [G5] Author: Edgar L. Butler Texas A & M Univ., College Station Texas A & M Univ., College StationView Profile Authors Info & Claims Communications of the ACMVolume 13Issue 1Jan. 1970pp 49–52https://doi.org/10.1145/361953.361974Published:01 January 1970Publication History 8citation469DownloadsMetricsTotal Citations8Total Downloads469Last 12 Months23Last 6 weeks4 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",Chaos-based Image/Signal Encryption,Computer Vision and Pattern Recognition,
465,W2065221212,An Efficient Method for Generating Discrete Random Variables with General Distributions,A.J. Walker,"article Free Access Share on An Efficient Method for Generating Discrete Random Variables with General Distributions Author: Alastair J. Walker Department of Electrical Engineering, University of Witwatersrand, 1 Jan Smuts Ave., Johnnesburg 2001, South Africa Department of Electrical Engineering, University of Witwatersrand, 1 Jan Smuts Ave., Johnnesburg 2001, South AfricaView Profile Authors Info & Claims ACM Transactions on Mathematical SoftwareVolume 3Issue 3Sept. 1977 pp 253–256https://doi.org/10.1145/355744.355749Online:01 September 1977Publication History 346citation3,515DownloadsMetricsTotal Citations346Total Downloads3,515Last 12 Months267Last 6 weeks41 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",Statistical Distribution Estimation and Applications,Statistics and Probability,
466,W2080516472,On Generating Random Variates from an Empirical Distribution,"Hui-Chuan Chen, Yoshinori Asau","Abstract This note presents a method for generating a sequence of random variates from an empirical distribution. Computational results show that the proposed method requires less computation time than two standard methods but requires only ten more words of memory. The savings in time becomes more significant as the number of distinct values contained in the distribution, or the sample size increases.",Bayesian Methods and Mixture Models,Artificial Intelligence,
467,W2083904924,"The Equivalence of Weak, Strong and Complete Convergence in $L_1$ for Kernel Density Estimates",Luc Devroye,"Let $f$ be a density on $R^d$, and let $f_n$ be the kernel estimate of $f$, $f_n(x) = (nh^d)^{-1} \sum^n_{i=1} K((x - X_i)/h)$ where $h = h_n$ is a sequence of positive numbers, and $K$ is an absolutely integrable function with $\int K(x) dx = 1$. Let $J_n = \int |f_n(x) - f(x)| dx$. We show that when $\lim_nh = 0$ and $\lim_nnh^d = \infty$, then for every $\varepsilon > 0$ there exist constants $r, n_0 > 0$ such that $P(J_n \geq \varepsilon) \leq \exp(-rn), n \geq n_0$. Also, when $J_n \rightarrow 0$ in probability as $n \rightarrow \infty$ and $K$ is a density, then $\lim_nh = 0$ and $\lim_nnh^d = \infty$.",Mathematical Approximation and Integration,Numerical Analysis,
468,W2151447176,On the Alias Method for Generating Random Variables from a Discrete Distribution,"Richard A. Kronmal, Arthur V. Peterson","Abstract Abstract The alias method of Walker is a clever, new, fast method for generating random variables from an arbitrary, specified discrete distribution. A simple probabilistic proof is given, in terms of mixtures, that the method works for any discrete distribution with a finite number of outcomes. A more efficient version of the table-generating portion of the method is described. Finally, a brief discussion on efficiency of the method is given. We believe that the generality, speed, and simplicity of the method make it attractive for use in generating discrete random variables. Key Words: Random number generationDiscrete distributionAlias methodRejection method",Statistical and Computational Modeling,Artificial Intelligence,
471,W2467604901,Tutorial on Variational Autoencoders,Carl Doersch,"In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
472,W2409550820,Density estimation using Real NVP,"Laurent Dinh, Jascha Sohl‐Dickstein, Samy Bengio","Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",Machine Learning and Data Classification,Artificial Intelligence,
473,W3104335016,PROBABILISTIC FORECASTING OF THE MASSES AND RADII OF OTHER WORLDS,"Jingjing Chen, David Kipping","ABSTRACT Mass and radius are two of the most fundamental properties of an astronomical object. Increasingly, new planet discoveries are being announced with a measurement of one of these quantities, but not both. This has led to a growing need to forecast the missing quantity using the other, especially when predicting the detectability of certain follow-up observations. We present an unbiased forecasting model built upon a probabilistic mass–radius relation conditioned on a sample of 316 well-constrained objects. Our publicly available code, Forecaster , accounts for observational errors, hyper-parameter uncertainties, and the intrinsic dispersions observed in the calibration sample. By conditioning our model on a sample spanning dwarf planets to late-type stars, Forecaster can predict the mass (or radius) from the radius (or mass) for objects covering nine orders of magnitude in mass. Classification is naturally performed by our model, which uses four classes we label as Terran worlds, Neptunian worlds, Jovian worlds, and stars. Our classification identifies dwarf planets as merely low-mass Terrans (like the Earth) and brown dwarfs as merely high-mass Jovians (like Jupiter). We detect a transition in the mass–radius relation at <?CDATA ${2.0}_{-0.6}^{+0.7}$?> M ⊕ , which we associate with the divide between solid, Terran worlds and Neptunian worlds. This independent analysis adds further weight to the emerging consensus that rocky super-Earths represent a narrower region of parameter space than originally thought. Effectively, then, the Earth is the super-Earth we have been looking for.","Stellar, planetary, and galactic studies",Astronomy and Astrophysics,
474,W2963407932,Looking for the Devil in the Details: Learning Trilinear Attention Sampling Network for Fine-Grained Image Recognition,"Heliang Zheng, Jianlong Fu, Zheng-Jun Zha, Jiebo Luo","Learning subtle yet discriminative features (e.g., beak and eyes for a bird) plays a significant role in fine-grained image recognition. Existing attention-based approaches localize and amplify significant parts to learn fine-grained details, which often suffer from a limited number of parts and heavy computational cost. In this paper, we propose to learn such fine-grained features from hundreds of part proposals by Trilinear Attention Sampling Network (TASN) in an efficient teacher-student manner. Specifically, TASN consists of 1) a trilinear attention module, which generates attention maps by modeling the inter-channel relationships, 2) an attention-based sampler which highlights attended parts with high resolution, and 3) a feature distiller, which distills part features into an object-level feature by weight sharing and feature preserving strategies. Extensive experiments verify that TASN yields the best performance under the same settings with the most competitive approaches, in iNaturalist-2017, CUB-Bird, and Stanford-Cars datasets.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
475,W1499798934,Stochastic Gradient VB and the Variational Auto-Encoder,"Diederik P. Kingma, Max Welling","Can we efficiently learn the parameters of directed probabilistic models, in
the presence of continuous latent variables with intractable posterior
distributions, and large datasets? We introduce an unsupervised on-line
learning algorithm that efficiently optimizes the variational lower bound on
the marginal likelihood and that, under some mild conditions, even works in the
intractable case. The algorithm, Stochastic Gradient Variational Bayes (SGVB),
optimizes a probabilistic encoder (also called a recognition model) to
approximate the intractable posterior distribution of the latent variables.
Crucial is a reparameterization of the variational bound with an independent
noise variable, yielding a stochastic objective function which can be jointly
optimized w.r.t. variational and generative parameters using standard
gradient-based stochastic optimization methods. Theoretical advantages are
reflected in experimental results.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
477,W2752332392,Analyzing the Training Processes of Deep Generative Models,"Mengchen Liu, Jiaxin Shi, Kelei Cao, Jun Zhu, Shi‐Xia Liu","Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.",Data Visualization and Analytics,Computer Vision and Pattern Recognition,
478,W2950880273,Graph Recurrent Networks With Attributed Random Walks,"Xiao Huang, Qingquan Song, Yuening Li, Xia Hu","Random walks are widely adopted in various network analysis tasks ranging from network embedding to label propagation. It could capture and convert geometric structures into structured sequences while alleviating the issues of sparsity and curse of dimensionality. Though random walks on plain networks have been intensively studied, in real-world systems, nodes are often not pure vertices, but own different characteristics, described by the rich set of data associated with them. These node attributes contain plentiful information that often complements the network, and bring opportunities to the random-walk-based analysis. However, it is unclear how random walks could be developed for attributed networks towards an effective joint information extraction. Node attributes make the node interactions more complicated and are heterogeneous with respect to topological structures.",Complex Network Analysis Techniques,Statistical and Nonlinear Physics,
596,W2047229728,Keeping the neural networks simple by minimizing the description length of the weights,"Geoffrey E. Hinton, Drew van Camp","Article Keeping the neural networks simple by minimizing the description length of the weights Share on Authors: Geoffrey E. Hinton View Profile , Drew van Camp View Profile Authors Info & Claims COLT '93: Proceedings of the sixth annual conference on Computational learning theoryAugust 1993 Pages 5–13https://doi.org/10.1145/168304.168306Online:01 August 1993Publication History 235citation2,394DownloadsMetricsTotal Citations235Total Downloads2,394Last 12 Months367Last 6 weeks44 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",Neural Networks and Applications,Artificial Intelligence,
598,W2113606819,Efficient sparse coding algorithms,"Honglak Lee, Alexis Battle, Rajat Raina, Andrew Y. Ng","Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.",Neural dynamics and brain function,Cognitive Neuroscience,
614,W2108384452,An Information-Maximization Approach to Blind Separation and Blind Deconvolution,"Anthony J. Bell, Terrence J. Sejnowski","We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in ""blind"" signal processing.",Blind Source Separation Techniques,Signal Processing,
617,W2135181320,The Neural Autoregressive Distribution Estimator,"Hugo Larochelle, Iain Murray","We describe a new approach for modeling the distribution of high-dimensional vectors of discrete variables. This model is inspired by the restricted Boltzmann machine (RBM), which has been shown to be a powerful model of such distributions. However, an RBM typically does not provide a tractable distribution estimator, since evaluating the probability it assigns to some given observation requires the computation of the so-called partition function, which itself is intractable for RBMs of even moderate size. Our model circumvents this diculty by decomposing the joint distribution of observations into tractable conditional distributions and modeling each conditional using a non-linear function similar to a conditional of an RBM. Our model can also be interpreted as an autoencoder wired such that its output can be used to assign valid probabilities to observations. We show that this new model outperforms other multivariate binary distribution estimators on several datasets and performs similarly to a large (but intractable) RBM.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
666,W2194775991,Deep Residual Learning for Image Recognition,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun","Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
687,W2962851448,Advances in Variational Inference,"Cheng Zhang, Judith Bütepage, Hedvig Kjellström, Stephan Mandt","Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
695,W1663973292,Pattern Recognition and Machine Learning,Christopher Bishop,"The <i>Journal of Electronic Imaging</i> (JEI), copublished bimonthly with the Society for Imaging Science and Technology, publishes peer-reviewed papers that cover research and applications in all areas of electronic imaging science and technology.",Statistical Methods and Inference,Statistics and Probability,
733,W2519887557,Semi-Supervised Classification with Graph Convolutional Networks,"Thomas Kipf, Max Welling",We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.,Advanced Graph Neural Networks,Artificial Intelligence,
747,W1537436316,Learning Representations by Maximizing Compression,"Karol Gregor, Yann LeCun","We give an algorithm that learns a representation of data through compression. The algorithm 1) predicts bits sequentially from those previously seen and 2) has a structure and a number of computations similar to an autoencoder. The likelihood under the model can be calculated exactly, and arithmetic coding can be used directly for compression. When training on digits the algorithm learns filters similar to those of restricted boltzman machines and denoising autoencoders. Independent samples can be drawn from the model by a single sweep through the pixels. The algorithm has a good compression performance when compared to other methods that work under random ordering of pixels.",Neural Networks and Applications,Artificial Intelligence,
748,W1810943226,Generating Sequences With Recurrent Neural Networks,Alex Graves,"This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",Handwritten Text Recognition Techniques,Computer Vision and Pattern Recognition,
749,W2013035813,A Connection Between Score Matching and Denoising Autoencoders,Pascal Vincent,"Denoising autoencoders have been previously shown to be competitive alternatives to restricted Boltzmann machines for unsupervised pretraining of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a specific energy-based model to that of a nonparametric Parzen density estimator of the data. This yields several useful insights. It defines a proper probabilistic model for the denoising autoencoder technique, which makes it in principle possible to sample from them or rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justifies the use of tied weights between the encoder and decoder and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
750,W2027311549,A review of simulation optimization techniques,Sigrún Andradóttir,We present a review of methods for optimizing stochastic systems using simulation. The focus is on gradient based techniques for optimization with respect to continuous decision parameters and on random search methods for optimization with respect to discrete decision parameters.,Simulation Techniques and Applications,Management Science and Operations Research,
752,W2123284177,A Generative Process for sampling Contractive Auto-Encoders,"Salah Rifai, Yoshua Bengio, Pascal Vincent, Yann Dauphin","The contractive auto-encoder learns a representation of the input data that captures the local manifold structure around each data point, through the leading singular vectors of the Jacobian of the transformation from input to representation. The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors, while remaining in a high-density region of the input space. This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto-encoder. The associated stochastic process defines a distribution from which one can sample, and which experimentally appears to converge quickly and mix well between modes, compared to Restricted Boltzmann Machines and Deep Belief Networks. The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer. We show that this can help learn and represent invariances present in the data and improve classification error.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
753,W2157002241,Evaluating probabilities under high-dimensional latent variable models,"Iain Murray, Ruslan Salakhutdinov","We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost.",Statistical Methods and Inference,Statistics and Probability,
754,W2963090522,Variational Inference with Normalizing Flows,"Danilo Jimenez Rezende, Shakir Mohamed","The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.",Model Reduction and Neural Networks,Statistical and Nonlinear Physics,
755,W2962897886,Stochastic Backpropagation and Approximate Inference in Deep Generative Models,"Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra","We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
756,W2164411961,Weight Uncertainty in Neural Network,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra","We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",Adversarial Robustness in Machine Learning,Artificial Intelligence,
763,W2097018403,Linear spatial pyramid matching using sparse coding for image classification,"Shuicheng Yan, Kai Yu, Yihong Gong, Thomas S. Huang","Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> ∼ n <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">3</sup> ) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handlemore than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
800,W2064675550,Long Short-Term Memory,"Sepp Hochreiter, Jürgen Schmidhuber","Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",Neural Networks and Applications,Artificial Intelligence,
805,W2131462252,A Scalable Hierarchical Distributed Language Model,"Andriy Mnih, Geoffrey E. Hinton","Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the non-hierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.",Natural Language Processing Techniques,Artificial Intelligence,
813,W1895577753,Show and tell: A neural image caption generator,"Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan","Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
839,W3035524453,Momentum Contrast for Unsupervised Visual Representation Learning,"Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick","We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,
877,W2144898279,Approximate Bayesian Inference for Latent Gaussian models by using Integrated Nested Laplace Approximations,"Håvard Rue, Sara Martino, Nicolás Chopin","Summary Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
879,W2964059111,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,"Yarin Gal, Zoubin Ghahramani","Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
882,W3101380508,Variational Inference: A Review for Statisticians,"David M. Blei, Alp Kucukelbir, Jon McAuliffe","One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
905,W4313156423,Masked Autoencoders Are Scalable Vision Learners,"Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick","This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,
915,W2015861736,Convolutional networks and applications in vision,"Yann LeCun, Koray Kavukcuoglu, Clément Farabet","Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or ""features"")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
920,W1995875735,A Mathematical Theory of Communication,Claude E. Shannon,"The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup> and Hartley <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.",graph theory and CDMA systems,Electrical and Electronic Engineering,
921,W2036084078,The Variational Gaussian Approximation Revisited,"Manfred Opper, Cédric Archambeau","The variational approximation of posterior distributions by multivariate gaussians has been much less popular in the machine learning community compared to the corresponding approximation by factorizing distributions. This is for a good reason: the gaussian approximation is in general plagued by an Omicron(N)(2) number of variational parameters to be optimized, N being the number of random variables. In this letter, we discuss the relationship between the Laplace and the variational approximation, and we show that for models with gaussian priors and factorizing likelihoods, the number of variational parameters is actually Omicron(N). The approach is applied to gaussian process regression with nongaussian likelihoods.",Advanced Multi-Objective Optimization Algorithms,Computational Theory and Mathematics,
980,W1530235965,Divergence measures and message passing,Thomas P. Minka,"This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (‘exclusive’ versus ‘inclusive’ Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
982,W1662191912,The EM algorithm for mixtures of factor analyzers,"Zoubin Ghahramani, GE Hinton","Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing di erent local factor models in di erent regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation{Maximization algorithm for tting the parameters of this mixture of factor analyzers.",Scientific Research and Discoveries,Statistical and Nonlinear Physics,
1000,W1994618660,Learning Factorial Codes by Predictability Minimization,Jürgen Schmidhuber,"I propose a novel general principle for unsupervised learning of distributed nonredundant internal representations of input patterns. The principle is based on two opposing forces. For each representational unit there is an adaptive predictor, which tries to predict the unit from the remaining units. In turn, each unit tries to react to the environment such that it minimizes its predictability. This encourages each unit to filter ""abstract concepts"" out of the environmental input such that these concepts are statistically independent of those on which the other units focus. I discuss various simple yet potentially powerful implementations of the principle that aim at finding binary factorial codes (Barlow et al. 1989), i.e., codes where the probability of the occurrence of a particular input is simply the product of the probabilities of the corresponding code symbols. Such codes are potentially relevant for (1) segmentation tasks, (2) speeding up supervised learning, and (3) novelty detection. Methods for finding factorial codes automatically implement Occam's razor for finding codes using a minimal number of units. Unlike previous methods the novel principle has a potential for removing not only linear but also nonlinear output redundancy. Illustrative experiments show that algorithms based on the principle of predictability minimization are practically feasible. The final part of this paper describes an entirely local algorithm that has a potential for learning unique representations of extended input sequences.",Neural Networks and Applications,Artificial Intelligence,
1003,W2963073614,Image-to-Image Translation with Conditional Adversarial Networks,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros","We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pi×2pi× software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
1005,W2954996726,A survey on Image Data Augmentation for Deep Learning,"Connor Shorten, Taghi M. Khoshgoftaar","Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
1008,W2125389028,Conditional Generative Adversarial Nets,"Mehdi Mirza, Simon Osindero","Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
1036,W1534417711,Two problems with variational expectation maximisation for time series models,"Richard E. Turner, Maneesh Sahani","Variational methods are a key component of the approximate inference and learning toolbox. These methods fill an important middle ground, retaining distributional information about uncertainty in latent variables, unlike maximum a posteriori methods, and yet generally requiring less computational time than Markov chain Monte Carlo methods. In particular the variational expectation maximisation (vEM) and variational Bayes algorithms, both involving variational optimisation of a free-energy, are widely used in time series modelling. Here, we investigate the success of vEM in simple probabilistic time series models. First we consider the inference step of vEM, and show that a consequence of the well-known compactness property of variational inference is a failure to propagate uncertainty in time, thus limiting the usefulness of the retained distributional information. In particular, the uncertainty may appear to be smallest precisely when the approximation is poorest. Second, we consider parameter learning and analytically reveal systematic biases in the parameters found by vEM. Surprisingly, simpler variational approximations (such as mean-field) can lead to less bias than more complicated structured approximations.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1049,W1753482797,Recurrent Continuous Translation Models,"Nal Kalchbrenner, Phil Blunsom","We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",Topic Modeling,Artificial Intelligence,
1053,W2603777577,Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization,"Xun Huang, Serge Belongie","Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
1072,W2115979064,Variational Algorithms for Approximate Bayesian Inference,Matthew J. Beal,"The Bayesian framework for machine learning allows for the incorporation of prior knowledge in a coherent way, avoids overfitting problems, and provides a principled basis for selecting between alternative models. Unfortunately the computations required are usually intractable. This thesis presents a unified variational Bayesian (VB) framework which approximates these computations in models with latent variables using a lower bound on the marginal likelihood.

Chapter 1 presents background material on Bayesian inference, graphical models, and propagation algorithms. Chapter 2 forms the theoretical core of the thesis, generalising the expectation- maximisation (EM) algorithm for learning maximum likelihood parameters to the VB EM algorithm which integrates over model parameters. The algorithm is then specialised to the large family of conjugate-exponential (CE) graphical models, and several theorems are presented to pave the road for automated VB derivation procedures in both directed and undirected graphs (Bayesian and Markov networks, respectively).

Chapters 3–5 derive and apply the VB EM algorithm to three commonly-used and important models: mixtures of factor analysers, linear dynamical systems, and hidden Markov models. It is shown how model selection tasks such as determining the dimensionality, cardinality, or number of variables are possible using VB approximations. Also explored are methods for combining sampling procedures with variational approximations, to estimate the tightness of VB bounds and to obtain more effective sampling algorithms. Chapter 6 applies VB learning to a long-standing problem of scoring discrete-variable directed acyclic graphs, and compares the performance to annealed importance sampling amongst other methods. Throughout, the VB approximation is compared to other methods including sampling, Cheeseman-Stutz, and asymptotic approximations such as BIC. The thesis concludes with a discussion of evolving directions for model selection including infinite models and alternative approximations to the marginal likelihood.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1093,W1515272691,A family of algorithms for approximate bayesian inference,"Thomas P. Minka, Rosalind W. Picard","One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, “Expectation Propagation,” unifies and generalizes two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. 
Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction—propagating richer belief states which incorporate correlations between variables. 
This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1105,W601603264,Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference,"Yarin Gal, Zoubin Ghahramani","Convolutional neural networks (CNNs) work well on large datasets. But labelled data is hard to collect, and in some applications larger amounts of data are not available. The problem then is how to use CNNs with small data -- as CNNs overfit quickly. We present an efficient Bayesian CNN, offering better robustness to over-fitting on small data than traditional approaches. This is by placing a probability distribution over the CNN's kernels. We approximate our model's intractable posterior with Bernoulli variational distributions, requiring no additional model parameters. On the theoretical side, we cast dropout network training as approximate inference in Bayesian neural networks. This allows us to implement our model using existing tools in deep learning with no increase in time complexity, while highlighting a negative result in the field. We show a considerable improvement in classification accuracy compared to standard techniques and improve on published state-of-the-art results for CIFAR-10.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1107,W1506690472,ARPACK Users' Guide,"Richard B. Lehoucq, D. C. Sorensen, Chao Yang","List of figures List of tables Preface 1. Introduction to ARPACK. Important features Getting started Reverse communication interface Availability Installation Documentation Dependence on LAPACK and BLAS Expected performance P_ARPACK Contributed additions Trouble shooting and problems 2. Getting started with ARPACK. Directory structure and contents Getting started An example for a symmetric Eigenvalue problem 3. General use of ARPACK. Naming conventions, Precisions, and types Shift and invert spectral transformation mode Reverse communication structure for shift-Invert Using the computational modes Computational modes for real symmetric problems Postprocessing for Eigenvectors using dseupd Computational modes for real nonsymmetric problems Postprocessing for Eigenvectors Using dneupd Computational modes for complex problems Postprocessing for Eigenvectors Using zneupd 4. The implicitly restarted Arnoldi method: structure of the Eigenvalue problem Krylov subspaces and projection methods The Arnoldi factorization Restarting the Arnoldi method The generalized Eigenvalue problem Stopping Criterion 5. Computational routines. ARPACK subroutines LAPACK routines used by ARPACK BLAS routines used by ARPACK Appendix A. Templates and driver routines Symmetric drivers Real Nonsymmetric drivers Complex drivers Band drivers The singular value decomposition Appendix B. Tracking the progress of ARPACK. Obtaining trace output Check-pointing ARPACK Appendix C. The XYaupd ARPACK Routines. DSAUPD DNAUPD ZNAUPD Bibliography Index.",Matrix Theory and Algorithms,Computational Theory and Mathematics,
1109,W2013737143,The algebraic eigenvalue problem,J. H. Wilkinson,Theoretical background Perturbation theory Error analysis Solution of linear algebraic equations Hermitian matrices Reduction of a general matrix to condensed form Eigenvalues of matrices of condensed forms The LR and QR algorithms Iterative methods Bibliography Index.,,,
1110,W2049633694,Maximum Likelihood from Incomplete Data Via the <i>EM</i> Algorithm,"A. P. Dempster, N. M. Laird, Donald B. Rubin","Summary A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.",Control Systems and Identification,Control and Systems Engineering,
1111,W2125027820,Probabilistic Principal Component Analysis,"Michael E. Tipping, Chris Bishop","Summary Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.",Spectroscopy and Chemometric Analyses,Analytical Chemistry,
1112,W2128221272,Supervised learning from incomplete data via an EM approach,"Zoubin Ghahramani, Michael I. Jordan","Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. We use mixture models for the density estimates and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm--EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark--the iris data set--are presented.",Bayesian Methods and Mixture Models,Artificial Intelligence,
1115,W2117756735,Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions,"Nathan Halko, Per‐Gunnar Martinsson, Joel A. Tropp","Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed—either explicitly or implicitly—to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an $m \times n$ matrix. (i) For a dense input matrix, randomized algorithms require $\bigO(mn \log(k))$ floating-point operations (flops) in contrast to $ \bigO(mnk)$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to $\bigO(k)$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data.",Sparse and Compressive Sensing Techniques,Computational Mechanics,
1117,W2170702893,Tracking Whole-Brain Connectivity Dynamics in the Resting State,"Elena A. Allen, Eswar Damaraju, Sergey M. Plis, Erik B. Erhardt, Tom Eichele, Vince D. Calhoun","Spontaneous fluctuations are a hallmark of recordings of neural signals, emergent over time scales spanning milliseconds and tens of minutes. However, investigations of intrinsic brain organization based on resting-state functional magnetic resonance imaging have largely not taken into account the presence and potential of temporal variability, as most current approaches to examine functional connectivity (FC) implicitly assume that relationships are constant throughout the length of the recording. In this work, we describe an approach to assess whole-brain FC dynamics based on spatial independent component analysis, sliding time window correlation, and k-means clustering of windowed correlation matrices. The method is applied to resting-state data from a large sample (n = 405) of young adults. Our analysis of FC variability highlights particularly flexible connections between regions in lateral parietal and cingulate cortex, and argues against a labeling scheme where such regions are treated as separate and antagonistic entities. Additionally, clustering analysis reveals unanticipated FC states that in part diverge strongly from stationary connectivity patterns and challenge current descriptions of interactions between large-scale networks. Temporal trends in the occurrence of different FC states motivate theories regarding their functional roles and relationships with vigilance/arousal. Overall, we suggest that the study of time-varying aspects of FC can unveil flexibility in the functional coordination between different neural systems, and that the exploitation of these dynamics in further investigations may improve our understanding of behavioral shifts and adaptive processes.",Functional Brain Connectivity Studies,Cognitive Neuroscience,
1118,W2137570937,Dimensionality Reduction: A Comparative Review,"Laurens van der Maaten, Eric Postma, H.J. van den Herik","In recent years, a variety of nonlinear dimensionality reduction techniques have been proposed that aim to address the limitations of traditional techniques such as PCA and classical scaling. The paper presents a review and systematic comparison of these techniques. The performances of the nonlinear techniques are investigated on artificial and natural tasks. The results of the experiments reveal that nonlinear techniques perform well on selected artificial tasks, but that this strong performance does not necessarily extend to real-world tasks. The paper explains these results by identifying weaknesses of current nonlinear techniques, and suggests how the performance of nonlinear dimensionality reduction techniques may be improved.",Face and Expression Recognition,Computer Vision and Pattern Recognition,
1120,W2089497633,Random projection in dimensionality reduction,"Ella Bingham, Heikki Mannila","Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burden-some computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
1121,W1973741448,A Baseline for the Multivariate Comparison of Resting-State Networks,"Elena A. Allen, Erik B. Erhardt, Eswar Damaraju, William Gruner, J.M. Segall, Rogers F. Silva, Martin Havlíček, Srinivas Rachakonda, Jill Fries, Ravi Kalyanam, Andrew M. Michael, Arvind Caprihan, Jessica A. Turner, Tom Eichele, Steven Adelsheim, Angela D. Bryan, Juan Bustillo, Vincent P. Clark, Sarah W. Feldstein Ewing, Francesca M. Filbey, Corey C. Ford, Kent E. Hutchison, Rex E. Jung, Kent A. Kiehl, Piyadasa Kodituwakku, Yuko M. Komesu, Andrew R. Mayer, Godfrey D. Pearlson, J. P. Phillips, Joseph Sadek, Michael C. Stevens, Ursina Teuscher, Robert J. Thoma, Vince D. Calhoun","As the size of functional and structural MRI datasets expands, it becomes increasingly important to establish a baseline from which diagnostic relevance may be determined, a processing strategy that efficiently prepares data for analysis, and a statistical approach that identifies important effects in a manner that is both robust and reproducible. In this paper, we introduce a multivariate analytic approach that optimizes sensitivity and reduces unnecessary testing. We demonstrate the utility of this mega-analytic approach by identifying the effects of age and gender on the resting state networks of 603 healthy adolescents and adults (mean age: 23.4 years, range: 12 to 71 years). Data were collected on the same scanner, preprocessed using an automated analysis pipeline based in SPM, and studied using group independent component analysis. Resting state networks were identified and evaluated in terms of three primary outcome measures: time course spectral power, spatial map intensity, and functional network connectivity. Results revealed robust effects of age on all three outcome measures, largely indicating decreases in network coherence and connectivity with increasing age. Gender effects were of smaller magnitude but suggested stronger intra-network connectivity in females and more inter-network connectivity in males, particularly with regard to sensorimotor networks. These findings, along with the analysis approach and statistical framework described here, provide a useful baseline for future investigations of brain networks in health and disease.",Neural dynamics and brain function,Cognitive Neuroscience,
1122,W2124609748,Gaussian Process Dynamical Models for Human Motion,"Jonathan M. Wang, David J. Fleet, Aaron Hertzmann","We introduce Gaussian process dynamical models (GPDM) for nonlinear time series analysis, with applications to learning models of human pose and motion from high-dimensionalmotion capture data. A GPDM is a latent variable model. It comprises a low-dimensional latent space with associated dynamics, and a map from the latent space to an observation space. We marginalize out the model parameters in closed-form, using Gaussian process priors for both the dynamics and the observation mappings. This results in a non-parametric model for dynamical systems that accounts for uncertainty in the model. We demonstrate the approach, and compare four learning algorithms on human motion capture data in which each pose is 50-dimensional. Despite the use of small data sets, the GPDM learns an effective representation of the nonlinear dynamics in these spaces.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1175,W2140190241,Data mining: concepts and techniques,"Jiawei Han, Micheline Kamber, Jian Pei","The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data",Data Management and Algorithms,Signal Processing,
1179,W2156267802,Missing data: Our view of the state of the art.,"Joseph L. Schafer, John W. Graham","Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, discourage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayesian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the mainstream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art.",Statistical Methods and Bayesian Inference,Statistics and Probability,
1200,W2100235918,A Survey of Collaborative Filtering Techniques,"Xiaoyuan Su, Taghi M. Khoshgoftaar","As one of the most successful approaches to building recommender systems, collaborative filtering ( CF ) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, model-based, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.",Recommender Systems and Techniques,Information Systems,
1299,W1880262756,Latent dirichlet allocation,"David M. Blei, Andrew Y. Ng, Michael I. Jordan","We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.",Natural Language Processing Techniques,Artificial Intelligence,
1387,W2117111086,Nonparametric factor analysis with beta process priors,"John Paisley, Lawrence Carin","We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.",Bayesian Methods and Mixture Models,Artificial Intelligence,
1388,W2127498532,Variational inference for Dirichlet process mixtures,"David M. Blei, Michael I. Jordan","Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.",Bayesian Methods and Mixture Models,Artificial Intelligence,
1389,W2139385756,Non-conjugate Variational Message Passing for Multinomial and Binary Regression,"David A. Knowles, Tom Minka","Variational Message Passing (VMP) is an algorithmic implementation of the Variational Bayes (VB) method which applies only in the special case of conjugate exponential family models. We propose an extension to VMP, which we refer to as Non-conjugate Variational Message Passing (NCVMP) which aims to alleviate this restriction while maintaining modularity, allowing choice in how expectations are calculated, and integrating into an existing message-passing framework: Infer.NET. We demonstrate NCVMP on logistic binary and multinomial regression. In the multinomial case we introduce a novel variational bound for the soft-max factor which is tighter than other commonly used bounds whilst maintaining computational tractability.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1390,W2142508340,Stochastic search using the natural gradient,"Yi Sun, Daan Wierstra, Tom Schaul, Jürgen Schmidhuber","To optimize unknown 'fitness' functions, we present Natural Evolution Strategies, a novel algorithm that constitutes a principled alternative to standard stochastic search methods. It maintains a multinormal distribution on the set of solution candidates. The Natural Gradient is used to update the distribution's parameters in the direction of higher expected fitness, by efficiently calculating the inverse of the exact Fisher information matrix whereas previous methods had to use approximations. Other novel aspects of our method include optimal fitness baselines and importance mixing, a procedure adjusting batches with minimal numbers of fitness evaluations. The algorithm yields competitive results on a number of benchmarks.",Evolutionary Algorithms and Applications,Artificial Intelligence,
1391,W2158266063,Hierarchical Dirichlet Processes,"Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, David M. Blei","AbstractWe consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures and describe applications to problems in information retrieval and text modeling.KEY WORDS: ClusteringHierarchical modelMarkov chain Monte CarloMixture modelNonparametric Bayesian statistics",Bayesian Methods and Mixture Models,Artificial Intelligence,
1392,W2162995719,A Tighter Bound for Graphical Models,"M.A.R. Leisink, Hilbert J. Kappen","We present a method to bound the partition function of a Boltzmann machine neural network with any odd-order polynomial. This is a direct extension of the mean-field bound, which is first order. We show that the third-order bound is strictly better than mean field. Additionally, we derive a third-order bound for the likelihood of sigmoid belief networks. Numerical experiments indicate that an error reduction of a factor of two is easily reached in the region where expansion-based approximations are useful.",Model Reduction and Neural Networks,Statistical and Nonlinear Physics,
1393,W2165599843,Online Learning for Latent Dirichlet Allocation,"Matthew D. Hoffman, Francis Bach, David M. Blei","We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.",Bayesian Methods and Mixture Models,Artificial Intelligence,
1394,W2187741934,Online variational inference for the hierarchical Dirichlet process,"Chong Wang, John Paisley, David M. Blei","The hierarchical Dirichlet process (HDP) is a Bayesian nonparametric model that can be used to model mixed-membership data with a potentially infinite number of components. It has been applied widely in probabilistic topic modeling, where the data are documents and the components are distributions of terms that reflect recurring patterns (or “topics”) in the collection. Given a document collection, posterior inference is used to determine the number of topics needed and to characterize their distributions. One limitation of HDP analysis is that existing posterior inference algorithms require multiple passes through all the data—these algorithms are intractable for very large scale applications. We propose an online variational inference algorithm for the HDP, an algorithm that is easily applicable to massive and streaming data. Our algorithm is significantly faster than traditional inference algorithms for the HDP, and lets us analyze much larger data sets. We illustrate the approach on two large collections of text, showing improved performance over online LDA, the finite counterpart to the HDP topic model.",Bayesian Methods and Mixture Models,Artificial Intelligence,
1395,W759726671,Piecewise bounds for estimating bernoulli-logistic latent Gaussian models,"Benjamin M. Marlin, Mohammad Emtiyaz Khan, Kevin P. Murphy","Bernoulli-logistic latent Gaussian models (bLGMs) are a useful model class, but accurate parameter estimation is complicated by the fact that the marginal likelihood contains an intractable logistic-Gaussian integral. In this work, we propose the use of fixed piecewise linear and quadratic upper bounds to the logistic-log-partition (LLP) function as a way of circumventing this intractable integral. We describe a framework for approximately computing minimax optimal piecewise quadratic bounds, as well a generalized expectation maximization algorithm based on using piecewise bounds to estimate bLGMs. We prove a theoretical result relating the maximum error in the LLP bound to the maximum error in the marginal likelihood estimate. Finally, we present empirical results showing that piece-wise bounds can be significantly more accurate than previously proposed variational bounds.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1406,W2045656233,Bayesian Data Analysis,"Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, Donald B. Rubin","FUNDAMENTALS OF BAYESIAN INFERENCE Probability and Inference Single-Parameter Models Introduction to Multiparameter Models Asymptotics and Connections to Non-Bayesian Approaches Hierarchical Models FUNDAMENTALS OF BAYESIAN DATA ANALYSIS Model Checking Evaluating, Comparing, and Expanding Models Modeling Accounting for Data Collection Decision Analysis ADVANCED COMPUTATION Introduction to Bayesian Computation Basics of Markov Chain Simulation Computationally Efficient Markov Chain Simulation Modal and Distributional Approximations REGRESSION MODELS Introduction to Regression Models Hierarchical Linear Models Generalized Linear Models Models for Robust Inference Models for Missing Data NONLINEAR AND NONPARAMETRIC MODELS Parametric Nonlinear Models Basic Function Models Gaussian Process Models Finite Mixture Models Dirichlet Process Models APPENDICES A: Standard Probability Distributions B: Outline of Proofs of Asymptotic Theorems C: Computation in R and Stan Bibliographic Notes and Exercises appear at the end of each chapter.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1410,W2126163471,Theory of probability,"Harold Jeffreys, R. Bruce Lindsay",1. Fundamental notions 2. Direct probabilities 3. Estimation problems 4. Approximate methods and simplifications 5. Significance tests: one new parameter 6. Significance tests: various complications 7. Frequency definitions and direct methods 8. General questions,Probability and Statistical Research,Statistics and Probability,
1419,W1481229783,Variational methods for inference and estimation in graphical models,"Tommi Jaakkola, Michael I. Jordan","Graphical models enhance the representational power of probability models through qualitative characterization of their properties. This also leads to greater efficiency in terms of the computational algorithms that empower such representations. The increasing complexity of these models, however, quickly renders exact probabilistic calculations infeasible. We propose a principled framework for approximating graphical models based on variational methods.
We develop variational techniques from the perspective that unifies and expands their applicability to graphical models. These methods allow the (recursive) computation of upper and lower bounds on the quantities of interest. Such bounds yield considerably more information than mere approximations and provide an inherent error metric for tailoring the approximations individually to the cases considered. These desirable properties, concomitant to the variational methods, are unlikely to arise as a result of other deterministic or stochastic approximations.
The thesis consists of the development of this variational methodology for probabilistic inference, Bayesian estimation, and towards efficient diagnostic reasoning in the domain of internal medicine. (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge, MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)",Machine Learning in Healthcare,Artificial Intelligence,
1462,W1967687583,Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems,C. Antoniak,"A random process called the Dirichlet process whose sample functions are almost surely probability measures has been proposed by Ferguson as an approach to analyzing nonparametric problems from a Bayesian viewpoint. An important result obtained by Ferguson in this approach is that if observations are made on a random variable whose distribution is a random sample function of a Dirichlet process, then the conditional distribution of the random measure can be easily calculated, and is again a Dirichlet process. This paper extends Ferguson's result to cases where the random measure is a mixing distribution for a parameter which determines the distribution from which observations are made. The conditional distribution of the random measure, given the observations, is no longer that of a simple Dirichlet process, but can be described as being a mixture of Dirichlet processes. This paper gives a formal definition for these mixtures and develops several theorems about their properties, the most important of which is a closure property for such mixtures. Formulas for computing the conditional distribution are derived and applications to problems in bio-assay, discrimination, regression, and mixing distributions are given.",Bayesian Methods and Mixture Models,Artificial Intelligence,
1482,W1546485431,Approximate Riemannian Conjugate Gradient Learning for Fixed-Form Variational Bayes,"Antti Honkela, Tapani Raiko, Mikael Kuusela, Matti Tornio, Juha Karhunen","Variational Bayesian (VB) methods are typically only applied to models in the conjugate-exponential family using the variational Bayesian expectation maximisation (VB EM) algorithm or one of its variants. In this paper we present an efficient algorithm for applying VB to more general models. The method is based on specifying the functional form of the approximation, such as multivariate Gaussian. The parameters of the approximation are optimised using a conjugate gradient algorithm that utilises the Riemannian geometry of the space of the approximations. This leads to a very efficient algorithm for suitably structured approximations. It is shown empirically that the proposed method is comparable or superior in efficiency to the VB EM in a case where both are applicable. We also apply the algorithm to learning a nonlinear state-space model and a nonlinear factor analysis model for which the VB EM is not applicable. For these models, the proposed algorithm outperforms alternative gradient-based methods by a significant margin.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1484,W1934021597,Expectation Propagation for approximate Bayesian inference,Thomas P. Minka,"This paper presents a new deterministic approximation technique in Bayesian networks. This method, ""Expectation Propagation"", unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. All three algorithms try to recover an approximate distribution which is close in KL divergence to the true distribution. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining certain expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Expectation Propagation also extends belief propagation in the opposite direction - it can propagate richer belief states that incorporate correlations between nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1485,W1984268382,Variational Bayesian learning of directed graphical models with hidden variables,"Matthew J. Beal, Zoubin Ghahramani","A key problem in statistics and machine learning is inferring suitable structure of a model given some observed data. A Bayesian approach to model comparison makes use of the marginal likelihood of each candidate model to form a posterior distribution over models; unfortunately for most models of interest, notably those containing hidden or latent variables, the marginal likelihood is intractable to compute. We present the variational Bayesian (VB) algorithm for directed graphical models, which optimises a lower bound approximation to the marginal likelihood in a procedure similar to the standard EM algorithm. We show that for a large class of models, which we call conjugate exponential, the VB algorithm is a straightforward generalisation of the EM algorithm that incorporates uncertainty over model parameters. In a thorough case study using a small class of bipartite DAGs containing hidden variables, we compare the accuracy of the VB approximation to existing asymptotic-data approximations such as the Bayesian Information Criterion (BIC) and the Cheeseman-Stutz (CS) criterion, and also to a sampling based gold standard, Annealed Importance Sampling (AIS). We find that the VB algorithm is empirically superior to CS and BIC, and much faster than AIS. Moreover, we prove that a VB approximation can always be constructed in such a way that guarantees it to be more accurate than the CS approximation.",Bayesian Methods and Mixture Models,Artificial Intelligence,
1495,W1955368298,Variational inference in nonconjugate models,"Chong Wang, David M. Blei","Mean-field variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest--like the correlated topic model and Bayesian logistic regression--are nonconjugate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world data sets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
1498,W1970789124,Natural Gradient Works Efficiently in Learning,Шун-ичи Амари,"When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed.",Blind Source Separation Techniques,Signal Processing,
1553,W2159080219,Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference,Judea Pearl,"From the Publisher:
Probabilistic Reasoning in Intelligent Systems is a complete andaccessible account of the theoretical foundations and computational methods that underlie plausible reasoning under uncertainty. The author provides a coherent explication of probability as a language for reasoning with partial belief and offers a unifying perspective on other AI approaches to uncertainty, such as the Dempster-Shafer formalism, truth maintenance systems, and nonmonotonic logic. The author distinguishes syntactic and semantic approaches to uncertaintyand offers techniques, based on belief networks, that provide a mechanism for making semantics-based systems operational. Specifically, network-propagation techniques serve as a mechanism for combining the theoretical coherence of probability theory with modern demands of reasoning-systems technology: modular declarative inputs, conceptually meaningful inferences, and parallel distributed computation. Application areas include diagnosis, forecasting, image interpretation, multi-sensor fusion, decision support systems, plan recognition, planning, speech recognitionin short, almost every task requiring that conclusions be drawn from uncertain clues and incomplete information.
Probabilistic Reasoning in Intelligent Systems will be of special interest to scholars and researchers in AI, decision theory, statistics, logic, philosophy, cognitive psychology, and the management sciences. Professionals in the areas of knowledge-based systems, operations research, engineering, and statistics will find theoretical and computational tools of immediate practical use. The book can also be used as an excellent text for graduate-level courses in AI, operations research, or applied probability.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1567,W2112447569,Online Learning for Matrix Factorization and Sparse Coding,"Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro","Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.",Sparse and Compressive Sensing Techniques,Computational Mechanics,
1583,W1915315806,Continuous time dynamic topic models,"Chong Wang, David M. Blei, David Heckerman","In this paper, we develop the continuous time dynamic model (cDTM). The cDTM is a dynamic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a topic is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.",Topic Modeling,Artificial Intelligence,
1639,W1570770495,The Variational Bayesian EM Algorithm for Incomplete Data: With Application to Scoring Graphical Model Structures,"Matthew J. Beal, Zoubin Ghahramani","Abstract We present an efficient procedure for estimating the marginal likelihood of probabilistic models with latent variables or incomplete data. This method constructs and optimizes a lower bound on the marginal likelihood using variational calculus, resulting in an iterative algorithm which generalizes the EM algorithm by maintaining posterior distributions over both latent variables and parameters. We define the family of conjugate-exponential models—which includes finite mixtures of exponential family models, factor analysis, hidden Markov models, linear state-space models, and other models of interest—for which this bound on the marginal likelihood can be computed very simply through a modification of the standard EM algorithm. In particular, we focus on applying these bounds to the problem of scoring discrete directed graphical model structures (Bayesian networks). Extensive simulations comparing the variational bounds to the usual approach based on the Bayesian Information Criterion (BIC) and to a sampling-based gold standard method known as Annealed Importance Sampling (AIS) show that variational bounds substantially outperform BIC in finding the correct model structure at relatively little computational cost, while approaching the performance of the much more costly AIS procedure. Using AIS allows us to provide the first serious case study of the tightness of variational bounds. We also analyze the performance of AIS through a variety of criteria, and outline directions in which this work can be extended.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1694,W1528708126,Bayesian Computation with R,Jim Albert,"There has been a dramatic growth in the development and application of Bayesian inferential methods. Some of this growth is due to the availability of powerful simulation-based algorithms to summarize posterior distributions. There has been also a growing interest in the use of the system R for statistical analyses. R's open source nature, free availability, and large number of contributor packages have made R the software of choice for many statisticians in education and industry. Bayesian Computation with R introduces Bayesian modeling by the use of computation using the R language. The early chapters present the basic tenets of Bayesian thinking by use of familiar one and two-parameter inferential problems. Bayesian computational methods such as Laplace's method, rejection sampling, and the SIR algorithm are illustrated in the context of a random effects model. The construction and implementation of Markov Chain Monte Carlo (MCMC) methods is introduced. These simulation-based algorithms are implemented for a variety of Bayesian applications such as normal and binary response regression, hierarchical modeling, order-restricted inference, and robust modeling. Algorithms written in R are used to develop Bayesian tests and assess Bayesian models by use of the posterior predictive distribution. The use of R to interface with WinBUGS, a popular MCMC computing language, is described with several illustrative examples. This book is a suitable companion book for an introductory course on Bayesian methods and is valuable to the statistical practitioner who wishes to learn more about the R language and Bayesian methodology. The LearnBayes package, written by the author and available from the CRAN website, contains all of the R functions described in the book. The second edition contains several new topics such as the use of mixtures of conjugate priors and the use of Zellners g priors to choose between models in linear regression. There are more illustrations of the construction of informative prior distributions, such as the use of conditional means priors and multivariate normal priors in binary regressions. The new edition contains changes in the R code illustrations according to the latest edition of the LearnBayes package.",Statistical Methods and Bayesian Inference,Statistics and Probability,
1702,W2122925692,Self-organization in a perceptual network,Ralph Linsker,"The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>",Neural Networks and Applications,Artificial Intelligence,
1703,W2142901448,Information Theory and Reliable Communication.,"P. M. Lee, Robert T. Gallager",Communication Systems and Information Theory. A Measure of Information. Coding for Discrete Sources. Discrete Memoryless Channels and Capacity. The Noisy-Channel Coding Theorem. Techniques for Coding and Decoding. Memoryless Channels with Discrete Time. Waveform Channels. Source Coding with a Fidelity Criterion. Index.,"Computability, Logic, AI Algorithms",Computational Theory and Mathematics,
1704,W2042925217,What Does the Retina Know about Natural Scenes?,"Joseph J. Atick, Amanda Redlich","By examining the experimental data on the statistical properties of natural scenes together with (retinal) contrast sensitivity data, we arrive at a first principle, theoretical hypothesis for the purpose of retinal processing and its relationship to an animal's environment. We argue that the retinal goal is to transform the visual input as much as possible into a statistically independent basis as the first step in creating a redundancy reduced representation in the cortex, as suggested by Barlow. The extent of this whitening of the input is limited, however, by the need to suppress input noise. Our explicit theoretical solutions for the retinal filters also show a simple dependence on mean stimulus luminance: they predict an approximate Weber law at low spatial frequencies and a De Vries-Rose law at high frequencies. Assuming that the dominant source of noise is quantum, we generate a family of contrast sensitivity curves as a function of mean luminance. This family is compared to psychophysical data.",Visual perception and processing mechanisms,Cognitive Neuroscience,
1705,W1993197592,Could information theory provide an ecological theory of sensory processing?,Joseph J. Atick,"The sensory pathways of animals are well adapted to processing a special class of signals, namely stimuli from the animal's environment. An important fact about natural stimuli is that they are typically very redundant and hence the sampled representation of these signals formed by the array of sensory cells is inefficient. One could argue for some animals and pathways, as we do in this review, that efficiency of information representation in the nervous system has several evolutionary advantages. Consequently, one might expect that much of the processing in the early levels of these sensory pathways could be dedicated towards recoding incoming signals into a more efficient form. In this review, we explore the principle of efficiency of information representation as a design principle for sensory processing. We give a preliminary discussion on how this principle could be applied in general to predict neural processing and then discuss concretely some neural systems where it recently has been shown to be successful. In particular, we examine the fly's LMC coding strategy and the mammalian retinal coding in the spatial, temporal and chromatic domains.",Neural dynamics and brain function,Cognitive Neuroscience,
1706,W2151351982,Towards a Theory of Early Visual Processing,"Joseph J. Atick, Amanda Redlich","We propose a theory of the early processing in the mammalian visual pathway. The theory is formulated in the language of information theory and hypothesizes that the goal of this processing is to recode in order to reduce a “generalized redundancy” subject to a constraint that specifies the amount of average information preserved. In the limit of no noise, this theory becomes equivalent to Barlow's redundancy reduction hypothesis, but it leads to very different computational strategies when noise is present. A tractable approach for finding the optimal encoding is to solve the problem in successive stages where at each stage the optimization is performed within a restricted class of transfer functions. We explicitly find the solution for the class of encodings to which the parvocellular retinal processing belongs, namely linear and nondivergent transformations. The solution shows agreement with the experimentally observed transfer functions at all levels of signal to noise.",Neural dynamics and brain function,Cognitive Neuroscience,
1707,W2936864631,Variational Information Distillation for Knowledge Transfer,"Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D. Lawrence, Zhenwen Dai",Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding hand-crafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.,Domain Adaptation and Few-Shot Learning,Artificial Intelligence,
1850,W1536680647,Fast R-CNN,Ross Girshick,"This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
1853,W2113325037,DeepPose: Human Pose Estimation via Deep Neural Networks,"Alexander Toshev, Christian Szegedy",We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regres- sors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formula- tion which capitalizes on recent advances in Deep Learn- ing. We present a detailed empirical analysis with state-of- art or better performance on four academic benchmarks of diverse real-world images.,Human Pose and Action Recognition,Computer Vision and Pattern Recognition,
1861,W2587284713,Improved Variational Inference with Inverse Autoregressive Flow,"Durk Kingma, Tim Salimans, Rafał Józefowicz, Xi Chen, Ilya Sutskever, Max Welling","The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
1873,W1618600317,Inferring parameters and structure of latent variable models by variational bayes,Hagai Attias,"Current methods for learning graphical models with latent variables and a fixed structure estimate optimal values for the model parameters. Whereas this approach usually produces overfitting and suboptimal generalization performance, carrying out the Bayesian program of computing the full posterior distributions over the parameters remains a difficult problem. Moreover, learning the structure of models with latent variables, for which the Bayesian approach is crucial, is yet a harder problem. In this paper I present the Variational Bayes framework, which provides a solution to these problems. This approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner without resorting to sampling methods. Unlike in the Laplace approximation, these posteriors are generally non-Gaussian and no Hessian needs to be computed. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. I demonstrate that this algorithm can be applied to a large class of models in several domains, including unsupervised clustering and blind source separation.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1875,W1686266550,On-Line Expectation–Maximization Algorithm for latent Data Models,"Olivier Cappé, Éric Moulines","In this contribution, we propose a generic online (also sometimes called adaptive or recursive) version of the Expectation-Maximisation (EM) algorithm applicable to latent variable models of independent observations. Compared to the algorithm of Titterington (1984), this approach is more directly connected to the usual EM algorithm and does not rely on integration with respect to the complete data distribution. The resulting algorithm is usually simpler and is shown to achieve convergence to the stationary points of the Kullback-Leibler divergence between the marginal distribution of the observation and the model distribution at the optimal rate, i.e., that of the maximum likelihood estimator. In addition, the proposed approach is also suitable for conditional (or regression) models, as illustrated in the case of the mixture of linear regressions model.",Statistical Methods and Inference,Statistics and Probability,
1877,W2223092947,<b>stm</b>: An <i>R</i> Package for Structural Topic Models,"Margaret E. Roberts, Brandon Stewart, Dustin Tingley","This paper demonstrates how to use the R package stm for structural topic modeling. The structural topic model allows researchers to flexibly estimate a topic model that includes document-level metadata. Estimation is accomplished through a fast variational approximation. The stm package provides many useful features, including rich ways to explore topics, estimate uncertainty, and visualize quantities of interest.",Computational and Text Analysis Methods,General Social Sciences,
1899,W1980452149,The computational complexity of probabilistic inference using bayesian belief networks,Gregory F. Cooper,"Bayesian belief networks provide a natural, efficient method for representing probabilistic dependencies among a set of variables. For these reasons, numerous researchers are exploring the use of belief networks as a knowledge representation in artificial intelligence. Algorithms have been developed previously for efficient probabilistic inference using special classes of belief networks. More general classes of belief networks, however, have eluded efforts to develop efficient inference algorithms. We show that probabilistic inference using belief networks is NP-hard. Therefore, it seems unlikely that an exact algorithm can be developed to perform probabilistic inference efficiently over all classes of belief networks. This result suggests that research should be directed away from the search for a general, efficient probabilistic inference algorithm, and toward the design of efficient special-case, average-case, and approximation algorithms.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
1900,W1999432334,Approximating probabilistic inference in Bayesian belief networks is NP-hard,"Paul Dagum, Michael Luby","It is known that exact computation of conditional probabilities in belief networks is NP-hard. Many investigators in the AI community have tacitly assumed that algorithms for performing approximate inference with belief networks are of polynomial complexity. Indeed, special cases of approximate inference can be performed in time polynomial in the input size. However, we have discovered that the general problem of approximating conditional probabilities with belief networks, like exact inference, resides in the NP-hard complexity class. We develop a complexity analysis to elucidate the difficulty of approximate probabilistic inference. More specifically, we show that the existence of a polynomial-time relative approximation algorithm for major classes of problem instances implies that NP ⊆ P. We present our proof and explore the implications of the result.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
2039,W1986931325,Atomic Decomposition by Basis Pursuit,"Scott Shaobing Chen, David L. Donoho, Michael A. Saunders","The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB). Basis Pursuit (BP) is a principle for decomposing a signal into an ""optimal"" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising. BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,
2040,W2010161441,K-SVD and its non-negative variant for dictionary design,"Michal Aharon, Michael Elad, Alfred M. Bruckstein⋆","In recent years there is a growing interest in the study of sparse representation for signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described as sparse linear combinations of these atoms. Recent activity in this field concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting pre-specified transforms, or by adapting the dictionary to a set of training signals. Both these techniques have been considered in recent years, however this topic is largely still open. In this paper we address the latter problem of designing dictionaries, and introduce the K-SVD algorithm for this task. We show how this algorithm could be interpreted as a generalization of the K-Means clustering process, and demonstrate its behavior in both synthetic tests and in applications on real data. Finally, we turn to describe its generalization to nonnegative matrix factorization problem that suits signals generated under an additive model with positive atoms. We present a simple and yet efficient variation of the K-SVD that handles such extraction of non-negative dictionaries.",Sparse and Compressive Sensing Techniques,Computational Mechanics,
2041,W2063978378,Least angle regression,"Bradley Efron, Trevor Hastie, Iain M. Johnstone, Robert Tibshirani","The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.",Statistical Methods and Inference,Statistics and Probability,
2042,W2082855665,Discriminative learned dictionaries for local image analysis,"Julien Mairal, Francis Bach, Jean Ponce, Guillermo Sapiro, Andrew Zisserman","Sparse signal models have been the focus of much recent research, leading to (or improving upon) state-of-the-art results in signal, image, and video restoration. This article extends this line of research into a novel framework for local image discrimination tasks, proposing an energy formulation with both sparse reconstruction and class discrimination components, jointly optimized during dictionary learning. This approach improves over the state of the art in texture segmentation experiments using the Brodatz database, and it paves the way for a novel scene analysis and recognition framework based on simultaneously learning discriminative and reconstructive dictionaries. Preliminary results in this direction using examples from the Pascal VOC06 and Graz02 datasets are presented as well.",Medical Image Segmentation Techniques,Computer Vision and Pattern Recognition,
2043,W2100543212,Sparse Coding via Thresholding and Local Competition in Neural Circuits,"Christopher J. Rozell, Don H. Johnson, Richard G. Baraniuk, Bruno A. Olshausen","While evidence indicates that neural systems may be employing sparse approximations to represent sensed stimuli, the mechanisms underlying this ability are not understood. We describe a locally competitive algorithm (LCA) that solves a collection of sparse coding principles minimizing a weighted combination of mean-squared error and a coefficient cost function. LCAs are designed to be implemented in a dynamical system composed of many neuron-like elements operating in parallel. These algorithms use thresholding functions to induce local (usually one-way) inhibitory competitions between nodes to produce sparse representations. LCAs produce coefficients with sparsity levels comparable to the most popular centralized sparse coding algorithms while being readily suited for neural implementation. Additionally, LCA coefficients for video sequences demonstrate inertial properties that are both qualitatively and quantitatively more regular (i.e., smoother and more predictable) than the coefficients produced by greedy algorithms.",Neural dynamics and brain function,Cognitive Neuroscience,
2044,W2151693816,Matching pursuits with time-frequency dictionaries,"Stéphane Mallat, Zhifeng Zhang","The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992).< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>",Machine Fault Diagnosis Techniques,Control and Systems Engineering,
2045,W2154332973,Optimally sparse representation in general (nonorthogonal) dictionaries via ℓ <sup>1</sup> minimization,"David L. Donoho, Michael Elad","Given a dictionary D = { d k } of vectors d k , we seek to represent a signal S as a linear combination S = ∑ k γ( k ) d k , with scalar coefficients γ ( k ). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the ℓ 1 norm of the coefficients γ̱. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.",Sparse and Compressive Sensing Techniques,Computational Mechanics,
2046,W2164647001,Image Denoising Via Learned Dictionaries and Sparse representation,"Michael Elad, Michal Aharon","We address the image denoising problem, where zeromean white and homogeneous Gaussian additive noise should be removed from a given image. The approach taken is based on sparse and redundant representations over a trained dictionary. The proposed algorithm denoises the image, while simultaneously trainining a dictionary on its (corrupted) content using the K-SVD algorithm. As the dictionary training algorithm is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm, with state-of-the-art performance, equivalent and sometimes surpassing recently published leading alternative denoising methods.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,
2047,W2022508996,Learning Hierarchical Features for Scene Labeling,"Clément Farabet, Camille Couprie, Laurent Najman, Yann LeCun","Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
2048,W2546302380,What is the best multi-stage architecture for object recognition?,"Kevin Jarrett, Koray Kavukcuoglu, M. Ranzato, Yann LeCun","In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (> 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%).",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
2119,W2282821441,"""Why Should I Trust You?""","Marco Túlio Ribeiro, Sameer Singh, Carlos Guestrin","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.",Explainable Artificial Intelligence (XAI),Artificial Intelligence,
2316,W1903029394,Fully convolutional networks for semantic segmentation,"Jonathan Long, Evan Shelhamer, Trevor Darrell","Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ""fully convolutional"" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
2366,W2558748708,Geometric Deep Learning: Going beyond Euclidean data,"Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre Vandergheynst","Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.",3D Shape Modeling and Analysis,Computational Mechanics,
2370,W3100777112,1D convolutional neural networks and applications: A survey,"Serkan Kıranyaz, Onur Avcı, Osama Abdeljaber, Türker İnce, Moncef Gabbouj, Daniel J. Inman","During the last decade, Convolutional Neural Networks (CNNs) have become the de facto standard for various Computer Vision and Machine Learning operations. CNNs are feed-forward Artificial Neural Networks (ANNs) with alternating convolutional and subsampling layers. Deep 2D CNNs with many hidden layers and millions of parameters have the ability to learn complex objects and patterns providing that they can be trained on a massive size visual database with ground-truth labels. With a proper training, this unique ability makes them the primary tool for various engineering applications for 2D signals such as images and video frames. Yet, this may not be a viable option in numerous applications over 1D signals especially when the training data is scarce or application specific. To address this issue, 1D CNNs have recently been proposed and immediately achieved the state-of-the-art performance levels in several applications such as personalized biomedical data classification and early diagnosis, structural health monitoring, anomaly detection and identification in power electronics and electrical motor fault detection. Another major advantage is that a real-time and low-cost hardware implementation is feasible due to the simple and compact configuration of 1D CNNs that perform only 1D convolutions (scalar multiplications and additions). This paper presents a comprehensive review of the general architecture and principals of 1D CNNs along with their major engineering applications, especially focused on the recent progress in this field. Their state-of-the-art performance is highlighted concluding with their unique properties. The benchmark datasets and the principal 1D CNN software used in those applications are also publicly shared in a dedicated website. While there has not been a paper on the review of 1D CNNs and its applications in the literature, this paper fulfills this gap.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
2371,W2949416428,Semi-Supervised Learning with Deep Generative Models,"Diederik P. Kingma, Danilo Jimenez Rezende, Shakir Mohamed, Max Welling","The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2372,W2166706236,Parallelized Stochastic Gradient Descent,"Martin Zinkevich, Markus Weimer, Lihong Li, Alex Smola","With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique — contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].",Stochastic Gradient Optimization Techniques,Artificial Intelligence,
2373,W1569098853,Optimization for Machine Learning,"Suvrit Sra, Sebastian Nowozin, Stephen J. Wright","An up-to-date account of the interplay between optimization and machine learning, accessible to students and researchers in both communities. The interplay between optimization and machine learning is one of the most important developments in modern computational science. Optimization formulations and methods are proving to be vital in designing algorithms to extract essential knowledge from huge volumes of data. Machine learning, however, is not simply a consumer of optimization technology but a rapidly evolving field that is itself generating new optimization ideas. This book captures the state of the art of the interaction between optimization and machine learning in a way that is accessible to researchers in both fields. Optimization approaches have enjoyed prominence in machine learning because of their wide applicability and attractive theoretical properties. The increasing complexity, size, and variety of today's machine learning models call for the reassessment of existing assumptions. This book starts the process of reassessment. It describes the resurgence in novel contexts of established frameworks such as first-order methods, stochastic approximations, convex relaxations, interior-point methods, and proximal methods. It also devotes attention to newer themes such as regularized optimization, robust optimization, gradient and subgradient methods, splitting techniques, and second-order methods. Many of these techniques draw inspiration from other fields, including operations research, theoretical computer science, and subfields of optimization. The book will enrich the ongoing cross-fertilization between the machine learning community and these other fields, and within the broader optimization community.",Sparse and Compressive Sensing Techniques,Computational Mechanics,
2374,W1503259811,Retrofitting Word Vectors to Semantic Lexicons,"Manaal Faruqui, Jesse Dodge, Sunil Kumar Jauhar, Chris Dyer, Eduard Hovy, Noah A. Smith","Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, Noah A. Smith. Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2015.",Natural Language Processing Techniques,Artificial Intelligence,
2403,W1832693441,Convolutional Neural Networks for Sentence Classification,Yoon Kim,"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks.We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks.Learning task-specific vectors through fine-tuning offers further gains in performance.We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors.The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",Topic Modeling,Artificial Intelligence,
2454,W2475287302,Image Style Transfer Using Convolutional Neural Networks,"Leon A. Gatys, Alexander S. Ecker, Matthias Bethge","Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2503,W1614298861,Efficient Estimation of Word Representations in Vector Space,"Tomáš Mikolov, Kai Chen, Greg S. Corrado, Jay B. Dean","We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",Topic Modeling,Artificial Intelligence,
2538,W1545319692,Riemann Manifold Langevin and Hamiltonian Monte Carlo Methods,"Mark Girolami, Ben Calderhead","Summary The paper proposes Metropolis adjusted Langevin and Hamiltonian Monte Carlo sampling methods defined on the Riemann manifold to resolve the shortcomings of existing Monte Carlo algorithms when sampling from target densities that may be high dimensional and exhibit strong correlations. The methods provide fully automated adaptation mechanisms that circumvent the costly pilot runs that are required to tune proposal densities for Metropolis–Hastings or indeed Hamiltonian Monte Carlo and Metropolis adjusted Langevin algorithms. This allows for highly efficient sampling even in very high dimensions where different scalings may be required for the transient and stationary phases of the Markov chain. The methodology proposed exploits the Riemann geometry of the parameter space of statistical models and thus automatically adapts to the local structure when simulating paths across this manifold, providing highly efficient convergence and exploration of the target density. The performance of these Riemann manifold Monte Carlo methods is rigorously assessed by performing inference on logistic regression models, log-Gaussian Cox point processes, stochastic volatility models and Bayesian estimation of dynamic systems described by non-linear differential equations. Substantial improvements in the time-normalized effective sample size are reported when compared with alternative sampling approaches. MATLAB code that is available from http://www.ucl.ac.uk/statistics/research/rmhmc allows replication of all the results reported.",Markov Chains and Monte Carlo Methods,Statistics and Probability,
2539,W156498718,A Language and Program for Complex Bayesian Modelling,"Walter R. Gilks, Andrew C. Thomas, D. J. Spiegelhalter","Gibbs sampling has enormous potential for analysing complex data sets. However, routine use of Gibbs sampling has been hampered by the lack of general purpose software for its implementation. Until now all applications have involved writing one-off computer code in low or intermediate level languages such as C or Fortran. We describe some general purpose software that we are currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling). The BUGS system comprises three components: first, a natural language for specifying complex models; second, an 'expert system' for deciding appropriate methods for obtaining samples required by the Gibbs sampler; third, a sampling module containing numerical routines to perform the sampling. S objects are used for data input and output. BUGS is written in Modula-2 and runs under both DOS and UNIX.",Time Series Analysis and Forecasting,Signal Processing,
2540,W1579579143,Contemporary Bayesian Econometrics and Statistics,John Geweke,"Preface. 1. Introduction. 1.1 Two Examples. 1.1.1 Public School Class Sizes. 1.1.2 Value at Risk. 1.2 Observables, Unobservables, and Objects of Interest. 1.3 Conditioning and Updating. 1.4 Simulators. 1.5 Modeling. 1.6 Decisionmaking. 2. Elements of Bayesian Inference. 2.1 Basics. 2.2 Sufficiency, Ancillarity, and Nuisance Parameters. 2.2.1 Sufficiency. 2.2.2 Ancillarity. 2.2.3 Nuisance Parameters. 2.3 Conjugate Prior Distributions. 2.4 Bayesian Decision Theory and Point Estimation. 2.5 Credible Sets. 2.6 Model Comparison. 2.6.1 Marginal Likelihoods. 2.6.2 Predictive Densities. 3. Topics in Bayesian Inference. 3.1 Hierarchical Priors and Latent Variables. 3.2 Improper Prior Distributions. 3.3 Prior Robustness and the Density Ratio Class. 3.4 Asymptotic Analysis. 3.5 The Likelihood Principle. 4. Posterior Simulation. 4.1 Direct Sampling,. 4.2 Acceptance and Importance Sampling. 4.2.1 Acceptance Sampling. 4.2.2 Importance Sampling. 4.3 Markov Chain Monte Carlo. 4.3.1 The Gibbs Sampler. 4.3.2 The Metropolis-Hastings Algorithm. 4.4 Variance Reduction. 4.4.1 Concentrated Expectations. 4.4.2 Antithetic Sampling. 4.5 Some Continuous State Space Markov Chain Theory. 4.5.1 Convergence of the Gibbs Sampler. 4.5.2 Convergence of the Metropolis-Hastings Algorithm. 4.6 Hybrid Markov Chain Monte Carlo Methods. 4.6.1 Transition Mixtures. 4.6.2 Metropolis within Gibbs. 4.7 Numerical Accuracy and Convergence in Markov Chain Monte Carlo. 5. Linear Models. 5.1 BACC and the Normal Linear Regression Model. 5.2 Seemingly Unrelated Regressions Models. 5.3 Linear Constraints in the Linear Model. 5.3.1 Linear Inequality Constraints. 5.3.2 Conjectured Linear Restrictions, Linear Inequality Constraints, and Covariate Selection. 5.4 Nonlinear Regression. 5.4.1 Nonlinear Regression with Smoothness Priors. 5.4.2 Nonlinear Regression with Basis Functions. 6. Modeling with Latent Variables. 6.1 Censored Normal Linear Models. 6.2 Probit Linear Models. 6.3 The Independent Finite State Model. 6.4 Modeling with Mixtures of Normal Distributions. 6.4.1 The Independent Student-t Linear Model. 6.4.2 Normal Mixture Linear Models. 6.4.3 Generalizing the Observable Outcomes. 7. Modeling for Time Series. 7.1 Linear Models with Serial Correlation. 7.2 The First-Order Markov Finite State Model. 7.2.1 Inference in the Nonstationary Model. 7.2.2 Inference in the Stationary Model. 7.3 Markov Normal Mixture Linear Model. 8. Bayesian Investigation. 8.1 Implementing Simulation Methods. 8.1.1 Density Ratio Tests. 8.1.2 Joint Distribution Tests. 8.2 Formal Model Comparison. 8.2.1 Bayes Factors for Modeling with Common Likelihoods. 8.2.2 Marginal Likelihood Approximation Using Importance Sampling. 8.2.3 Marginal Likelihood Approximation Using Gibbs Sampling. 8.2.4 Density Ratio Marginal Likelihood Approximation. 8.3 Model Specification. 8.3.1 Prior Predictive Analysis. 8.3.2 Posterior Predictive Analysis. 8.4 Bayesian Communication. 8.5 Density Ratio Robustness Bounds. Bibliography. Author Index. Subject Index.",Statistical Methods and Bayesian Inference,Statistics and Probability,
2541,W1934017349,Dynamic Trees: A Structured Variational Method Giving Efficient Propagation Rules,Amos Storkey,"Dynamic trees are mixtures of tree structured belief networks. They solve some of the problems of fixed tree networks at the cost of making exact inference intractable. For this reason approximate methods such as sampling or mean field approaches have been used. However, mean field approximations assume a factorized distribution over node states. Such a distribution seems unlickely in the posterior, as nodes are highly correlated in the prior. Here a structured variational approach is used, where the posterior distribution over the non-evidential nodes is itself approximated by a dynamic tree. It turns out that this form can be used tractably and efficiently. The result is a set of update rules which can propagate information through the network to obtain both a full variational approximation, and the relevant marginals. The progagtion rules are more efficient than the mean field approach and give noticeable quantitative and qualitative improvement in the inference. The marginals calculated give better approximations to the posterior than loopy propagation on a small toy problem.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
2543,W1992208280,Robust Stochastic Approximation Approach to Stochastic Programming,"Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro","In this paper we consider optimization problems where the objective function is given in a form of the expectation. A basic difficulty of solving such stochastic optimization problems is that the involved multidimensional integrals (expectations) cannot be computed with high accuracy. The aim of this paper is to compare two computational approaches based on Monte Carlo sampling techniques, namely, the stochastic approximation (SA) and the sample average approximation (SAA) methods. Both approaches, the SA and SAA methods, have a long history. Current opinion is that the SAA method can efficiently use a specific (say, linear) structure of the considered problem, while the SA approach is a crude subgradient method, which often performs poorly in practice. We intend to demonstrate that a properly modified SA approach can be competitive and even significantly outperform the SAA method for a certain class of convex stochastic problems. We extend the analysis to the case of convex-concave stochastic saddle point problems and present (in our opinion highly encouraging) results of numerical experiments.",Risk and Portfolio Optimization,Management Science and Operations Research,
2544,W2059424427,Explaining Variational Approximations,"John T. Ormerod, M. P. Wand","Variational approximations facilitate approximate inference for the parameters in complex statistical models and provide fast, deterministic alternatives to Monte Carlo methods. However, much of the contemporary literature on variational approximations is in Computer Science rather than Statistics, and uses terminology, notation, and examples from the former field. In this article we explain variational approximation in statistical terms. In particular, we illustrate the ideas of variational approximation using examples that are familiar to statisticians.",Statistical Methods and Inference,Statistics and Probability,
2545,W2951595529,Variational Dropout and the Local Reparameterization Trick,"Diederik P. Kingma, Tim Salimans, Max Welling","We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,
2546,W4206566734,An Introduction to Variational Autoencoders,"Diederik P. Kingma, Max Welling","Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models.In this work, we provide an introduction to variational autoencoders and some important extensions.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2547,W2592505114,Multiplicative Normalizing Flows for Variational Bayesian Neural Networks,"Christos Louizos, Max Welling",We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.,Gaussian Processes and Bayesian Inference,Artificial Intelligence,
2620,W2419501139,f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization,"Sebastian Nowozin, Botond Cseke, Ryota Tomioka","Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.",Adversarial Robustness in Machine Learning,Artificial Intelligence,
2648,W2577537660,<i>Stan</i>: A Probabilistic Programming Language,"Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel C. Lee, Ben Goodrich, Michael Betancourt, Marcus A. Brubaker, Jiqiang Guo, Peter Li, Allen Riddell","Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.",Statistical Methods and Bayesian Inference,Statistics and Probability,
2801,W2759136286,Knowledge Graph Embedding: A Survey of Approaches and Applications,"Quan Wang, Zhendong Mao, Bin Wang, Li Guo","Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.",Advanced Graph Neural Networks,Artificial Intelligence,
2867,W1542941005,"The Jumping Frog: In English, Then in French, Then Clawed Back Into a Civilized Language Once More by Patient, Unremunerated Toil",Mark Twain,"Mark Twain's Notorious Jumping Frog of Calaveras County is a story of the crusty Jim Smiley, a man who so loved to bet on animals - horses, dogs, etc. - that he trained a frog to be the strongest jumper in his county. Twain's broad yet graceful humor is beautifully complemented by the elegant woodcuts of Alan James Robinson. Finely reproduced, these illustrations bring Twain's great comic tale to life. Twain's classic tale of the Notorious Jumping Frog of Calaveras County is in three parts: the original tale published in 1865, the first French translation of the story, and Twain's tongue-in-cheek verbatim re-translation into English.",American Literature and Humor Studies,Literature and Literary Theory,
2868,W1896934482,Modeling object appearance using Context-Conditioned Component Analysis,"Daniyar Turmukhambetov, Neill D. F. Campbell, Simon J. D. Prince, Jan Kautz","Subspace models have been very successful at modeling the appearance of structured image datasets when the visual objects have been aligned in the images (e.g., faces). Even with extensions that allow for global transformations or dense warps of the image, the set of visual objects whose appearance may be modeled by such methods is limited. They are unable to account for visual objects where occlusion leads to changing visibility of different object parts (without a strict layered structure) and where a one-to-one mapping between parts is not preserved. For example bunches of bananas contain different numbers of bananas but each individual banana shares an appearance subspace. In this work we remove the image space alignment limitations of existing subspace models by conditioning the models on a shape dependent context that allows for the complex, non-linear structure of the appearance of the visual object to be captured and shared. This allows us to exploit the advantages of subspace appearance models with non-rigid, deformable objects whilst also dealing with complex occlusions and varying numbers of parts. We demonstrate the effectiveness of our new model with examples of structured inpainting and appearance transfer.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2869,W1905829557,"Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture","David Eigen, Rob Fergus","In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.",Advanced Vision and Imaging,Computer Vision and Pattern Recognition,
2870,W1926639317,"FlowWeb: Joint image set alignment by weaving consistent, pixel-wise correspondences","Tinghui Zhou, Yong Jae Lee, Stella X. Yu, Alexei A. Efros","Given a set of poorly aligned images of the same visual concept without any annotations, we propose an algorithm to jointly bring them into pixel-wise correspondence by estimating a FlowWeb representation of the image set. FlowWeb is a fully-connected correspondence flow graph with each node representing an image, and each edge representing the correspondence flow field between a pair of images, i.e. a vector field indicating how each pixel in one image can find a corresponding pixel in the other image. Correspondence flow is related to optical flow but allows for correspondences between visually dissimilar regions if there is evidence they correspond transitively on the graph. Our algorithm starts by initializing all edges of this complete graph with an off-the-shelf, pairwise flow method. We then iteratively update the graph to force it to be more self-consistent. Once the algorithm converges, dense, globally-consistent correspondences can be read off the graph. Our results suggest that FlowWeb improves alignment accuracy over previous pairwise as well as joint alignment methods.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
2871,W2019969451,Data-driven hallucination of different times of day from a single outdoor photo,"YiChang Shih, Sylvain Paris, Frédo Durand, William T. Freeman","We introduce ""time hallucination"": synthesizing a plausible image at a different time of day from an input image. This challenging task often requires dramatically altering the color appearance of the picture. In this paper, we introduce the first data-driven approach to automatically creating a plausible-looking photo that appears as though it were taken at a different time of day. The time of day is specified by a semantic time label, such as ""night"". Our approach relies on a database of time-lapse videos of various scenes. These videos provide rich information about the variations in color appearance of a scene throughout the day. Our method transfers the color appearance from videos with a similar scene as the input photo. We propose a locally affine model learned from the video for the transfer, allowing our model to synthesize new color data while retaining image details. We show that this model can hallucinate a wide range of different times of day. The model generates a large sparse linear system, which can be solved by off-the-shelf solvers. We validate our methods by synthesizing transforming photos of various outdoor scenes to four times of interest: daytime, the golden hour, the blue hour, and nighttime.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,
2872,W2058017216,Disambiguating visual relations using loop constraints,"Christopher Zach, Manfred Klopschitz, Marc Pollefeys","Repetitive and ambiguous visual structures in general pose a severe problem in many computer vision applications. Identification of incorrect geometric relations between images solely based on low level features is not always possible, and a more global reasoning approach about the consistency of the estimated relations is required. We propose to utilize the typically observed redundancy in the hypothesized relations for such reasoning, and focus on the graph structure induced by those relations. Chaining the (reversible) transformations over cycles in this graph allows to build suitable statistics for identifying inconsistent loops in the graph. This data provides indirect evidence for conflicting visual relations. Inferring the set of likely false positive geometric relations from these non-local observations is formulated in a Bayesian framework. We demonstrate the utility of the proposed method in several applications, most prominently the computation of structure and motion from images.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
2873,W2083366168,Transient attributes for high-level understanding and editing of outdoor scenes,"Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao, Chao Qian, James Hays","We live in a dynamic visual world where the appearance of scenes changes dramatically from hour to hour or season to season. In this work we study ""transient scene attributes"" -- high level properties which affect scene appearance, such as ""snow"", ""autumn"", ""dusk"", ""fog"". We define 40 transient attributes and use crowdsourcing to annotate thousands of images from 101 webcams. We use this ""transient attribute database"" to train regressors that can predict the presence of attributes in novel images. We demonstrate a photo organization method based on predicted attributes. Finally we propose a high-level image editing method which allows a user to adjust the attributes of a scene, e.g. change a scene to be ""snowy"" or ""sunset"". To support attribute manipulation we introduce a novel appearance transfer technique which is simple and fast yet competitive with the state-of-the-art. We show that we can convincingly modify many transient attributes in outdoor scenes.",Image Enhancement Techniques,Computer Vision and Pattern Recognition,
2874,W2116013899,Texture synthesis by non-parametric sampling,"Alexei A. Efros, Thomas Leung","A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.",Image Retrieval and Classification Techniques,Computer Vision and Pattern Recognition,
2875,W2118557299,Image Co-segmentation via Consistent Functional Maps,"Fan Wang, Qixing Huang, Leonidas Guibas","Joint segmentation of image sets has great importance for object recognition, image classification, and image retrieval. In this paper, we aim to jointly segment a set of images starting from a small number of labeled images or none at all. To allow the images to share segmentation information with each other, we build a network that contains segmented as well as unsegmented images, and extract functional maps between connected image pairs based on image appearance features. These functional maps act as general property transporters between the images and, in particular, are used to transfer segmentations. We define and operate in a reduced functional space optimized so that the functional maps approximately satisfy cycle-consistency under composition in the network. A joint optimization framework is proposed to simultaneously generate all segmentation functions over the images so that they both align with local segmentation cues in each particular image, and agree with each other under network transportation. This formulation allows us to extract segmentations even with no training data, but can also exploit such data when available. The collective effect of the joint processing using functional maps leads to accurate information sharing among images and yields superior segmentation results, as shown on the iCoseg, MSRC, and PASCAL data sets.",Visual Attention and Saliency Detection,Computer Vision and Pattern Recognition,
2876,W2143133882,Consistent shape maps via semidefinite programming,"Qixing Huang, Leonidas Guibas","Recent advances in shape matching have shown that jointly optimizing the maps among the shapes in a collection can lead to significant improvements when compared to estimating maps between pairs of shapes in isolation. These methods typically invoke a cycle-consistency criterion --- the fact that compositions of maps along a cycle of shapes should approximate the identity map. This condition regularizes the network and allows for the correction of errors and imperfections in individual maps. In particular, it encourages the estimation of maps between dissimilar shapes by compositions of maps along a path of more similar shapes.In this paper, we introduce a novel approach for obtaining consistent shape maps in a collection that formulates the cycle-consistency constraint as the solution to a semidefinite program (SDP). The proposed approach is based on the observation that, if the ground truth maps between the shapes are cycle-consistent, then the matrix that stores all pair-wise maps in blocks is low-rank and positive semidefinite. Motivated by recent advances in techniques for low-rank matrix recovery via semidefinite programming, we formulate the problem of estimating cycle-consistent maps as finding the closest positive semidefinite matrix to an input matrix that stores all the initial maps. By analyzing the Karush-Kuhn-Tucker (KKT) optimality condition of this program, we derive theoretical guarantees for the proposed algorithm, ensuring the correctness of the recovery when the errors in the inputs maps do not exceed certain thresholds. Besides this theoretical guarantee, experimental results on benchmark datasets show that the proposed approach outperforms state-of-the-art multiple shape matching methods.",3D Shape Modeling and Analysis,Computational Mechanics,
2877,W2157575844,Back-Translation for Cross-Cultural Research,Richard W. Brislin,"Two aspects of translation were investigated: (1) factors that affect translation quality, and (2) how equivalence between source and target versions can be evaluated. The variables of language, content, and difficulty were studied through an analysis of variance design. Ninety-four bilinguals from the University of Guam, representing ten languages, translated or back-translated six essays incorporating three content areas and two levels of difficulty. The five criteria for equivalence were based on comparisons of meaning or predictions of similar responses to original or translated versions. The factors of content, difficulty, language and content-language interaction were significant, and the five equivalence criteria proved workable. Conclusions are that translation quality can be predicted, and that a functionally equivalent translation can be demonstrated when responses to the original and target versions are studied.",Natural Language Processing Techniques,Artificial Intelligence,
2878,W2963800363,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,"Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro","We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 × 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2879,W2963767194,StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation,"Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo","Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2880,W2765811365,Generative Adversarial Networks: An Overview,"Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A. Bharath","Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2881,W2962974533,Semantic Image Synthesis With Spatially-Adaptive Normalization,"Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu","We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the network, forcing the network to memorize the information throughout all the layers. Instead, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned affine transformation. Experiments on several challenging datasets demonstrate the superiority of our method compared to existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows users to easily control the style and content of image synthesis results as well as create multi-modal results. Code is available upon publication.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2883,W2775795276,The Effectiveness of Data Augmentation in Image Classification using Deep Learning,"Luis Perez, Jason Wang","In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2884,W2963444790,DualGAN: Unsupervised Dual Learning for Image-to-Image Translation,"Zili Yi, Hao Zhang, Ping Tan, Minglun Gong","Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently [7, 8, 21, 12, 4, 18]. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation [23], we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
2886,W2963995996,End-to-End Recovery of Human Shape and Pose,"Angjoo Kanazawa, Michael J. Black, David W. Jacobs, Jitendra Malik","We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.",Human Pose and Action Recognition,Computer Vision and Pattern Recognition,
2897,W1527575280,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,"Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel","Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - ""blue"" + ""red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
2953,W1032684693,Context as Supervisory Signal: Discovering Objects with Predictable Context,"Carl Doersch, Abhinav Gupta, Alexei A. Efros","This paper addresses the well-established problem of unsupervised object discovery with a novel method inspired by weakly-supervised approaches. In particular, the ability of an object patch to predict the rest of the object (its context) is used as supervisory signal to help discover visually consistent object clusters. The main contributions of this work are: 1) framing unsupervised clustering as a leave-one-out context prediction task; 2) evaluating the quality of context prediction by statistical hypothesis testing between thing and stuff appearance models; and 3) an iterative region prediction and context alignment approach that gradually discovers a visual object cluster together with a segmentation mask and fine-grained correspondences. The proposed method outperforms previous unsupervised as well as weakly-supervised object discovery approaches, and is shown to provide correspondences detailed enough to transfer keypoint annotations.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
2962,W1993120651,PatchMatch,"Connelly Barnes, Eli Shechtman, Adam Finkelstein, Dan B Goldman","This paper presents interactive image editing tools using a new randomized algorithm for quickly finding approximate nearest-neighbor matches between image patches. Previous research in graphics and vision has leveraged such nearest-neighbor searches to provide a variety of high-level digital image editing tools. However, the cost of computing a field of such matches for an entire image has eluded previous efforts to provide interactive performance. Our algorithm offers substantial performance improvements over the previous state of the art (20-100x), enabling its use in interactive editing tools. The key insights driving the algorithm are that some good patch matches can be found via random sampling, and that natural coherence in the imagery allows us to propagate such matches quickly to surrounding areas. We offer theoretical analysis of the convergence properties of the algorithm, as well as empirical and practical evidence for its high quality and performance. This one simple algorithm forms the basis for a variety of tools -- image retargeting, completion and reshuffling -- that can be used together in the context of a high-level image editing application. Finally, we propose additional intuitive constraints on the synthesis process that offer the user a level of control unavailable in previous methods.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
2969,W2055132753,What makes Paris look like Paris?,"Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Šivic, Alexei A. Efros","Given a large repository of geotagged imagery, we seek to automatically find visual elements, e. g. windows, balconies, and street signs, that are most distinctive for a certain geo-spatial area, for example the city of Paris. This is a tremendously difficult task as the visual features distinguishing architectural elements of different places can be very subtle. In addition, we face a hard search problem: given all possible patches in all images, which of them are both frequently occurring and geographically informative? To address these issues, we propose to use a discriminative clustering approach able to take into account the weak geographic supervision. We show that geographically representative image elements can be discovered automatically from Google Street View imagery in a discriminative manner. We demonstrate that these elements are visually interpretable and perceptually geo-informative. The discovered visual elements can also support a variety of computational geography tasks, such as mapping architectural correspondences and influences within and across cities, finding representative elements at different geo-spatial scales, and geographically-informed image retrieval.",Remote-Sensing Image Classification,Media Technology,
3136,W3043547428,Generative Image Inpainting with Contextual Attention,"Jiahui Yu, Zhe Lin, Shuicheng Yan, Xiaohui Shen, Xin Lu, Thomas S. Huang","Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image. These methods can generate visually plausible image structures and textures, but often create distorted structures or blurry textures inconsistent with surrounding areas. This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations. On the other hand, traditional texture and patch synthesis approaches are particularly suitable when it needs to borrow textures from the surrounding regions. Motivated by these observations, we propose a new deep generative model-based approach which can not only synthesize novel image structures but also explicitly utilize surrounding image features as references during network training to make better predictions. The model is a feedforward, fully convolutional neural network which can process images with multiple holes at arbitrary locations and with variable sizes during the test time. Experiments on multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and natural images (ImageNet, Places2) demonstrate that our proposed approach generates higher-quality inpainting results than existing ones. Code, demo and models are available at: https://github.com/JiahuiYu/generative_inpainting.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3157,W4390873054,Adding Conditional Control to Text-to-Image Diffusion Models,"Lvmin Zhang, Anyi Rao, Maneesh Agrawala","We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with ""zero convolutions"" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, e.g., edges, depth, segmentation, human pose, etc., with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small (<50k) and large (>1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.",Advanced Neuroimaging Techniques and Applications,"Radiology, Nuclear Medicine and Imaging",
3163,W2405756170,Generative Adversarial Text to Image Synthesis,"Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, Honglak Lee","Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image model- ing, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3166,W2521028896,Energy-based Generative Adversarial Network,"Junbo Zhao, Michaël Mathieu, Yann LeCun","We introduce the ""Energy-based Generative Adversarial Network"" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3170,W2566832195,Stacked Generative Adversarial Networks,"Xun Huang, Yixuan Li, Omid Poursaeed, John E. Hopcroft, Serge Belongie","In this paper, we propose a novel generative model named Stacked Generative Adversarial Networks (SGAN), which is trained to invert the hierarchical representations of a bottom-up discriminative network. Our model consists of a top-down stack of GANs, each learned to generate lower-level representations conditioned on higher-level representations. A representation discriminator is introduced at each feature hierarchy to encourage the representation manifold of the generator to align with that of the bottom-up discriminative network, leveraging the powerful discriminative representations to guide the generative model. In addition, we introduce a conditional loss that encourages the use of conditional information from the layer above, and a novel entropy loss that maximizes a variational lower bound on the conditional entropy of generator outputs. We first train each stack independently, and then train the whole model end-to-end. Unlike the original GAN that uses a single noise vector to represent all the variations, our SGAN decomposes variations into multiple levels and gradually resolves uncertainties in the top-down generative process. Based on visual inspection, Inception scores and visual Turing test, we demonstrate that SGAN is able to generate images of much higher quality than GANs without stacking.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3173,W3121661546,EnlightenGAN: Deep Light Enhancement Without Paired Supervision,"Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Fang Chen, Xiaohui Shen, Shuicheng Yan, Pan Zhou, Zhangyang Wang","Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and the attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. Our codes and pre-trained models are available at: https://github.com/VITA-Group/EnlightenGAN.",Image Enhancement Techniques,Computer Vision and Pattern Recognition,
3195,W2545656684,A Learned Representation For Artistic Style,"Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur","The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.",Aesthetic Perception and Analysis,Cognitive Neuroscience,
3253,W1786904711,Disentangling Factors of Variation via Generative Entangling,"Guillaume Desjardins, Aaron Courville, Yoshua Bengio","Here we propose a novel model family with the objective of learning to disentangle the factors of variation in data. Our approach is based on the spike-and-slab restricted Boltzmann machine which we generalize to include higher-order interactions among multiple latent variables. Seen from a generative perspective, the multiplicative interactions emulates the entangling of factors of variation. Inference in the model can be seen as disentangling these generative factors. Unlike previous attempts at disentangling latent factors, the proposed model is trained using no supervised information regarding the latent factors. We apply our model to the task of facial expression classification.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3254,W2087681821,One millisecond face alignment with an ensemble of regression trees,"Vahid Kazemi, Josephine Sullivan","This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.",Face recognition and analysis,Computer Vision and Pattern Recognition,
3255,W2112594540,Improving generalization performance using double backpropagation,"Harris Drucker, Y. Le Cun","In order to generalize from a training set to a test set, it is desirable that small changes in the input space of a pattern do not change the output components. This can be done by forcing this behavior as part of the training algorithm. This is done in double backpropagation by forming an energy function that is the sum of the normal energy term found in backpropagation and an additional term that is a function of the Jacobian. Significant improvement is shown with different architectures and different test sets, especially with architectures that had previously been shown to have very good performance when trained using backpropagation. It is shown that double backpropagation, as compared to backpropagation, creates weights that are smaller, thereby causing the output of the neurons to spend more time in the linear region.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>",Neural Networks and Applications,Artificial Intelligence,
3257,W2583638424,Demystifying Neural Style Transfer,"Yanghao Li, Naiyan Wang, Jiaying Liu, Xiaodi Hou","Neural Style Transfer has recently demonstrated very exciting results which catches eyes in both academia and industry. Despite the amazing results, the principle of neural style transfer, especially why the Gram matrices could represent style remains unclear. In this paper, we propose a novel interpretation of neural style transfer by treating it as a domain adaptation problem. Specifically, we theoretically show that matching the Gram matrices of feature maps is equivalent to minimize the Maximum Mean Discrepancy (MMD) with the second order polynomial kernel. Thus, we argue that the essence of neural style transfer is to match the feature distributions between the style images and the generated images. To further support our standpoint, we experiment with several other distribution alignment methods, and achieve appealing results. We believe this novel interpretation connects these two important research fields, and could enlighten future researches.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3258,W2617602381,Universal Style Transfer via Feature Transforms,"Yijun Li, Fang Chen, Shuicheng Yan, Zhaowen Wang, Xin Lü, Ming–Hsuan Yang","Universal style transfer aims to transfer arbitrary visual styles to content images. Existing feed-forward based methods, while enjoying the inference efficiency, are mainly limited by inability of generalizing to unseen styles or compromised visual quality. In this paper, we present a simple yet effective method that tackles these limitations without training on any pre-defined styles. The key ingredient of our method is a pair of feature transforms, whitening and coloring, that are embedded to an image reconstruction network. The whitening and coloring transforms reflect a direct matching of feature covariance of the content image to a given style image, which shares similar spirits with the optimization of Gram matrix based cost in neural style transfer. We demonstrate the effectiveness of our algorithm by generating high-quality stylized images with comparisons to a number of recent methods. We also analyze our method by visualizing the whitened features and synthesizing textures via simple feature coloring.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3259,W2622255292,On the Emergence of Invariance and Disentangling in Deep Representations.,"Alessandro Achille, Stefano Soatto","Using classical notions of statistical decision and information theory, we show that invariance in a deep neural network is equivalent to minimality of the representation it computes, and can be achieved by stacking layers and injecting noise in the computation, under realistic and empirically validated assumptions. We use an Information Decomposition of the empirical loss to show that overfitting can be reduced by limiting the information content stored in the weights. We then present a sharp inequality that relates the information content in the weights -- which are a representation of the training set and inferred by generic optimization agnostic of invariance and disentanglement -- and the minimality and total correlation of the activation functions, which are a representation of the test datum. This allows us to tackle recent puzzles concerning the generalization properties of deep networks and their relation to the geometry of the optimization residual.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3260,W2753738274,beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,"Irina Higgins, Löıc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, Alexander Lerchner","Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta > 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3261,W2768346313,Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients,"Andrew Slavin Ross, Finale Doshi‐Velez","Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more ""legitimate,"" interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.",Adversarial Robustness in Machine Learning,Artificial Intelligence,
3262,W2785519580,Disentangling by Factorising,"Hyunjik Kim, Andriy Mnih","We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon $\beta$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.",Digital Media Forensic Detection,Computer Vision and Pattern Recognition,
3263,W2785678896,Spectral Normalization for Generative Adversarial Networks,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida","One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3264,W2785961484,A framework for the quantitative evaluation of disentangled representations,"Cian Eastwood, Christopher K. I. Williams","Recent AI research has emphasized the importance of learning disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by a state-of-the-art model (InfoGAN) and those learned by a baseline model (PCA).",Adversarial Robustness in Machine Learning,Artificial Intelligence,
3265,W3035574324,Analyzing and Improving the Image Quality of StyleGAN,"Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila","The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign the generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent codes to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably attribute a generated image to a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3267,W3162926177,Diffusion Models Beat GANs on Image Synthesis,"Prafulla Dhariwal, Alex Nichol","We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet 256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our code at https://github.com/openai/guided-diffusion",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3268,W3023371261,Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey,"Longlong Jing, Yingli Tian","Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
3269,W3176923149,Text Data Augmentation for Deep Learning,"Connor Shorten, Taghi M. Khoshgoftaar, Borko Furht","Natural Language Processing (NLP) is one of the most captivating applications of Deep Learning. In this survey, we consider how the Data Augmentation training strategy can aid in its development. We begin with the major motifs of Data Augmentation summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form. We follow these motifs with a concrete list of augmentation frameworks that have been developed for text data. Deep Learning generally struggles with the measurement of generalization and characterization of overfitting. We highlight studies that cover how augmentations can construct test sets for generalization. NLP is at an early stage in applying Data Augmentation compared to Computer Vision. We highlight the key differences and promising ideas that have yet to be tested in NLP. For the sake of practical implementation, we describe tools that facilitate Data Augmentation such as the use of consistency regularization, controllers, and offline and online augmentation pipelines, to preview a few. Finally, we discuss interesting topics around Data Augmentation in NLP such as task-specific augmentations, the use of prior knowledge in self-supervised learning versus Data Augmentation, intersections with transfer and multi-task learning, and ideas for AI-GAs (AI-Generating Algorithms). We hope this paper inspires further research interest in Text Data Augmentation.",Topic Modeling,Artificial Intelligence,
3275,W2117130368,A unified architecture for natural language processing,"Ronan Collobert, Jason Weston","We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.",Natural Language Processing Techniques,Artificial Intelligence,
3361,W2962785568,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,"Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang","While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ""perceptual losses""? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,
3423,W1836533770,Unsupervised Learning of Spatiotemporally Coherent Metrics,"Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, Yann LeCun","Current state-of-the-art classification and detection algorithms train deep convolutional networks using labeled data. In this work we study unsupervised feature learning with convolutional networks in the context of temporally coherent unlabeled data. We focus on feature learning from unlabeled video data, using the assumption that adjacent video frames contain semantically similar information. This assumption is exploited to train a convolutional pooling auto-encoder regularized by slowness and sparsity priors. We establish a connection between slow feature learning and metric learning. Using this connection we define ""temporal coherence"" -- a criterion which can be used to set hyper-parameters in a principled and automated manner. In a transfer learning experiment, we show that the resulting encoder can be used to define a more semantically coherent metric without the use of labels.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,
3442,W2580360036,Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities,Guo-Jun Qi,"In this paper, we present the Lipschitz regularization theory and algorithms for a novel Loss-Sensitive Generative Adversarial Network (LS-GAN). Specifically, it trains a loss function to distinguish between real and fake samples by designated margins, while learning a generator alternately to produce realistic samples by minimizing their losses. The LS-GAN further regularizes its loss function with a Lipschitz regularity condition on the density of real data, yielding a regularized model that can better generalize to produce new data from a reasonable number of training examples than the classic GAN. We will further present a Generalized LS-GAN (GLS-GAN) and show it contains a large family of regularized GAN models, including both LS-GAN and Wasserstein GAN, as its special cases. Compared with the other GAN models, we will conduct experiments to show both LS-GAN and GLS-GAN exhibit competitive ability in generating new images in terms of the Minimum Reconstruction Error (MRE) assessed on a separate test set. We further extend the LS-GAN to a conditional form for supervised and semi-supervised learning problems, and demonstrate its outstanding performance on image classification tasks.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
3449,W3013529009,Deep Learning for Image Super-Resolution: A Survey,"Zhihao Wang, Jian Chen, Steven C. H. Hoi","Image Super-Resolution (SR) is an important class of image processing techniqueso enhance the resolution of images and videos in computer vision. Recent years have witnessed remarkable progress of image super-resolution using deep learning techniques. This article aims to provide a comprehensive survey on recent advances of image super-resolution using deep learning approaches. In general, we can roughly group the existing studies of SR techniques into three major categories: supervised SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other important issues, such as publicly available benchmark datasets and performance evaluation metrics. Finally, we conclude this survey by highlighting several future directions and open issues which should be further addressed by the community in the future.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,
3500,W1485009520,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,"Xingjian Shi, Zhourong Chen, Hao Wang, Dit‐Yan Yeung, Wai Kin Wong, Wang‐chun Woo","The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",Meteorological Phenomena and Simulations,Atmospheric Science,
3501,W1520997877,Learning to See by Moving,"Pulkit Agrawal, João Carreira, Jitendra Malik","The current dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it also possible to learn features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigated if the awareness of egomotion(i.e. self motion) can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,
3506,W1947481528,Long-term recurrent convolutional networks for visual recognition and description,"Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell, Kate Saenko","Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or ""temporally deep"", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are ""doubly deep"" in that they can be compositional in spatial and temporal ""layers"". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3554,W1586939924,Describing Videos by Exploiting Temporal Structure,"Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville","Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3555,W1591801644,Recurrent Neural Network Regularization,"Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals","We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.",Neural Networks and Applications,Artificial Intelligence,
3556,W1687846465,Composing Simple Image Descriptions using Web-scale N-grams,"Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, Yejin Choi","Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -- making for more human-like annotations than previous approaches.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3557,W1858383477,Corpus-Guided Sentence Generation of Natural Images,"Yezhou Yang, Ching L. Teo, Hal Daumé, Yiannis Aloimonos","We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3558,W1905882502,Deep visual-semantic alignments for generating image descriptions,"Andrej Karpathy, Li Fei-Fei","We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3559,W1931639407,From captions to visual concepts and back,"Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K. Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John Platt, C. Lawrence Zitnick, Geoffrey Zweig","This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3560,W1969616664,BabyTalk: Understanding and Generating Simple Image Descriptions,"Girish Kulkarni, Visruth Premraj, Vicente Ordóñez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, Tamara L. Berg","We present a system to automatically generate natural language descriptions from images. This system consists of two parts. The first part, content planning, smooths the output of computer vision-based detection and recognition algorithms with statistics mined from large pools of visually descriptive text to determine the best content words to use to describe an image. The second step, surface realization, chooses words to construct natural language sentences based on the predicted content and general statistics from natural language. We present multiple approaches for the surface realization step and evaluate each using automatic measures of similarity to human generated reference descriptions. We also collect forced choice human evaluations between descriptions from the proposed generation system and descriptions from competing approaches. The proposed system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3562,W1902237438,Effective Approaches to Attention-based Neural Machine Translation,"Thang Luong, Hieu Pham, Christopher D. Manning","An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there has been little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",Natural Language Processing Techniques,Artificial Intelligence,
3563,W603908379,Spatial transformer networks,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu","Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3564,W2745461083,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Jay Gould, Lei Zhang","Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3565,W2470673105,Hierarchical Attention Networks for Document Classification,"Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy","Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",Topic Modeling,Artificial Intelligence,
3635,W2185175083,From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,"Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier","We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
3726,W2962739339,Deep Contextualized Word Representations,"Matthew E. Peters, Mark E Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer","Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",Topic Modeling,Artificial Intelligence,
3773,W4210257598,A Comprehensive Survey on Graph Neural Networks,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S. Yu","Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",Advanced Graph Neural Networks,Artificial Intelligence,
3844,W2011181254,An Iterative Regularization Method for Total Variation-Based Image Restoration,"Stanley Osher, Martin Burger, Donald Goldfarb, Jinjun Xu, Wotao Yin","We introduce a new iterative regularization procedure for inverse problems based on the use of Bregman distances, with particular focus on problems arising in image processing. We are motivated by the problem of restoring noisy and blurry images via variational methods by using total variation regularization. We obtain rigorous convergence results and effective stopping criteria for the general procedure. The numerical results for denoising appear to give significant improvement over standard models, and preliminary results for deblurring/denoising are very encouraging.",Numerical methods in inverse problems,Mathematical Physics,
3845,W2074551195,Modeling Natural Images Using Gated MRFs,"Marc’Aurelio Ranzato, Volodymyr Mnih, Joshua M. Susskind, Geoffrey E. Hinton","This paper describes a Markov Random Field for real-valued image modeling that has two sets of latent variables. One set is used to gate the interactions between all pairs of pixels, while the second set determines the mean intensities of each pixel. This is a powerful model with a conditional distribution over the input that is Gaussian, with both mean and covariance determined by the configuration of latent variables, which is unlike previous models that were restricted to using Gaussians with either a fixed mean or a diagonal covariance matrix. Thanks to the increased flexibility, this gated MRF can generate more realistic samples after training on an unconstrained distribution of high-resolution natural images. Furthermore, the latent variables of the model can be inferred efficiently and can be used as very effective descriptors in recognition tasks. Both generation and discrimination drastically improve as layers of binary latent variables are added to the model, yielding a hierarchical model called a Deep Belief Network.",Image Retrieval and Classification Techniques,Computer Vision and Pattern Recognition,
3846,W2798991696,Unsupervised Feature Learning via Non-parametric Instance Discrimination,"Zhirong Wu, Yuanjun Xiong, Stella X. Yu, Dahua Lin","Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,
4038,W2166944917,Estimating Divergence Functionals and the Likelihood Ratio by Convex Risk Minimization,"XuanLong Nguyen, Martin J. Wainwright, Michael I. Jordan","We develop and analyze <i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">M</i> -estimation methods for divergence functionals and the likelihood ratios of two probability distributions. Our method is based on a nonasymptotic variational characterization of <i xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">f</i> -divergences, which allows the problem of estimating divergences to be tackled via convex empirical risk optimization. The resulting estimators are simple to implement, requiring only the solution of standard convex programs. We present an analysis of consistency and convergence for these estimators. Given conditions only on the ratios of densities, we show that our estimators can achieve optimal minimax rates for the likelihood ratio and the divergence functionals in certain regimes. We derive an efficient optimization algorithm for computing our estimates, and illustrate their convergence behavior and practical viability by simulations.",Statistical Methods and Inference,Statistics and Probability,
4039,W2554314924,Unrolled Generative Adversarial Networks,"Luke Metz, Ben Poole, David Pfau, Jascha Sohl‐Dickstein","We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.",Anomaly Detection Techniques and Applications,Artificial Intelligence,
4040,W2557449848,Plug &amp; Play Generative Networks: Conditional Iterative Generation of Images in Latent Space,"Anh‐Tu Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski","Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [37] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 × 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models Plug and Play Generative Networks. PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable condition network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [40], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
4041,W2585630030,Mode Regularized Generative Adversarial Networks,"Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, Wenjie Li","Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,
4042,W2613718673,Faster R-CNN: towards real-time object detection with region proposal networks,"Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun","State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster_rcnn.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
4209,W113489683,A method of computing generalized Bayesian probability values for expert systems,Peter Cheeseman,"This paper presents a new method for calculating the conditional probability of any multi-valued predicate given particular information about the individual case. This calculation is based on the principle of Maximum Entropy (ME), sometimes called the principle of least information, and gives the most unbiased probability estimate given the available evidence. Previous methods for computing maximum entropy values shows that they are either very restrictive in the probabilistic information (constraints) they can use or combinatorially explosive. The computational complexity of the new procedure depends on the inter-connectedness of the constraints, but in practical cases it is small. In addition, the maximum entropy method can give a measure of how accurately a calculated conditional probability is known.",Statistical Mechanics and Entropy,Statistical and Nonlinear Physics,
4210,W117261757,Non-monotonic reasoning using dempster's rule,Matthew L. Ginsberg,"Rich's suggestion that the arcs of semantic nets be labelled so as to reflect confidence in the properties they represent is investigated in greater detail. If these confidences are thought of as ranges of acceptable probabilities, existing statistical methods can be used effectively to combine them. The framework developed also seems to be a natural one in which to describe higher levels of deduction, such as reasoning about reasoning.",Rough Sets and Fuzzy Logic,Computational Theory and Mathematics,
4211,W122836976,"Chronological ignorance: time, nonmonotonicity, necessity and causal theories",Yoav Shoham,"Concerned with the problem of reasoning efficiently about change within a formal system, we identify the initiation problem. The solution to it which we offer, called the logic of chronological ignorance, combines temporal logic, nonmonotonic logic, and the modal logic of necessity. We identify a class of theories, called causal theories, which have elegant model-theoretic and complexity properties in the new logic.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4212,W13520770,In defense of probability,Peter Cheeseman,"In this paper, it is argued that probability theory, when used correctly, is suffrcient for the task of reasoning under uncertainty. Since numerous authors have rejected probability as inadequate for various reasons, the bulk of the paper is aimed at refuting these claims and indicating the scources of error. In particular, the definition of probability as a measure of belief rather than a frequency ratio is advocated, since a frequency interpretation of probability drastically restricts the domain of applicability. Other sources of error include the confusion between relative and absolute probability, the distinction between probability and the uncertainty of that probability. Also, the interaction of and probability is discusses and it is argued that many extensions of logic, such as default logic are better understood in a probabilistic framework. The main claim of this paper is that the numerous schemes for representing and reasoning about uncertainty that have appeared in the AI literature are unnecessary--probability is all that is needed.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4213,W142918741,A synthetic view of approximate reasoning techniques,Henri Prade,"This paper presents a review of different approximate reasoning techniques which have been proposed for dealing with uncertain or imprecise knowledge, especially in expert systems based on production rule methodology. Theoretical approaches such as Bayesian inference, Shafer's belief theory or Zadeh's possibility theory as well as more empirical proposals such as the ones used in MYCIN or in PROSPECTOR, are considered. The presentation is focused on two basic inference schemes : the deductive inference and the combination of several uncertain or imprecise evidences relative to a same matter. Several kinds of uncertainty are taken into account in the models which are described in the paper : different degrees of certainty or of truth may be associated with the observed or produced facts or with the  if.., then... rules; moreover the statements of facts or of rules may be imprecise or fuzzy and the values of the degrees of certainty which are used may be only approximately known. An extensive bibliography, to which it is referred in the text, is appended.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4214,W143545926,An improved constraint-propagation algorithm for diagnosis,"Héctor Geffner, Judea Pearl","Diagnosing a system requires the identification of a set of components whose abnormal behavior could explain the faulty system behavior. Previously, model-based diagnosis schemes have proceeded through a cycle of assumptions - predictions observations assumptions-adjustment, where the basic assumptions entail the proper functioning of those components whose failure is not established. Here we propose a scheme in which every component's status is treated as a variable; therefore, predictions covering all possible behavior of the system can be generated. Remarkably, the algorithm exhibits a drastic reduction in complexity for a large family of system-models. Additionally, the intermediate computations provide useful guidance for selecting new tests.

The proposed scheme may be considered as either an enhancement of the scheme proposed in [de Kleer, 1986] or an adaptation of the probabilistic propagation scheme proposed in [Pearl, 1986] for the diagnosis of deterministic systems.",AI-based Problem Solving and Planning,Artificial Intelligence,
4215,W145128706,Found ations of assumption-based truth maintenance systems: preliminary report,"Raymond Reiter, Johan de Kleer","In this paper we (1) define the concept of a Clause Management System (CMS) -- a generalization of de Kleer's ATMS, (2) motivate such systems in terms of efficiency of search and abductive reasoning, and (3) characterize the computation affected by a CMS in terms of the concept of prime implicants.",Semantic Web and Ontologies,Artificial Intelligence,
4218,W1480135136,Reasoning about Knowledge and Probability,"Ronald Fagin, Joseph Y. Halpern","We provide a model for reasoning about knowledge and probability together. We allow explicit mention of probabilities in formulas, so that our language has formulas that essentially say according to agent i, formula φ holds with probability at least α. The language is powerful enough to allow reasoning about higher-order probabilities, as well as allowing explicit comparisons of the probabilities an agent places on distinct events. We present a general framework for interpreting such formulas, and consider various properties that might hold of the interrelationship between agents' subjective probability spaces at different states. We provide a complete axiomatization for reasoning about knowledge and probability, prove a small model property, and obtain decision procedures. We then consider the effects of adding common knowledge and a probabilistic variant of common knowledge to the language.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4220,W1485493958,An Outlook on Truth Maintenance.,David McAllester,"Abstract : Truth maintenance systems (TMS) have been used in several recent problem-solving systems to record justifications for deduced assertions, to track down the assumptions which underlie contradictions when they arise, and to incrementally modify assertional data structures when assumptions are retracted. This report describes a TMS algorithm that is substantially different from previous systems. This algorithm performs deduction in traditional propositional logic in such a way that the premise set from which deduction is being done can be easily manipulated. A novel approach is also taken to the role of a TMS in larger deductive systems. In this approach the TMS performs all propositional deduction in a uniform manner while the larger system is responsible for controlling the instantiation of universally quantified formulae and axiom schemas.",Semantic Web and Ontologies,Artificial Intelligence,
4224,W1495982240,NESTOR: A Computer-Based Medical Diagnostic Aid That Integrates Causal and Probabilistic Knowledge.,Gregory F. Cooper,"Abstract : In order to address some existing problems in computer-aided medical decision making, a computer program called NESTOR has been developed to aid physicians in determining the most likely diagnostic hypothesis to account for a set of patient findings. The domain of hypercalcemic disorders is used to test solution methods that should be applicable to other medical areas. A key design philosophy underlying NESTOR is that the physicians should have control of the computer interaction to determine what is done and when. In order to provide such controllable, interactive aid, specific technical tasks to be addressed. The unifying philosophy in addressing them is the use of knowledge-based methods within a formal probability theory framework. A user interface module gives the physician control over when and how these tasks are used to aid in diagnosing the cause of a patient's condition. This dissertation presents the problems that are addressed by each of the three tasks, and the details of the methods used to address them. In addition, the results of an evaluation of the hypothesis scoring and search techniques are presented and discussed. Additional keywords: artificial intelligence; expert systems; medical applications; computer aided diagnosis; medical computer applications.",Biomedical Text Mining and Ontologies,Molecular Biology,
4228,W1484413656,Fast algorithms for mining association rules,"Rakesh Agrawal, Ramakrishnan Srikant","We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving thii problem that are fundamentally different from the known algorithms. Empirical evaluation shows that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database.",Data Mining Algorithms and Applications,Information Systems,
4229,W2158899491,Natural Language Processing (almost) from Scratch,"Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel P. Kuksa","We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",Natural Language Processing Techniques,Artificial Intelligence,
4231,W1647729745,An Information-Theoretic Definition of Similarity,Dekang Lin,Similarity is an important and widely used concept. Previous definitions of similarity are tied to a particular application or a form of knowledge representation. We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model. We demonstrate how our definition can be used to measure the similarity in a number of different domains.,AI-based Problem Solving and Planning,Artificial Intelligence,
4232,W2015071685,Subjective bayesian methods for rule-based inference systems,"Richard O. Duda, Peter E. Hart, Nils J. Nilsson","The general problem of drawing inferences from uncertain or incomplete evidence has invited a variety of technical approaches, some mathematically rigorous and some largely informal and intuitive. Most current inference systems in artificial intelligence have emphasized intuitive methods, because the absence of adequate statistical samples forces a reliance on the subjective judgment of human experts. We describe in this paper a subjective Bayesian inference method that realizes some of the advantages of both formal and informal approaches. Of particular interest are the modifications needed to deal with the inconsistencies usually found in collections of subjective statements.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4233,W2052958516,Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy,"John E. Shore, Ryan F. Johnson","Jaynes's principle of maximum entropy and Kullbacks principle of minimum cross-entropy (minimum directed divergence) are shown to be uniquely correct methods for inductive inference when new information is given in the form of expected values. Previous justifications use intuitive arguments and rely on the properties of entropy and cross-entropy as information measures. The approach here assumes that reasonable methods of inductive inference should lead to consistent results when there are different ways of taking the same information into account (for example, in different coordinate system). This requirement is formalized as four consistency axioms. These are stated in terms of an abstract information operator and make no reference to information measures. It is proved that the principle of maximum entropy is correct in the following sense: maximizing any function but entropy will lead to inconsistency unless that function and entropy have identical maxima. In other words given information in the form of constraints on expected values, there is only one (distribution satisfying the constraints that can be chosen by a procedure that satisfies the consistency axioms; this unique distribution can be obtained by maximizing entropy. This result is established both directly and as a special case (uniform priors) of an analogous result for the principle of minimum cross-entropy. Results are obtained both for continuous probability densities and for discrete distributions.",Statistical Mechanics and Entropy,Statistical and Nonlinear Physics,
4235,W2061408624,Contingency tables with given marginals,"C. T. Ireland, S. Kullback","In its simplest formulation the problem considered is to estimate the cell probabilities Pij of an r × c contingency table for which the marginal probabilities Pi and Pj are known and fixed, so as to minimize ΣΣpij In (pij/πij), where πij are the corresponding entries in a given contingency table. An iterative procedure is given for determining the estimates and it is shown that the estimates are BAN, and that the iterative procedure is convergent. A summary of results for a four-way contingency table is given. An illustrative example is given.",Advanced Statistical Methods and Models,Statistics and Probability,
4238,W2113700325,Approximating discrete probability distributions,"H. H. Ku, S. Kullback","The method of minimum discrimination information estimation is applied to the problem of estimating an <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">n</tex> -dimensional discrete probability distribution in terms of lower order marginal distributions. The procedure provides a convergent iterative algorithm. The method yields regular best asymptotically normal (RBAN) estimates. The general procedure includes as a particular case that proposed by a method using dependence trees. An example is given.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4239,W2163166770,Approximating discrete probability distributions with dependence trees,"Chee Lap Chow, C. Liu","A method is presented to approximate optimally an <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">n</tex> -dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">n - 1</tex> first order dependence relationship among the <tex xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">n</tex> variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution.",Probabilistic and Robust Engineering Design,"Statistics, Probability and Uncertainty",
4240,W2294604761,Prior Probabilities,E. T. Jaynes,"In decision theory, mathematical analysis shows that once the sampling distribution, loss function, and sample are specified, the only remaining basis for a choice among different admissible decisions lies in the prior probabilities. Therefore, the logical foundations of decision theory cannot be put in fully satisfactory form until the old problem of arbitrariness (sometimes called subjectiveness) in assigning prior probabilities is resolved. The principle of maximum entropy represents one step in this direction. Its use is illustrated, and a correspondence property between maximum-entropy probabilities and frequencies is demonstrated. The consistency of this principle with the principles of conventional direct probability analysis is illustrated by showing that many known results may be derived by either method. However, an ambiguity remains in setting up a prior on a continuous parameter space because the results lack invariance under a change of parameters; thus a further principle is needed. It is shown that in many problems, including some of the most important in practice, this ambiguity can be removed by applying methods of group theoretical reasoning which have long been used in theoretical physics. By finding the group of transformations on the parameter space which convert the problem into an equivalent one, a basic desideratum of consistency can be stated in the form of functional equations which impose conditions on, and in some cases fully determine, an invariant measure on the parameter space.",Quantum Mechanics and Applications,"Atomic and Molecular Physics, and Optics",
4242,W2797148637,A Mathematical Theory of Evidence,Glenn Shafer,"Both in science and in practical affairs we reason by combining facts only inconclusively supported by evidence. Building on an abstract understanding of this process of combination, this book constructs a new theory of epistemic probability. The theory draws on the work of A. P. Dempster but diverges from Depster's viewpoint by identifying his as epistemic probabilities and taking his rule for combining upper and lower as fundamental. The book opens with a critique of the well-known Bayesian theory of epistemic probability. It then proceeds to develop an alternative to the additive set functions and the rule of conditioning of the Bayesian theory: set functions that need only be what Choquet called monotone of order of infinity. and Dempster's rule for combining such set functions. This rule, together with the idea of weights of evidence, leads to both an extensive new theory and a better understanding of the Bayesian theory. The book concludes with a brief treatment of statistical inference and a discussion of the limitations of epistemic probability. Appendices contain mathematical proofs, which are relatively elementary and seldom depend on mathematics more advanced that the binomial theorem.",Statistics Education and Methodologies,Statistics and Probability,
4244,W62763226,Efficient minimum information updating for bayesian inferencing in expert systems,"John F. Lemmer, Stephen W. Barth","This short paper Dresents a new algorithm for minimun information Inferencing within Expert Systems. This algorithm is as efficient in both time and space as previously reported work [3 3 but always provides a minimum information result. In addition to describing the new algorithm, we will prove that it does indeed satisfy minimum information criteria. Since both algorithms are sub stantially different from the Bayesian approaches in well known expert systems such as the original Prospector [1], AL/X [8], and MYCIN [9 3, and from the approach of Kulikowski [5], background is provided to show the motivation for using the minimum information approach to updating.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4246,W2020647649,Connectionist expert systems,Stephen I. Gallant,"Connectionist networks can be used as expert system knowledge bases. Furthermore, such networks can be constructed from training examples by machine learning techniques. This gives a way to automate the generation of expert systems for classification problems.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4247,W1975717592,"Who Reasons Well? Two Studies of Informal Reasoning Among Children of Different Grade, Ability, and Knowledge Levels","Mary L. Means, James F. Voss","This article presents the results of two experiments addressing the relation of reasoning skill to student grade, ability, and knowledge levels. In the first experiment, three levels of students within each of four grades-5, 7, 9, or 1 1-were designated as intellectually gifted, average, or below average. They were given three tasks involving everyday problems for which they provided solutions and justifications. The second experiment included the measurement of domain knowledge with grade and ability level. Measures of informal reasoning showed a substantial relation between ability level and performance, with knowledge significantly related to performance measures, such as number and type of reasons generated, but not to measures involving soundness or acceptability of arguments, which were explained by ability level. Grade was related only to an increase in personal and broadly defined social reasons; other effects were ""washed out"" by knowledge. The findings were interpreted in terms of a two-component model of informal reasoning, a knowledgeexperiential component and an informal reasoning skill component based on the acquisition of argumentation-based language structures termed conventions of reasoning. Consideration of the relation of reasoning to learning and to instruction emphasized the importance of teaching informal reasoning skill.",Innovative Teaching and Learning Methods,Developmental and Educational Psychology,
4249,W41895731,Probabilistic temporal reasoning,"Thomas Dean, Keiji Kanazawa","Reasoning about change requires predicting how long a proposition, having become true, will continue to be so. Lacking perfect knowledge, an agent may be constrained to believe that a proposition persists indefinitely simply because there is no way for the agent to infer a contravening proposition with certainty. In this paper, we describe a theory of causal reasoning under uncertainty. Our theory uses easily obtainable statistical data to provide expectations concerning how long propositions are likely to persist in the absence of specific knowledge to the contrary. We consider a number of issues that arise in combining evidence, and describe an approach to computing probabilistic assessments of the sort licensed by our theory.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4251,W2069138748,RATIONALITY AND ITS ROLES IN REASONING,Jon Doyle,"The economic theory of rationality promises to equal mathematical logic in its importance for the mechanization of reasoning. We survey the growing literature on how the basic notions of probability, utility, and rational choice, coupled with practical limitations on information and resources, influence the design and analysis of reasoning and representation systems.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4263,W1989020770,Bayesian Artificial Intelligence,"Kevin B. Korb, Ann E. Nicholson",Bayesian Reasoning. Introduction to Bayesian Networks. Inference in Bayesian Networks. Bayesian Network Applications. Bayesian Planning and Decision-Making. Bayesian Network Applications II. Learning Bayesian Networks I. Learning Bayesian Networks II. Causality vs. Probability. Knowledge Engineering with Bayesian Networks I. Knowledge Engineering with Bayesian Networks II. Application Software.,Bayesian Modeling and Causal Inference,Artificial Intelligence,
4355,W2071039357,A Computational Approach to Approximate and Plausible Reasoning with Applications to Expert Systems,Henri Prade,"The intended purpose of this paper is twofold: proposing a common basis for the modeling of uncertainty and imprecision, and discussing various kinds of approximate and plausible reasoning schemes in this framework. Together with probability, different kinds of uncertainty measures (credibility and plausibility functions in the sense of Shafer, possibility measures in the sense of Zadeh and the dual measures of necessity, Sugeno's gλ-fuzzy measures) are introduced in a unified way. The modeling of imprecision in terms of possibility distribution is then presented, and related questions such as the measure of the uncertainty of fuzzy events, the probability and possibility qualification of statements, the concept of a degree of truth, and the truth qualification of propositions, are discussed at length. Deductive inference from premises weighted by different kinds of measures by uncertainty, or by truth-values in the framework of various multivalued logics, is fully investigated. Then, deductive inferences from imprecise or fuzzy premises are dealt with; patterns of reasoning where both uncertainty and imprecision are present are also addressed. The last section is devoted to the combination of uncertain or imprecise pieces of information given by different sources. On the whole, this paper is a tentative survey of quantitative approaches in the modeling of uncertainty and imprecision including recent theoretical proposals as well as more empirical techniques such as the ones developed in expert systems such as MYCIN or PROSPECTOR, the management of uncertainty and imprecision in reasoning patterns being a key issue in artificial intelligence.",Multi-Criteria Decision Making,Management Science and Operations Research,
4358,W2016665860,An analysis of four uncertainty calculi,"Steven J. Henkind, Malcolm C. Harrison","An important issue faced by contemporary artificial intelligence workers is how to deal with uncertain information. Four of the more prominent calculi-probability theory (especially the Bayesian approach), the Dempster-Shafer theory, fuzzy set theory, and the MYCIN and EMYCIN calculi-are examined. Particular attention is paid to the underlying assumptions of these calculi and to their computational complexities. Each of the four calculi has a different perspective in uncertainty, and each manipulates uncertain information in a different way. Despite what some authors have claimed, there does not seem to be one calculus that is the best for all situations. Each of the calculi has its strong points; the main disadvantage seen in all of the calculi is that they compute aggregate numbers, but keep no record of divergence in opinions.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>",Multi-Criteria Decision Making,Management Science and Operations Research,
4415,W1522290492,An inference technique for integrating knowledge from disparate sources,"Thomas D. Garvey, John D. Lowrance, Martin A. Fischler","This paper introduces a formal method for integrating knowledge derived from a variety of sources for use in perceptual reasoning. The formalism is based on the proposltlonal calculus, a derivative of Shafer's mathematical theory of evidence [4]. It is more general than either a Boolean or Bayeslan approach, providing for Boolean and Bayeslan inferencing when the appropriate information is available. In this formalism, the likelihood of a proposition A is represented as a subinterval, [s(A), p(A)], of the unit interval, [0, 1]. The evidential support for proposition A is represented by s(A), while p(A) represents its degree of plausibility; p(A) can also be interpreted as the degree to which one fails to doubt A, p(A) being equal to one minus the evidential support for A. This paper describes how evidential information, furnished by a knowledge source in the form of a probability mass distribution, can be converted to this interval representation; how, through a set of inference rules for computing intervals of dependent propositions, this Information can be extrapolated from those propositions it directly bears upon, to those it indirectly bears upon; and how multiple bodies of evidential Information can be pooled. A sample application of this approach, modeling the operation of a collection of sensors (a particular type of knowledge source), illustrates these techniques.",Neural Networks and Applications,Artificial Intelligence,
4429,W2039125701,Representation and combination of uncertainty with belief functions and possibility measures,"Didler Dubois, Henri Prade","The theory of evidence proposed by G. Shafer is gaining more and more acceptance in the field of artificial intelligence, for the purpose of managing uncertainty in knowledge bases. One of the crucial problems is combining uncertain pieces of evidence stemming from several sources, whether rules or physical sensors. This paper examines the framework of belief functions in terms of expressive power for knowledge representation. It is recalled that probability theory and Zadeh's theory of possibility are mathematically encompassed by the theory of evidence, as far as the evaluation of belief is concerned. Empirical and axiomatic foundations of belief functions and possibility measures are investigated. Then the general problem of combining uncertain evidence is addressed, with focus on Dempster rule of combination. It is pointed out that this rule is not very well adapted to the pooling of conflicting information. Alternative rules are proposed to cope with this problem and deal with specific cases such as nonreliable sources, nonexhaustive sources, inconsistent sources, and dependent sources. It is also indicated that combination rules issued from fuzzy set and possibility theory look more flexible than Dempster rule because many variants exist, and their numerical stability seems to be better.",Multi-Criteria Decision Making,Management Science and Operations Research,
4479,W1279140112,Applications of Circumscription to Formalizing Common Sense Knowledge.,John McCarthy,"We present a new and more symmetric version of the circumscription method of nonmonotonic reasoning first described in (McCarthy 1980) and some applications to formalizing common sense knowledge. The applications in this paper are mostly based on minimizing the abnormality of different aspects of various entities. Included are nonmonotonic treatments of is-a hierarchies, the unique names hypothesis, and the frame problem. The new circumscription may be called formula circumscription to distinguish it from the previously defined domain circumscription and predicate circumscription. A still more general formalism called prioritized circumscription is briefly explored.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4480,W165103996,Default reasoning as likelihood reasoning,Elaine Rich,"Several attempts to define formal logics for some type of default reasoning have been made. All of these logics share the property that in any given state, a proposition p is either held to be true, it is held to be false, or no belief about it is held. But if we ask what default reasoning really is, we see that it is form of likelihood reasoning. The goal of this paper is to show that if default reasoning is treated as likelihood reasoning (similar to that of Mycin), then natural solutions emerge for several of the problems that are encountered when default reasoning is used. This is shown by presenting 7 such problems and showing how they are solved.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4482,W2093713825,Multivalued logics: a uniform approach to reasoning in artificial intelligence,Matthew L. Ginsberg,"This paper describes a uniform formalization of much of the current work in artificial intelligence on inference systems. We show that many of these systems, including first‐order theorem provers, assumption‐based truth maintenance systems (atmss), and unimplemented formal systems such as default logic or circumscription, can be subsumed under a single general framework. We begin by defining this framework, which is based on a mathematical structure known as a bilattice. We present a formal definition of inference using this structure and show that this definition generalizes work involving atmss and some simple nonmonotonic logics. Following the theoretical description, we describe a constructive approach to inference in this setting; the resulting generalization of both conventional inference and atmss is achieved without incurring any substantial computational overhead. We show that our approach can also be used to implement a default reasoner, and discuss a combination of default and atms methods that enables us to formally describe an “incremental” default reasoning system. This incremental system does not need to perform consistency checks before drawing tentative conclusions, but can instead adjust its beliefs when a default premise or conclusion is overturned in the face of convincing contradictory evidence. The system is therefore much more computationally viable than earlier approaches. Finally, we discuss the implementation of our ideas. We begin by considering general issues that need to be addressed when implementing a multivalued approach such as that we are proposing, and then turn to specific examples showing the results of an existing implementation. This single implementation is used to solve a digital simulation task using first‐order logic, a diagnostic task using atmss as suggested by de Kleer and Williams, a problem in default reasoning as in Reiter's default logic or McCarthy's circumscription, and to solve the same problem more efficiently by combining default methods with justification information. All of these applications use the same general‐purpose bilattice theorem prover and differ only in the choice of bilattice being considered.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4484,W2155397158,Overextension of conjunctive concepts: Evidence for a unitary model of concept typicality and class inclusion.,James A. Hampton,"Four experiments investigated how people judge both the typicality and membership of items in conjunctive concepts such as school furniture or sports which are games. Judgments of membership in conjunctions were overextended, and there was asymmetry between the constituent concepts in their influence on relative conjunctive concept membership. The results are discussed in the light of recent theoretical disputes about the modeling of concept representations and the process of forming conjunctions (Cohen & Murphy, 1984; Osherson & Smith, 1981, 1982; Smith & Osherson, 1984). A theory is proposed in which constituent intensions are combined to form a composite prototype for the conjunction. Membership in both single and conjunctive concepts is then determined in the same unitary fashion, by placing a membership criterion on the perceived similarity of possible exemplars to the prototype. An important issue in the study of natural language concepts is the way in which common semantic concepts combine to form conjunctions. For a wide range of concepts, two very reliable phenomena have been established: One, members of concept categories vary in their representativeness, and two, for many concepts the boundary around the class of concept members is unclear or fuzzy. However, to date few empirical studies have considered what happens when two of these fuzzy concepts are placed in conjunction. Do conjunctions show similar typicality and fuzziness phenomena? If so, how can they be related to typicality in the constituent concepts? Do fuzzy concepts follow the same logical rules for conjunction as well-defined concepts? The first aim of this article is to provide empirical evidence that may begin to answer these questions. The current interest in concept conjunctions comes largely from their relevance to the basic question of concept definitions: that is, how a concept picks out an extensional set of concept members. A second aim of this research is therefore to use the study of conceptual combination to provide important constraints on models of conceptualization. In particular, it will be argued that evidence on conjunctions can have theoretical implications for distinguishing the following two opposing accounts of concept definitions.",Advanced Text Analysis Techniques,Artificial Intelligence,
4485,W1579248359,The structure-mapping engine,"Brian Falkenhainer, Kenneth D. Forbus, Dedre Gentner","This paper describes the Structure-Mapping Engine (SME), a cognitive simulation program for studying human analogical processing. SME is based on Gentner's Structure-Mapping theory of analogy, and provides a tool kit for constructing matching algorithms consistent with this theory. This flexibility enhances cognitive simulation studies by simplifying experimentation. Furthermore, SME is very efficient, making it a candidate component for machine learning systems as well. We review the Structure-Mapping theory and describe the design of the engine. Next we demonstrate some examples of its operation. Finally, we discuss our plans for using SME in cognitive simulation studies.",AI-based Problem Solving and Planning,Artificial Intelligence,
4487,W2159853262,"""I Lied about the Trees"" Or, Defaults and Definitions in Knowledge Representation",Ronald J. Brachman,"Over the past few years, the notion of a “prototype” (e.g., TYPICAL-ELEPHANT) seems to have caught on securely in knowledge representation research. Along with a way to specify default properties for instances of a description, proto-representations allow the overriding, or “cancelling” of properties that don’t apply in particular cases. This supposedly makes representing exceptions (three-legged elephants and the like) easy; but, alas, it makes one crucial type of representation impossiblethat of composite descriptions whose meanings are functions of the structure and interrelation of their parts. This article explores this and other ramifications of the emphasis on default properties and “typical” objects.",Semantic Web and Ontologies,Artificial Intelligence,
4606,W1549071417,Inexact inference for rule-based damage assessment of existing structures,"Mitsuru Ishizuka, K. S. Fu, James T. P. Yao","The knowledge organization of a rule-based damage assessment system of existing structures subjected to earthquake excitation is outlined in this paper. A principle of inexact inference to obtain a rational solution is presented. The fuzzy set theory and the production system with certainty factor are employed jointly in the inexact inference to deal with the continuous nature of the damage state and to attain the modularity of uncertain knowledge, respectively.",Advanced Computational Techniques and Applications,Artificial Intelligence,
4641,W1531751116,A guide to the modal logics of knowledge and belief: preliminary draft,"Joseph Y. Halpern, Yoram Moses","We review and re-examine possible-worlds semantics for propositional logics of knowledge and belief with four particular points of emphasis: (1) we show how general techniques for finding decision procedures and complete axiomatizations apply to models for knowledge and belief, (2) we show how sensitive the difficulty of the decision procedure is to such issues as the choice of modal operators and the axiom system, (3) we discuss how notions of common knowledge and implicit knowledge among a group of agents fit into the possible-worlds framework, and (4) we consider to what extent the possible-worlds approach is a viable one for modelling knowledge and belief. As far as complexity is concerned, we show among other results that while the problem of deciding satisfiability of an S5 formula with one knower is NP-complete, the problem for many knowers is PSPACE-complete. Adding an implicit knowledge operator does not change the complexity substantially, but once a common knowledge operator is added to the language, the problem becomes complete for exponential time.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4643,W2150191218,Reasoning About Change: Time and Causation from the Standpoint of Artificial Intelligence,Yoav Shoham,Reasoning About Change presents a comprehensive approach to temporal reasoning in artificial intelligence.,Multi-Agent Systems and Negotiation,Artificial Intelligence,
4644,W2169602487,Nonmonotonic Logic II,Drew McDermott,"article Free Access Share on Nonmonotonic Logic II: Nonmonotonic Modal Theories Author: Drew McDermott Department of Computer Science, Yale University, P O Box 2158, New Haven, Connecticut Department of Computer Science, Yale University, P O Box 2158, New Haven, ConnecticutView Profile Authors Info & Claims Journal of the ACMVolume 29Issue 1pp 33–57https://doi.org/10.1145/322290.322293Published:01 January 1982Publication History 280citation716DownloadsMetricsTotal Citations280Total Downloads716Last 12 Months43Last 6 weeks12 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4648,W2143336154,System <i>Z</i> : A Natural Ordering of Defaults with Tractable Applications to Nonmonotonic Reasoning,Judea Pearl,"chapter Share on System Z: A Natural Ordering of Defaults with Tractable Applications to Nonmonotonic Reasoning Author: Judea Pearl Search about this author Authors Info & Claims Probabilistic and Causal Inference: The Works of Judea PearlFebruary 2022 Pages 201–214https://doi.org/10.1145/3501714.3501730Online:04 March 2022Publication History 0citation2DownloadsMetricsTotal Citations0Total Downloads2Last 12 Months2Last 6 weeks1 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my Alerts New Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteGet Access",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4649,W2047463693,On distributed artificial intelligence,"Luís Eduardo, Castillo Hern","Abstract Distributed Artificial Intelligence has been loosely defined in terms of computation by distributed, intelligent agents. Although a variety of projects employing widely ranging methodologies have been reported, work in the field has matured enough to reveal some consensus about its main characteristics and principles. A number of prominent projects are described in detail, and two general frameworks, the System conceptual model and the agent conceptual model , are used to compare the different approaches. The paper concludes by reviewing approaches to formalizing some of the more critical capabilities required by multi-agent interaction.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4650,W2040804169,Computer Integration: Reducing Fragmentation in AEC Industry,"H. Craig Howard, Raymond E. Levitt, Boyd C. Paulson, Jens G. Pohl, C. B. Tatum","Emerging and existing computer technologies can be synthesized in ways that provide new kinds of decision support for integrating the data, design decisions, and knowledge normally dispersed among the many participants in the architecture/engineering/construction (AEC) process. This paper briefly examines the origins and impacts of fragmentation in the industry in the U.S. and describes six thrust areas where computer‐integrated design and construction can substantially improve the competitiveness of the U.S. AEC industry and the quality of its products. In each of these thrust areas, AEC problems pose important challenges to developing technologies for artificial intelligence, graphic and nongraphic databases, process automation and robotics, and management and dissemination of technology. The application of these advanced computer technologies and the AEC industry offers the promise of significant gains in productivity and will infuse new excitement into civil engineering education and practice.",BIM and Construction Integration,Building and Construction,
4651,W284629942,"Coordinating, Planning and Control",Thomas Dean,"This book is devoted to the design of complex systems for applications in robotics, automated manufacturing, and time-critical decision support systems.In exploring the issues involved in the design of such systems, we investigate techniques from artificiaJ intelligence, control theory, operations research, and the decision sciences.In the process, we attempt to draw correspondences between concepts from the various fields.However, this work is not intended as a grand unification of these disciplines, even as they pertain to the specific issues of interest.Instead, we present tools from these areas as component technologies, each playing a pivotal role in the design of complex autonomous systems.In our attempt to draw a coherent picture of the broad range of problems and techniques considered here, we rely on the central themes of observation, prediction, and computation.In an uncertain environment, we must employ observation to augment our incomplete knowledge with evidence from the senses.We invoke prediction to extrapolate from our knowledge and observations the effects of our actions over time.Revising and making effective use of our knowledge requires computation to translate models and observations to meaningful action.The design of a system to control complex processes consists largely of strategies for deciding dynamically what and how to observe, predict, and compute.In the 1980s, the traditional view of planning as offline computation r,lying on precise models and perfect information was challenged by research in artificial intelligence on robotic control systems embedded in complex environments.The challenge was met with proposals for reactive systems.systems designed to respond directly to perceived conditions in situations where there is little or no time to deliberate on how best to act.One disconcerting aspect of the focus on reactive systems was that it diverted effort from planning: predicting possible futures and formulating plans of action that take into account those possibilities.As research progressed, it became V vi Preface apparent that there was significant overlap between the work on reactive svstems and the work in control theory.This book connects traditional research in planning with the constraints governing embedded systems, by reformulating the process of planning in terms of control.Viewed from a control perspective, reactive systems embody particular strategies for controlling processes.In order to evaluate reactivesystems, we have to analyze the connection between such strategies and the physical systems they seek to control.The tools required to perform such analyses are readily available from control theory, computer science, and artificial intelligence.This book focuses on the issues involved in modeling processes and generating sequences of commands in a timely manner.The practice of constructing formal models of physical systems and then using those models to develop programs to control processes is examined in some depth.This book is intended for graduate and advanced undergraduate students in computer science and engineering.It is meant for students trying to orient themselves with respect to the many disciplines that have something significant to say about planning and control for applications in robotics and automation.The material in this book is suitable for a one-semester course offered to graduate and advanced undergraduate students.Given that the material covers a range of disciplines, we assume a somewhat varied background.From computer science, we assume some familiarity with the theory of computation [121 and basic complexity theory [8].Pidgin ALGOL [1] and Edinburgh PROLOG [5] are employed in describing algorithms.Some background in logic [14] and its application in artificial intelligence are also expected [4, 151.Elementary probability theory plays a role in the chapters on uncertainty and stochastic modeling [11, 13].While no background in control theory is required, we assume some familiarity with linear algebra and elementary differential equations [17].We refer occasionally to standard techniques in robotics and machine vision, but no detailed knowledge is assumed.References, both general and specific, are provided at the end of each chapter, so that readers can fill in any missing background knowledge.The book introduces advanced techniques that derive from work in a number of disciplines.The exposition of these techniques is largely selfcontained, with pointers to more detailed treatments.In particular, the text explores the use of default reasoning [9] and temporal logics [18] in modeling processes, a framework for integrating techniques from control theory [6, 101 into a theory of planning, and several methods for coping with uncertainty derived from work in artificial intelligence [16], control theory [2], and deci-",Scheduling and Optimization Algorithms,Industrial and Manufacturing Engineering,
4652,W28460116,Nonmonotonic databases and epistemic queries,Vladimir Lifschitz,"The approach to database query evaluation developed by Levesque and Reiter treats databases as first order theories, and queries as formulas of the language which includes, in addition to the language of the database, an epistemic modal operator. In this epistemic query language, one can express questions not only about the external world described by the database, but also about the database itself-- about what the database knows. On the other hand, epistemic formulas are used in knowledge representation for the purpose of expressing defaults. Autoepistemic logic is the best known epistemic nonmonotonic formalism; the logic of grounded knowledge, proposed recently by Lin and Shoham, is another such system. This paper brings these two directions of research together. We describe a new version of the Lin/Shoham logic, similar in spirit to the Levesque/Reiter theory of epistemic queries. Using this formalism, we can give meaning to epistemic queries in the context of nonmonotonic databases, including logic programs with negation as failure.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4653,W186758419,Nonmonotonic logics: meaning and utility,Yoav Shoham,"We propose a unifying framework for nonmonotonic logics, which subsumes previously published systems, and at the same time is very simple. We discuss some of the technicalities of the new general framework, illustrate briefly how some previous systems are special cases of it, and finish an informal discussion of the intuitive meaning of nonmonotonic inferences.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4667,W2088135982,Knowledge and common knowledge in a distributed environment,"Joseph Y. Halpern, Yoram Moses","We argue that the right way to understand distributed protocols is by considering how messages change the state of knowledge of a system. We present a hierarchy of knowledge states that a system may be in, and discuss how communication can move the system's state of knowledge of a fact up the hierarchy. Of special interest is the notion of common knowledge. Common knowledge is an essential state of knowledge for reaching agreements and coordinating action. We show that in practical distributed systems, common knowledge is not attainable. We introduce various relaxations of common knowledge that are attainable in many cases of interest. We describe in what sense these notions are appropriate, and discuss their relationship to each other. We conclude with a discussion of the role of knowledge in distributed systems.",Distributed systems and fault tolerance,Computer Networks and Communications,
4692,W2100738443,A Machine-Oriented Logic Based on the Resolution Principle,John A. Robinson,"article Free Access Share on A Machine-Oriented Logic Based on the Resolution Principle Author: J. A. Robinson Argonne National Laboratory, Argonne, Illinois and Rice University, Houston, Texas Argonne National Laboratory, Argonne, Illinois and Rice University, Houston, TexasView Profile Authors Info & Claims Journal of the ACMVolume 12Issue 1pp 23–41https://doi.org/10.1145/321250.321253Published:01 January 1965Publication History 2,696citation6,808DownloadsMetricsTotal Citations2,696Total Downloads6,808Last 12 Months797Last 6 weeks75 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF","Logic, Reasoning, and Knowledge",Artificial Intelligence,
4835,W2113634327,Interpretation as abduction,"Jerry R. Hobbs, Mark E. Stickel, Paul C. Martin, Douglas Edwards","An approach to abductive inference developed in the TACITUS project has resulted in a dramatic simplification of how the problem of interpreting texts is conceptualized. Its use in solving the local pragmatics problems of reference, compound nominals, syntactic ambiguity, and metonymy is described and illustrated. It also suggests an elegant and thorough integration of syntax, semantics, and pragmatics.",Natural Language Processing Techniques,Artificial Intelligence,
4842,W173004666,Extended plausible inference,Leonard Friedman,"A generalized theory of plausible inference has been developed. extended to arbitrary expressions of propositional calculus from Shortliffe and Buchanan's original MYCIN formulation. The theory represents uncertainty of belief, and invokes four rules of inference, instead of the two of standard logic. Areas of application include diagnostic problems and deciding between alternative hypotheses. The theory has been implemented in the PI system. The intended area of application for PI is the trouble-shooting of a failed spacecraft, and the solution to a simplified problem drawn from this area is described.",AI-based Problem Solving and Planning,Artificial Intelligence,
4854,W2130915832,Application of Fuzzy Logic to Approximate Reasoning Using Linguistic Synthesis,Mamdani,"This paper describes an application of fuzzy logic in designing controllers for industrial plants. A fuzzy logic is used to synthesize linguistic control protocol of a skilled operator. The method has been applied to pilot scale plants as well as in practical situations. The merits of this method and its usefulness to control engineering are discussed. An avenue for further work in this area is described where the need is to go beyond a purely descriptive approach, and means for implementing a prescriptive or a self-organizing system are explored.",Fuzzy Logic and Control Systems,Artificial Intelligence,
4857,W2157676069,An AI view of the treatment of uncertainty,Alessandro Saffiotti,"Abstract This paper reviews many of the very varied concepts of uncertainty used in AI. Because of their great popularity and generality “parallel certainty inference” techniques, so-called, are prominently in the foreground. We illustrate and comment in detail on three of these techniques; Bayes' theory (section 2); Dempster-Shafer theory (section 3); Cohen's model of endorsements (section 4), and give an account of the debate that has arisen around each of them. Techniques of a different kind (such as Zadeh's fuzzy-sets, fuzzy-logic theory, and the use of non-standard logics and methods that manage uncertainty without explicitly dealing with it) may be seen in the background (section 5). The discussion of technicalities is accompanied by a historical and philosophical excursion on the nature and the use of uncertainty (section 1), and by a brief discussion of the problem of choosing an adequate AI approach to the treatment of uncertainty (section 6). The aim of the paper is to highlight the complex nature of uncertainty and to argue for an open-minded attitude towards its representation and use. In this spirit the pros and cons of uncertainty treatment techniques are presented in order to reflect the various uncertainty types. A guide to the literature in the field, and an extensive bibliography are appended.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4985,W2154069902,Distributed revision of belief commitment in multi-hypotheses interpretation,Judea Pearl,"This paper extends the applications of belief-networks models to include the revision of belief commitments, i.e., the categorical instantiation of a subset of hypotheses which constitute the most satisfactory explanation of the evidence at hand. We show that, in singly-connected networks, the most satisfactory explanation can be found in linear time by a message-passing algorithm similar to the one used in belief updating. In multiply-connected networks, the problem may be exponentially hard but, if the network is sparse, topological considerations can be used to render the interpretation task tractable. In general, finding the most probable combination of hypotheses is no more complex than computing the degree of belief for any individual hypothesis.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
4987,W38794278,Reasoning about multiple faults,"Johan de Kleer, Brian C. Williams","Diagnostic tasks require determining the differences between a model of an artifact and the artifact itself. The differences between the manifested behavior of the artifact and the predicted behavior of the model guide the search for the differences between the artifact and its model. The diagnostic procedure presented in this paper is model-based, inferring the behavior of the composite device from knowledge of the structure and function of the individual components comprising the device. The system (GDE — General Diagnostic Engine) has been implemented and tested on examples in the domain of troubleshooting digital circuits.

This research makes several novel contributions: First, the system diagnoses failures due to multiple faults. Second, failure candidates are represented and manipulated in terms of minimal sets of violated assumptions, resulting in an efficient diagnostic procedure. Third, the diagnostic procedure is incremental, reflecting the iterative nature of diagnosis. Finally, a clear separation is drawn between diagnosis and behavior prediction, resulting in a domain (and inference procedure) independent diagnostic procedure.",AI-based Problem Solving and Planning,Artificial Intelligence,
4988,W2148546044,Algorithms for constraint-satisfaction problems: a survey,Vipin Kumar,"A large number of problems in AI and other areas of computer science can be viewed as special cases of the constraint-satisfaction problem. Some examples are machine vision, belief maintenance, scheduling, temporal reasoning, graph problems, floor plan design, the planning of genetic experiments, and the satisfiability problem. A number of different approaches have been developed for solving these problems. Some of them use constraint propagation to simplify the original problem. Others use backtracking to directly search for possible solutions. Some are a combination of these two techniques. This article overviews many of these approaches in a tutorial fashion.",Constraint Satisfaction and Optimization,Computer Networks and Communications,
4991,W1562027785,Model-Based Diagnosis using Structured System Descriptions,Adnan Darwiche,"This paper presents a comprehensive approach for model-based diagnosis which includes proposals for characterizing and computing preferred diagnoses, assuming that the system description is augmented with a system structure (a directed graph explicating the interconnections between system components). Specifically, we first introduce the notion of a consequence, which is a syntactically unconstrained propositional sentence that characterizes all consistency-based diagnoses and show that standard characterizations of diagnoses, such as minimal conflicts, correspond to syntactic variations on a consequence. Second, we propose a new syntactic variation on the consequence known as negation normal form (NNF) and discuss its merits compared to standard variations. Third, we introduce a basic algorithm for computing consequences in NNF given a structured system description. We show that if the system structure does not contain cycles, then there is always a linear-size consequence in NNF which can be computed in linear time. For arbitrary system structures, we show a precise connection between the complexity of computing consequences and the topology of the underlying system structure. Finally, we present an algorithm that enumerates the preferred diagnoses characterized by a consequence. The algorithm is shown to take linear time in the size of the consequence if the preference criterion satisfies some general conditions.",AI-based Problem Solving and Planning,Artificial Intelligence,
4993,W1597829571,Diagnosing tree-decomposable circuits,"Yousri El Fattah, Rina Dechter","This paper describes a diagnosis algorithm called structure-based abduction (SAB) which was developed in the framework of constraint networks [12]. The algorithm exploits the structure of the constraint network and is most efficient for near-tree problem domains. By analyzing the structure of the problem domain, the performance of such algorithms can be bounded in advance. We present empirical results comparing SAB with two modelbased algorithms, MBD1 and MBD2, for the task of finding one or all minimal-cardinality diagnoses. MBD1 uses the same computing strategy as algorithm GDE [9]. MBD2 adopts a breadth-first search strategy similar to the algorithm DIAGNOSE [24]. The main conclusion is that for nearly acyclic circuits, such as the N-bit adder, the performance of SAB being linear provides definite advantages as the size of the circuit increases.",Constraint Satisfaction and Optimization,Computer Networks and Communications,
4995,W2152936123,Combining FDI and AI Approaches Within Causal-Model-Based Diagnosis,"S. Gentil, Jacky Montmain, Christophe Combastel","This paper presents a model-based diagnostic method designed in the context of process supervision. It has been inspired by both artificial intelligence and control theory. AI contributes tools for qualitative modeling, including causal modeling, whose aim is to split a complex process into elementary submodels. Control theory, within the framework of fault detection and isolation (FDI), provides numerical models for generating and testing residuals, and for taking into account inaccuracies in the model, unknown disturbances and noise. Consistency-based reasoning provides a logical foundation for diagnostic reasoning and clarifies fundamental assumptions, such as single fault and exoneration. The diagnostic method presented in the paper benefits from the advantages of all these approaches. Causal modeling enables the method to focus on sufficient relations for fault isolation, which avoids combinatorial explosion. Moreover, it allows the model to be modified easily without changing any aspect of the diagnostic algorithm. The numerical submodels that are used to detect inconsistency benefit from the precise quantitative analysis of the FDI approach. The FDI models are studied in order to link this method with DX component-oriented reasoning. The recursive on-line use of this algorithm is explained and the concept of local exoneration is introduced.",Fault Detection and Control Systems,Control and Systems Engineering,
5127,W1523296148,RESIDUE: a deductive approach to design synthesis,"Jonathan Finger, Michael Genesereth","We present a new approach to deductive design synthesis, the Residue Approach, in which designs are represented as sets of constraints. Previous approaches, such as PROLOG [18] or the work of Manna and Waldinger [11], express designs as bindings on single terms. We give a complete and sound procedure for finding sets of propositions constituting a legal design. The size of the search space of the procedure and the advantages and disadvantages of the Residue Approach are analysed. In particular we show how Residue can avoid backtracking caused by making design decisions of overly coarse granularity. In contrast, it is awkward for the single term approaches to do the same. In addition we give a rule for constraint propagation in deductive synthesis, and show its use in pruning the design space. Finally, Residue is related to other work, in particular, to Default Logic [16] and to Assumption-Based Truth Maintenance [1].","Logic, programming, and type systems",Artificial Intelligence,
5128,W1600736227,Some theories of reasoned assumptions : an essay in rational psychology,Jon Doyle,"We examine several formulations of the common practice of jumping to conclusions when actions demand decisions but solid knowledge fails. This practice permeates artificial intelligence, where systems assume many conclusions automatically as defaults simply because the questions they decide are known to occur frequently, and where other assumptions are formulated and adopted only when ignorance stalls action. After developing the motivations and general nature of these inferences, we introduce a formal basis for describing them. This formulation allows separate introduction of the several ideas involved, and so facilitates characterization of some important combinations and some previous proposals. Initial results are proved about these theories, including the aptness of the formal notions with respect to the intuitive motivations. Benefits of this formulation include an indication of the ways notions from logic and metamathematics can enter into psychologies without subscribing to all of logic or metamathematics, an indication of the importance of conservation of mental states in the description of psychologies, and formal and intuitive relations between the approach of reasoned assumptions and its popular alternatives, deductivism and Bayesianism.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
5133,W3099451540,A Knowledge Compilation Map,"Adnan Darwiche, Pierre Marquis","We propose a perspective on knowledge compilation which calls for analyzing different compilation approaches according to two key dimensions: the succinctness of the target compilation language, and the class of queries and transformations that the language supports in polytime. We then provide a knowledge compilation map, which analyzes a large number of existing target compilation languages according to their succinctness and their polytime transformations and queries. We argue that such analysis is necessary for placing new compilation approaches within the context of existing ones. We also go beyond classical, flat target compilation languages based on CNF and DNF, and consider a richer, nested class based on directed acyclic graphs (such as OBDDs), which we show to include a relatively large number of target compilation languages.",Natural Language Processing Techniques,Artificial Intelligence,
5136,W2038897673,Knowledge compilation and theory approximation,"Bart Selman, Henry Kautz","Computational efficiency is a central concern in the design of knowledge representation systems. In order to obtain efficient systems, it has been suggested that one should limit the form of the statements in the knowledge base or use an incomplete inference mechanism. The former approach is often too restrictive for practical applications, whereas the latter leads to uncertainty about exactly what can and cannot be inferred from the knowledge base. We present a third alternative, in which knowledge given in a general representation language is translated (compiled) into a tractable form—allowing for efficient subsequent query answering. We show how propositional logical theories can be compiled into Horn theories that approximate the original information. The approximations bound the original theory from below and above in terms of logical strength. The procedures are extended to other tractable languages (for example, binary clauses) and to the first-order case. Finally, we demonstrate the generality of our approach by compiling concept descriptions in a general frame-based language into a tractable form.",Machine Learning and Algorithms,Artificial Intelligence,
5264,W1502279330,Resource-bounded Knowledge,Yoram Moses,"Traditional treatments of knowledge in distributed systems have not been able to account for processors' limited computational resources. This paper presents definitions of resource-bounded knowledge, belief, and common knowledge that in a precise sense capture the behavior of resource-bounded processors. Subtle properties of the resulting notions are discussed, and they are successfully applied to two problems in distributed computing.","Computability, Logic, AI Algorithms",Computational Theory and Mathematics,
5268,W1999537009,A knowledge-based analysis of zero knowledge,"Joseph Y. Halpern, Yjoram Moses, Mark R. Tuttle","While the intuition underlying a zero knowledge proof system [GMR85] is that no “knowledge” is leaked by the prover to the verifier, researchers are just beginning to analyze such proof systems in terms of formal notions of knowledge. In this paper, we show how interactive proof systems motivate a new notion of practical knowledge, and we capture the definition of an interactive proof system in terms of practical knowledge. Using this notion of knowledge, we formally capture and prove the intuition that the prover does not leak any knowledge of any fact (other than the fact being proven) during a zero knowledge proof. We extend this result to show that the prover does not leak any knowledge of how to compute any information (such as the factorization of a number) during a zero knowledge proof. Finally, we define the notion of a weak interactive proof in which the prover is limited to probabilistic, polynomial-time computations, and we prove analogous security results for such proof systems. We show that, in a precise sense, any nontrivial weak interactive proof must be a proof about the prover's knowledge, and show that, under natural conditions, the notions of interactive proofs of knowledge defined in [TW87] and [FFS87] are instances of weak interactive proofs.",Cryptography and Data Security,Artificial Intelligence,
5272,W2106622011,Agreeing to Disagree,Robert J. Aumann,"Two people, 1 and 2, are said to have common knowledge of an event $E$ if both know it, 1 knows that 2 knows it, 2 knows that 1 knows is, 1 knows that 2 knows that 1 knows it, and so on. THEOREM. If two people have the same priors, and their posteriors for an event $A$ are common knowledge, then these posteriors are equal.","Epistemology, Ethics, and Metaphysics",Philosophy,
5273,W2121375212,A Formal Theory of Knowledge and Action,Robert C. Moore,"Abstract : Most work on planning and problem solving within the field of artificial intelligence assumes that the agent has complete knowledge of all relevant aspects of the problem domain and problem situation In the real world, however, planning and acting must frequently be performed without complete knowledge. This imposes two additional burdens on an intelligent agent trying to act effectively. First, when the agent entertains a plan for achieving some goal, he must consider not only whether the physical prerequisites of the plan have been satisfied, but also whether he has all the information necessary to carry out the plan. Second, he must be able to reason about what be can do to obtain necessary information that he lacks. In this paper, we present a theory of action in which these problems are taken into account, showing how to formalize both the knowledge prerequisites of action and the effects of action on knowledge.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
5275,W2138695497,Probabilistic temporal logics for finite and bounded models,"Sergiu Hart, Micha Sharir","We present two (closely-related) propositional probabilistic temporal logics based on temporal logics of branching time as introduced by Ben-Ari, Pnueli and Manna and by Clarke and Emerson. The first logic, PTLf, is interpreted over finite models, while the second logic, PTLb, which is an extension of the first one, is interpreted over infinite models with transition probabilities bounded away from 0. The logic PTLf allows us to reason about finite-state sequential probabilistic programs, and the logic PTLb allows us to reason about (finite-state) concurrent probabilistic programs, without any explicit reference to the actual values of their state-transition probabilities. A generalization of the tableau method yields exponential-time decision procedures for our logics, and complete axiomatizations of them are given. Several meta-results, including the absence of a finite-model property for PTLb, and the connection between satisfiable formulae of PTLb and finite state concurrent probabilistic programs, are also discussed.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
5276,W2144752539,The knowledge complexity of interactive proof-systems,"Sbafi Goldwasser, Silvio Micali, Charles Rackoff","Article Free Access Share on The knowledge complexity of interactive proof-systems Authors: S Goldwasser MIT MITView Profile , S Micali MIT MITView Profile , C Rackoff University of Toronto University of TorontoView Profile Authors Info & Claims STOC '85: Proceedings of the seventeenth annual ACM symposium on Theory of computingDecember 1985Pages 291–304https://doi.org/10.1145/22145.22178Published:01 December 1985Publication History 787citation6,988DownloadsMetricsTotal Citations787Total Downloads6,988Last 12 Months2,341Last 6 weeks250 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF","Logic, programming, and type systems",Artificial Intelligence,
5280,W2293413371,Knowledge and control,"Wiebe van der Hoek, Nicolas Troquard, Michael Wooldridge","Logics of propositional control, such as van der Hoek and Wooldridge's CL-PC [14], were introduced in order to represent and reason about scenarios in which each agent within a system is able to exercise unique control over some set of system variables. Our aim in the present paper is to extend the study of logics of propositional control to settings in which these agents have incomplete information about the society they occupy. We consider two possible sources of incomplete information. First, we consider the possibility that an agent is only able to read a subset of the overall system variables, and so in any given system state, will have partial information about the state of the system. Second, we consider the possibility that an agent has incomplete information about which agent controls which variables. For both cases, we introduce a logic combining epistemic modalities with the operators of CL-PC, investigate its axiomatization, and discuss its properties.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
5281,W2067887313,"Knowledge, probability, and adversaries","Joseph Y. Halpern, Mark R. Tuttle","What should it mean for an agent to know or believe an assertion is true with probability 9.99? Different papers [2, 6, 15] give different answers, choosing to use quite different probability spaces when computing the probability that an agent assigns to an event. We show that each choice can be understood in terms of a betting game. This betting game itself can be understood in terms of three types of adversaries influencing three different aspects of the game. The first selects the outcome of all nondeterministic choices in the system; the second represents the knowledge of the agent's opponent in the betting game (this is the key place the papers mentioned above differ); and the third is needed in asynchronous systems to choose the time the bet is placed. We illustrate the need for considering all three types of adversaries with a number of examples. Given a class of adversaries, we show how to assign probability spaces to agents in a way most appropriate for that class, where “most appropriate” is made precise in terms of this betting game. We conclude by showing how different assignments of probability spaces (corresponding to different opponents) yield different levels of guarantees in probabilistic coordinated attack.","Logic, Reasoning, and Knowledge",Artificial Intelligence,
5283,W2016917647,A model-theoretic analysis of knowledge,"Ronald Fagin, Joseph Y. Halpern, Moshe Y. Vardi","article Free Access Share on A model-theoretic analysis of knowledge Authors: Ronald Fagin IBM Almaden Research Center, San Jose, California IBM Almaden Research Center, San Jose, CaliforniaView Profile , Joseph Y. Halpern IBM Almaden Research Center, San Jose, California IBM Almaden Research Center, San Jose, CaliforniaView Profile , Moshe Y. Vardi IBM Almaden Research Center, San Jose, California IBM Almaden Research Center, San Jose, CaliforniaView Profile Authors Info & Claims Journal of the ACMVolume 38Issue 201 April 1991pp 382–428https://doi.org/10.1145/103516.128680Published:01 April 1991Publication History 64citation1,030DownloadsMetricsTotal Citations64Total Downloads1,030Last 12 Months49Last 6 weeks2 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF","Logic, Reasoning, and Knowledge",Artificial Intelligence,
5455,W1988814833,LEARNING BAYESIAN BELIEF NETWORKS: AN APPROACH BASED ON THE MDL PRINCIPLE,"Wai Lam, Fahiem Bacchus","A new approach for learning Bayesian belief networks from raw data is presented. The approach is based on Rissanen's minimal description length (MDL) principle, which is particularly well suited for this task. Our approach does not require any prior assumptions about the distribution being learned. In particular, our method can learn unrestricted multiply‐connected belief networks. Furthermore, unlike other approaches our method allows us to trade off accuracy and complexity in the learned model. This is important since if the learned model is very complex (highly connected) it can be conceptually and computationally intractable. In such a case it would be preferable to use a simpler model even if it is less accurate. The MDL principle offers a reasoned method for making this trade‐off. We also show that our method generalizes previous approaches based on Kullback cross‐entropy. Experiments have been conducted to demonstrate the feasibility of the approach.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
5457,W89983288,HUGIN: a shell for building Bayesian belief universes for expert systems,"Stig Kjær Andersen, Kristian G. Olesen, Finn V. Jensen, Frank Jensen","Causal probabilistic networks have proved to be a useful knowledge representation tool for modelling domains where causal relations in a broad sense are a natural way of relating domain objects and where uncertainty is inherited in these relations. This paper outlines an implementation the HUGIN shell - for handling a domain model expressed by a causal probabilistic network. The only topological restriction imposed on the network is that, it must not contain any directed loops. The approach is illustrated step by step by solving a genetic breeding problem. A graph representation of the domain model is interactively created by using instances of the basic network components-- nodes and arcs--as building blocks. This structure, together with the quantitative relations between nodes and their immediate causes expressed as conditional probabilities, are automatically transformed into a tree structure, a junction tree. Here a computationally efficient and conceptually simple algebra of Bayesian belief universes supports incorporation of new evidence, propagation of information, and calculation of revised beliefs in the states of the nodes in the network. Finally, as an exam ple of a real world application, MUNIN an expert system for electromyography is discussed.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
5547,W101663867,"Learning and Soft Computing: Support Vector Machines, Neural Networks, and Fuzzy Logic Models",Vojislav Kecman,"This textbook provides a thorough introduction to the field of learning from experimental data and soft computing. Support vector machines (SVM) and neural networks (NN) are the mathematical structures, or models, that underlie learning, while fuzzy logic systems (FLS) enable us to embed structured human knowledge into workable algorithms. The book assumes that it is not only useful, but necessary, to treat SVM, NN, and FLS as parts of a connected whole. Throughout, the theory and algorithms are illustrated by practical examples, as well as by problem sets and simulated experiments. This approach enables the reader to develop SVM, NN, and FLS in addition to understanding them. The book also presents three case studies: on NN-based control, financial time series analysis, and computer graphics. A solutions manual and all of the MATLAB programs needed for the simulated experiments are available.",Neural Networks and Applications,Artificial Intelligence,
5550,W134112423,Map cube,"Shashi Shekhar, Chang‐Tien Lu, X Tan, Sanjay Chawla, Ranga Raju Vatsavai","Advances in automated data collection are creating massive databases and a whole new field, Knowledge Discovery Databases (KDD), has emerged to develop new methods of managing and exploiting them. Geographic Data Mining and Knowledge Discovery is the interrogation of large databases using efficient computational methods. The unique challenges broug",,,
5551,W147860157,Scaling clustering algorithms to large databases,"Patricia Bradley, Usama M. Fayyad, Cory Reina","Practical clustering algorithms require multiple data scans to achieve convergence. For large databases, these scans become prohibitively expensive. We present a scalable clustering framework applicable to a wide class of iterative clustering. We require at most one scan of the database. In this work, the framework is instantiated and numerically justified with the popular K-Means clustering algorithm. The method is based on identifying regions of the data that are compressible, regions that must be maintained in memory, and regions that are discardable. The algorithm operates within the confines of a limited memory buffer. Empirical results demonstrate that the scalable scheme outperforms a sampling-based approach. In our scheme, data resolution is preserved to the extent possible based upon the size of the allocated memory buffer and the fit of current clustering model to the data. The framework is naturally extended to update multiple clustering models simultaneously. We empirically evaluate on synthetic and publicly available data sets.",Advanced Clustering Algorithms Research,Artificial Intelligence,
5552,W1483135265,SPRINT: A Scalable Parallel Classifier for Data Mining,"John Shafer, Rakesh Agrawal, Manish Mehta","Classification is an important data mining problem. Although classification is a wellstudied problem, most of the current classification algorithms require that all or a portion of the the entire dataset remain permanently in memory. This limits their suitability for mining over large databases. We present a new decision-tree-based classification algorithm, called SPRINT that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily parallelized, allowing many processors to work together to build a single consistent model. This parallelization, also presented here, exhibits excellent scalability as well. The combination of these characteristics makes the proposed algorithm an ideal tool for data mining.",Data Mining Algorithms and Applications,Information Systems,
5553,W1483839075,SPOOK: a system for probabilistic object-oriented knowledge representation,"Avi Pfeffer, Daphne Koller, Brian Milch, Ken T. Takusagawa","In previous work, we pointed out the limitations of standard Bayesian networks as a modeling framework for large, complex domains. We proposed a new, richly structured modeling language, Object-oriented Bayesian Networks, that we argued would be able to deal with such domains. However, it turns out that OOBNs are not expressive enough to model many interesting aspects of complex domains: the existence of specific named objects, arbitrary relations between objects, and uncertainty over domain structure. These aspects are crucial in real-world domains such as battlefield awareness. In this paper, we present SPOOK, an implemented system that addresses these limitations. SPOOK implements a more expressive language that allows it to represent the battlespace domain naturally and compactly. We present a new inference algorithm that utilizes the model structure in a fundamental way, and show empirically that it achieves orders of magnitude speedup over existing approaches.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
5555,W1484833499,Attribute-Oriented Induction in Relational Databases.,Yandong Cai,"It is beneficial as well as challenging to learn knowledge rules from relational databases because of the vast amount of knowledge implied in databases and the large amount of data stored in databases. In this thesis, we develop an attributeoriented induction method to extract characteristic rules and classification rules from relational databases. The method adopts the artificial intelligence from examples paradigm and applies an attribute-oriented concept tree ascending technique in the learning process which integrates database operations with the learning process and provides a simple, efficient way of learning from large databases. Conjunctive rules as well as restricted forms of disjunctive rules are learned using this method. Moreover, by incorporating statistical techniques, qualitative rules with quantitative information can be learned and noisy data and exceptional cases are elegantly handled. Our analysis of the algorithms indicates that attribute-oriented induction substantially reduces the complexity of database learning processes. A prototype database learning system, DBLEARN, has been designed and implemented; early experiments with the prototype system illustrate the promise of attribute-oriented learning in relational databases.",Data Management and Algorithms,Signal Processing,
5556,W1485156179,"Declarative Data Cleaning: Language, Model, and Algorithms","Héléna Galhardas, Daniela Florescu, Dennis Shasha, Eric Simon, Cristian-Augustin Saita","The problem of data cleaning, which consists of emoving inconsistencies and errors from original data sets, is well known in the area of decision support systems and data warehouses. However, for non-conventional applications, such as the migration of largely unstructured data into structured one, or the integration of heterogeneous scientific data sets in inter-discipl- inary fields (e.g., in environmental science), existing ETL (Extraction Transformation Loading) and data cleaning tools for writing data cleaning programs are insufficient. The main challenge with them is the design of a data flow graph that effectively generates clean data, and can perform efficiently on large sets of input data. The difficulty with them comes from (i) a lack of clear separation between the logical specification of data transformations and their physical implementation and (ii) the lack of explanation of cleaning results and user interaction facilities to tune a data cleaning program. This paper addresses these two problems and presents a language, an execution model and algorithms that enable users to express data cleaning specifications declaratively and perform the cleaning efficiently. We use as an example a set of bibliographic references used to construct the Citeseer Web site. The underlying data integration problem is to derive structured and clean textual records so that meaningful queries can be performed. Experimental results report on the assessement of the proposed framework for data cleaning.",Data Quality and Management,Management Science and Operations Research,
5558,W1487588218,Learning Probabilistic Models of Relational Structure,"Lise Getoor, Nir Friedman, Daphne Koller, Benjamin Taskar","Most real-world data is stored in relational form. In contrast, most statistical learning methods work with “flat” data representations, forcing us to convert our data into a form that loses much of the relational structure. The recently introduced framework of probabilistic relational models (PRMs) allows us to represent probabilistic models over multiple entities that utilize the relations between them. In this paper, we propose the use of probabilistic models not only for the attributes in a relational model, but for the relational structure itself. We propose two mechanisms for modeling structural uncertainty: reference uncertainty and existence uncertainty. We describe the appropriate conditions for using each model and present learning algorithms for each. We present experimental results showing that the learned models can be used to predict relational structure and, moreover, the observed relational structure can be used to provide better predictions for the attributes in the model.",Data Quality and Management,Management Science and Operations Research,
5559,W1491318920,Intelligent Rollups in Multidimensional OLAP Data,"Gayatri Sathe, Sunita Sarawagi","In this paper we propose a new operator for advanced exploration of large multidimensional databases. The proposed operator can automatically generalize from a specific problem case in detailed data and return the broadest context in which the problem occurs. Such a functionality would be useful to an analyst who after observing a problem case, say a drop in sales for a product in a store, would like to find the exact scope of the problem. With existing tools he would have to manually search around the problem tuple trying to draw a pattern. This process is both tedious and imprecise. Our proposed operator can automate these manual steps and return in a single step a compact and easy-to-interpret summary of all possible maximal generalizations along various roll-up paths around the case. We present a fle xible cost-based framework that can generalize various kinds of behaviour (not simply drops) while requiring little additional customization from the user. We design an algorithm that can work efficiently on large multidimensional hierarchical data cubes so as to be usable in an interactive setting.",Advanced Database Systems and Queries,Computer Networks and Communications,
5560,W1491761690,Enterprise knowledge management: the data quality approach,David Loshin,"1. Introduction 2. Who Owns Information? 3. Data Quality in Practice 4. Economic Framework of Data Quality and the Value Proposition 5. Dimensions of Data Quality 6. Statistical Process Control and the Improvement Cycle 7. Domains, Mappings, and Enterprise Reference Data 8. Data Quality Assertions and Business Rules 9. Measurement and Current State Assessment 10. Data Quality Requirements 11. Metadata, Guidelines, and Policy 12. Rule-Based Data Quality 13. Metadata and Rule Discovery 14. Data Cleansing 15. Root Cause Analysis and Supplier Management 16. Data Enrichment/Enhancement 17. Data Quality and Business Rules in Practice 18. Building the Data Quality Practice",Data Quality and Management,Management Science and Operations Research,
5562,W1493217831,Biclustering of expression data.,"Yao Cheng, George M. Church","An efficient node-deletion algorithm is introduced to find submatrices in expression data that have low mean squared residue scores and it is shown to perform well in finding co-regulation patterns in yeast and human. This introduces ""biclustering"", or simultaneous clustering of both genes and conditions, to knowledge discovery from expression data. This approach overcomes some problems associated with traditional clustering methods, by allowing automatic discovery of similarity based on a subset of attributes, simultaneous clustering of genes and conditions, and overlapped grouping that provides a better representation for genes with multiple functions or regulated by many factors.",Gene expression and cancer classification,Molecular Biology,
5564,W149472151,On biases in estimating multi-valued attributes,Igor Kononenko,"We analyse the basics of eleven measures for estimating the quality of the multivalued attributes. The values of information gain J-measure, gini-index and relevance tend to lin early increase with the number of values of an attribute. The values of gam-ratio dis tance measure, Relief and the weight of evidence decrease for informative attributes and increase for irrelevant attributes. The bias of the statistic tests based on the chi-square distribution is similar but these functions are not able to discriminate among the attributes of different quality. We also introduce a new func tion based on the MDL principle whose value slightly decreases with the increasing number of attributes values.",Rough Sets and Fuzzy Logic,Computational Theory and Mathematics,
5569,W2102831150,Toward integrating feature selection algorithms for classification and clustering,"Huan Liu, Lei Yu","This paper introduces concepts and algorithms of feature selection, surveys existing feature selection algorithms for classification and clustering, groups and compares different algorithms with a categorizing framework based on search strategies, evaluation criteria, and data mining tasks, reveals unattempted combinations, and provides guidelines in selecting feature selection algorithms. With the categorizing framework, we continue our efforts toward-building an integrated system for intelligent feature selection. A unifying platform is proposed as an intermediate step. An illustrative example is presented to show how existing feature selection algorithms can be integrated into a meta algorithm that can take advantage of individual algorithms. An added advantage of doing so is to help a user employ a suitable algorithm without knowing details of each algorithm. Some real-world applications are included to demonstrate the use of feature selection in data mining. We conclude this work by identifying trends and challenges of feature selection research and development.",Data Mining Algorithms and Applications,Information Systems,
5838,W1533179050,Statistical Methods for Research Workers,Ronald Aylmer Fisher,"The prime object of this book is to put into the hands of research workers, and especially of biologists, the means of applying statistical tests accurately to numerical data accumulated in their own laboratories or available in the literature.","Genetics, Bioinformatics, and Biomedical Research",Molecular Biology,
5839,W2156273867,Bayesian Inference in Statistical Analysis.,"Joseph B. Kadane, George E. P. Box, George C. Tiao",Nature of Bayesian Inference Standard Normal Theory Inference Problems Bayesian Assessment of Assumptions: Effect of Non-Normality on Inferences About a Population Mean with Generalizations Bayesian Assessment of Assumptions: Comparison of Variances Random Effect Models Analysis of Cross Classification Designs Inference About Means with Information from More than One Source: One-Way Classification and Block Designs Some Aspects of Multivariate Analysis Estimation of Common Regression Coefficients Transformation of Data Tables References Indexes.,Advanced Statistical Methods and Models,Statistics and Probability,
5840,W1951724000,Fitting Linear Mixed-Effects Models Using<b>lme4</b>,"Douglas M. Bates, Martin Mächler, Benjamin M. Bolker, Steve Walker","Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.",Data Analysis with R,Artificial Intelligence,
5841,W2098126593,Inference of Population Structure Using Multilocus Genotype Data,"Jonathan K. Pritchard, Matthew Stephens, Peter Donnelly","Abstract We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci—e.g., seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/~pritch/home.html.",Genetic diversity and population structure,Genetics,
5842,W2158196600,Multimodel Inference,"Kenneth P. Burnham, David R. Anderson","The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a “savvy” prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
5843,W1617328014,The PRISMA Extension Statement for Reporting of Systematic Reviews Incorporating Network Meta-analyses of Health Care Interventions: Checklist and Explanations,"Brian Hutton, Georgia Salanti, Deborah M Caldwell, Anna Chaimani, Christopher H. Schmid, Chris Cameron, John P. A. Ioannidis, Sharon E. Straus, Kristian Thorlund, Jeroen P. Jansen, Cynthia D. Mulrow, Ferrán Catalá-López, Peter C Gøtzsche, Kay Dickersin, Isabelle Boutron, Douglas G. Altman, David Moher","The PRISMA statement is a reporting guideline designed to improve the completeness of reporting of systematic reviews and meta-analyses. Authors have used this guideline worldwide to prepare their reviews for publication. In the past, these reports typically compared 2 treatment alternatives. With the evolution of systematic reviews that compare multiple treatments, some of them only indirectly, authors face novel challenges for conducting and reporting their reviews. This extension of the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) statement was developed specifically to improve the reporting of systematic reviews incorporating network meta-analyses. A group of experts participated in a systematic review, Delphi survey, and face-to-face discussion and consensus meeting to establish new checklist items for this extension statement. Current PRISMA items were also clarified. A modified, 32-item PRISMA extension checklist was developed to address what the group considered to be immediately relevant to the reporting of network meta-analyses. This document presents the extension and provides examples of good reporting, as well as elaborations regarding the rationale for new checklist items and the modification of previously existing items from the PRISMA statement. It also highlights educational information related to key considerations in the practice of network meta-analysis. The target audience includes authors and readers of network meta-analyses, as well as journal editors and peer reviewers.",Meta-analysis and systematic reviews,"Statistics, Probability and Uncertainty",
5845,W2144673831,MCMC Methods for Multi-Response Generalized Linear Mixed Models: The<b>MCMCglmm</b><i>R</i>Package,Jarrod D. Hadfield,"Generalized linear mixed models provide a flexible framework for modeling a range of data, although with non-Gaussian response variables the likelihood cannot be obtained in closed form. Markov chain Monte Carlo methods solve this problem by sampling from a series of simpler conditional distributions that can be evaluated. The R package <b>MCMCglmm</b> implements such an algorithm for a range of model fitting problems. More than one response variable can be analyzed simultaneously, and these variables are allowed to follow Gaussian, Poisson, multi(bi)nominal, exponential, zero-inflated and censored distributions. A range of variance structures are permitted for the random effects, including interactions with categorical or continuous variables (i.e., random regression), and more complicated variance structures that arise through shared ancestry, either through a pedigree or through a phylogeny. Missing values are permitted in the response variable(s) and data can be known up to some level of measurement error as in meta-analysis. All simu- lation is done in C/ C++ using the <b>CSparse</b> library for sparse linear systems.",Statistical Methods and Bayesian Inference,Statistics and Probability,
6065,W1497163089,A Simple Relational Classifier,"Sofus A. Macskassy, Foster Provost","We analyze a Relational Neighbor (RN) classifier, a simple relational predictive model that predicts only based on class labels of related neighbors, using no learning and no inherent attributes.We show that it performs surprisingly well by comparing it to more complex models such as Probabilistic Relational Models and Relational Probability Trees on three data sets from published work.We argue that a simple model such as this should be used as a baseline to assess the performance of relational learners.",Data Mining Algorithms and Applications,Information Systems,
6066,W1511160855,Diffusion Kernels on Graphs and Other Discrete Input Spaces,"Risi Kondor, John Lafferty","The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.",Face and Expression Recognition,Computer Vision and Pattern Recognition,
6067,W1585529040,Introduction to Statistical Relational Learning,"Lise Getoor, Ben Taskar","Advanced statistical modeling and knowledge representation techniques for a newly emerging area of machine learning and probabilistic reasoning; includes introductory material, tutorials for different proposed approaches, and applications. Handling inherent uncertainty and exploiting compositional structure are fundamental to understanding and designing large-scale systems. Statistical relational learning builds on ideas from probability theory and statistics to address uncertainty while incorporating tools from logic, databases and programming languages to represent structure. In Introduction to Statistical Relational Learning, leading researchers in this emerging area of machine learning describe current formalisms, models, and algorithms that enable effective and robust reasoning about richly structured systems and data. The early chapters provide tutorials for material used in later chapters, offering introductions to representation, inference and learning in graphical models, and logic. The book then describes object-oriented approaches, including probabilistic relational models, relational Markov networks, and probabilistic entity-relationship models as well as logic-based formalisms including Bayesian logic programs, Markov logic, and stochastic logic programs. Later chapters discuss such topics as probabilistic models with unknown objects, relational dependency networks, reinforcement learning in relational domains, and information extraction. By presenting a variety of approaches, the book highlights commonalities and clarifies important differences among proposed approaches and, along the way, identifies important representational and algorithmic issues. Numerous applications are provided throughout.",Bayesian Modeling and Causal Inference,Artificial Intelligence,
6069,W2001325956,It's who you know,"Keith Henderson, Brian Gallagher, Lei Li, Leman Akoglu, Tina Eliassi‐Rad, Hanghang Tong, Christos Faloutsos","Given a graph, how can we extract good features for the nodes? For example, given two large graphs from the same domain, how can we use information in one to do classification in the other (i.e., perform across-network classification or transfer learning on graphs)? Also, if one of the graphs is anonymized, how can we use information in one to de-anonymize the other? The key step in all such graph mining tasks is to find effective node features. We propose ReFeX (Recursive Feature eXtraction), a novel algorithm, that recursively combines local (node-based) features with neighborhood (egonet-based) features; and outputs regional features -- capturing ""behavioral"" information. We demonstrate how these powerful regional features can be used in within-network and across-network classification and de-anonymization tasks -- without relying on homophily, or the availability of class labels. The contributions of our work are as follows: (a) ReFeX is scalable and (b) it is effective, capturing regional (""behavioral"") information in large graphs. We report experiments on real graphs from various domains with over 1M edges, where ReFeX outperforms its competitors on typical graph mining tasks like network classification and de-anonymization.",Advanced Graph Neural Networks,Artificial Intelligence,
6070,W2037933327,,"Diane Kelly, Nina Wacholder, Robert Rittman, Ying Sun, Paul B. Kantor, Sharon Small, Tomek Strzalkowski","In this article we discuss re-retrieving personal information objects and relate the task to recovering from lapse(s) in memory.We propose that fundamentally it is lapses in memory that impede users from successfully re-finding the information they need.Our hypothesis is that by learning more about memory lapses in non-computing contexts and how people cope and recover from these lapses, we can better inform the design of PIM tools and improve the user's ability to re-access and re-use objects.We describe a diary study that investigates the everyday memory problems of 25 people from a wide range of backgrounds.Based on the findings, we present a series of principles that we hypothesize will improve the design of personal information management tools.This hypothesis is validated by an evaluation of a tool for managing personal photographs, which was designed with respect to our findings.The evaluation suggests that users' performance when re-finding objects can be improved by building personal information management tools to support characteristics of human memory.retrieval -if they could remember everything that they once knew about an object then it would be simple to re-access it.To improve PIM systems we need to understand in more detail what people can remember, what strategies are successful for remembering and how we can design tools that better support personal information management.The role memory plays in PIM is non-trivial and involves different types of memory.For example, when re-retrieving an object from our personal stores our strategy may be based on the recollection of a property that object has (semantic memory), a previous experience with the object (autobiographical memory), a temporal reference to that object, such as when it was previously accessed, etc. Depending on the context of the search it may be easier for the searcher to utilise some types of memory over others, e.g. in email retrieval, it may be easier to remember who sent an email, when it was sent or what it said depending on properties of the email and the search.Thus, supporting PIM should, we argue, allow for searchers to utilise different types of memory in retrieval.Further, it is lapses in memory, such as a failure to recall the specific location, property, or source of an object that prevents successful reretrieval in PIM.For example, in the period shortly after an information object has been stored or accessed it can be re-accessed with ease because the recollection of the object and its location is lucid.However, popular theories of memory emphasize the transient nature of human memory; recollection diminishes over time [decay theory e.g.Rubin & Wenzel 1996] and focusing on other tasks and interaction with other objects can also degrade the recollection [interference theory e.g.Bower et al. 1994].We hypothesize that in order to ascertain which types of tool will be effective, and how existing tools can be changed to enhance rather than restrict human recall, it will be useful to investigate memory lapses in other contexts: what do people forget, why do they forget and what automated support might make the process of remembering easier?Further, as we show in section 4, there are similarities between memory lapses that people suffer from and learn to deal with effectively in everyday life and those that hinder PIM.Therefore, can lessons be learned from everyday behaviour with respect to improving PIM practises and tools?These are questions we address in this work.This article is divided into two main parts.In the first part, we report on a diary study that evaluates the variety, frequency and severity of everyday memory lapses.The study also explores the types of tasks that cause memory failure (or the memory failure to be reported), as well as the methods employed to recover from lapses.By comparing and contrasting the recorded memory problems and compensatory strategies with those that hinder PIM, we demonstrate restrictive aspects of existing PIM",Expert finding and Q&A systems,Information Systems,
6071,W2045107949,"Nearly-linear time algorithms for graph partitioning, graph sparsification, and solving linear systems","Daniel A. Spielman, Shang‐Hua Teng","We present algorithms for solving symmetric, diagonally-dominant linear systems to accuracy ε in time linear in their number of non-zeros and log (κf (A) ε), where κf (A) is the condition number of the matrix defining the linear system. Our algorithm applies the preconditioned Chebyshev iteration with preconditioners designed using nearly-linear time algorithms for graph sparsification and graph partitioning.",Matrix Theory and Algorithms,Computational Theory and Mathematics,
6072,W2046253692,Relational learning via latent social dimensions,"Lei Tang, Huan Liu","Social media such as blogs, Facebook, Flickr, etc., presents data in a network format rather than classical IID distribution. To address the interdependency among data instances, relational learning has been proposed, and collective inference based on network connectivity is adopted for prediction. However, connections in social media are often multi-dimensional. An actor can connect to another actor for different reasons, e.g., alumni, colleagues, living in the same city, sharing similar interests, etc. Collective inference normally does not differentiate these connections. In this work, we propose to extract latent social dimensions based on network information, and then utilize them as features for discriminative learning. These social dimensions describe diverse affiliations of actors hidden in the network, and the discriminative learning can automatically determine which affiliations are better aligned with the class labels. Such a scheme is preferred when multiple diverse relations are associated with the same network. We conduct extensive experiments on social media data (one from a real-world blog site and the other from a popular content sharing site). Our model outperforms representative relational learning methods based on collective inference, especially when few labeled data are available. The sensitivity of this model and its connection to existing methods are also examined.",Complex Network Analysis Techniques,Statistical and Nonlinear Physics,
6073,W2086254934,Local Graph Partitioning using PageRank Vectors,"Reid Andersen, Fan Chung, Kevin Lang","A local graph partitioning algorithm finds a cut near a specified starting vertex, with a running time that depends largely on the size of the small side of the cut, rather than the size of the input graph. In this paper, we present a local partitioning algorithm using a variation of PageRank with a specified starting distribution. We derive a mixing result for PageRank vectors similar to that for random walks, and show that the ordering of the vertices produced by a PageRank vector reveals a cut with small conductance. In particular, we show that for any set C with conductance Phi and volume k, a PageRank vector with a certain starting distribution can be used to produce a set with conductance (O(radic(Phi log k)). We present an improved algorithm for computing approximate PageRank vectors, which allows us to find such a set in time proportional to its size. In particular, we can find a cut with conductance at most oslash, whose small side has volume at least 2 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">b</sup> in time O(2 log m/(2 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">b</sup> log <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> m/oslash <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">2</sup> ) where m is the number of edges in the graph. By combining small sets found by this local partitioning algorithm, we obtain a cut with conductance oslash and approximately optimal balance in time O(m log <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">4</sup> m/oslash)",Algorithms and Data Compression,Artificial Intelligence,
6074,W2099352187,Using ghost edges for classification in sparsely labeled networks,"Brian Gallagher, Hanghang Tong, Tina Eliassi‐Rad, Christos Faloutsos","We address the problem of classification in partially labeled networks (a.k.a. within-network classification) where observed class labels are sparse. Techniques for statistical relational learning have been shown to perform well on network classification tasks by exploiting dependencies between class labels of neighboring nodes. However, relational classifiers can fail when unlabeled nodes have too few labeled neighbors to support learning (during training phase) and/or inference (during testing phase). This situation arises in real-world problems when observed labels are sparse.",Complex Network Analysis Techniques,Statistical and Nonlinear Physics,
6075,W2105543219,Scalable learning of collective behavior based on sparse social dimensions,"Lei Tang, Huan Liu","The study of collective behavior is to understand how individuals behave in a social network environment. Oceans of data generated by social media like Facebook, Twitter, Flickr and YouTube present opportunities and challenges to studying collective behavior in a large scale. In this work, we aim to learn to predict collective behavior in social media. In particular, given information about some individuals, how can we infer the behavior of unobserved individuals in the same network? A social-dimension based approach is adopted to address the heterogeneity of connections presented in social media. However, the networks in social media are normally of colossal size, involving hundreds of thousands or even millions of actors. The scale of networks entails scalable learning of models for collective behavior prediction. To address the scalability issue, we propose an edge-centric clustering scheme to extract sparse social dimensions. With sparse social dimensions, the social-dimension based approach can efficiently handle networks of millions of actors while demonstrating comparable prediction performance as other non-scalable methods.",Complex Network Analysis Techniques,Statistical and Nonlinear Physics,
6076,W2107792892,On the Foundations of Relaxation Labeling Processes,"Robert A. Hummel, Steven W. Zucker","A large class of problems can be formulated in terms of the assignment of labels to objects. Frequently, processes are needed which reduce ambiguity and noise, and select the best label among several possible choices. Relaxation labeling processes are just such a class of algorithms. They are based on the parallel use of local constraints between labels. This paper develops a theory to characterize the goal of relaxation labeling. The theory is founded on a definition of con-sistency in labelings, extending the notion of constraint satisfaction. In certain restricted circumstances, an explicit functional exists that can be maximized to guide the search for consistent labelings. This functional is used to derive a new relaxation labeling operator. When the restrictions are not satisfied, the theory relies on variational cal-culus. It is shown that the problem of finding consistent labelings is equivalent to solving a variational inequality. A procedure nearly identical to the relaxation operator derived under restricted circum-stances serves in the more general setting. Further, a local convergence result is established for this operator. The standard relaxation labeling formulas are shown to approximate our new operator, which leads us to conjecture that successful applications of the standard methods are explainable by the theory developed here. Observations about con-vergence and generalizations to higher order compatibility relations are described.",Constraint Satisfaction and Optimization,Computer Networks and Communications,
6077,W2118585731,LIBLINEAR: A Library for Large Linear Classification,"Rong-En Fan, Kai‐Wei Chang, Cho‐Jui Hsieh, Wang Xiang-rui, Chih‐Jen Lin",LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.,Face and Expression Recognition,Computer Vision and Pattern Recognition,
6078,W2121250409,Iterative Classification in Relational Data,"Jennifer Neville, David Jensen","Relational data offer a unique opportunity for improving the c lassification accuracy o f statistical m odels. If two objects are related, inferring something about one object can aid inferences about the other. We present an iterative classification p rocedure that exploits this characteristic of relational data. This approach uses simple Bayesian classifiers in an iterative fashion, dynamically upd ating the attributes of some objects as inferences are made about related ob jects. Inferences made with h igh confidence in initial iterations are fed back into the data and are used to inform subsequent i nferences about related ob jects. We evaluate the performance of this approach on a binary classification task. Experiments indicate that it erative classification significantly increases accuracy when compared to a single-pass approach.",Bayesian Methods and Mixture Models,Artificial Intelligence,
6079,W2122646361,Anomaly detection,"Varun Chandola, Arindam Banerjee, Vipin Kumar","Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.",Anomaly Detection Techniques and Applications,Artificial Intelligence,
6080,W3152893301,Graph neural networks: A review of methods and applications,"Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, Maosong Sun","Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.",Advanced Graph Neural Networks,Artificial Intelligence,
6081,W3100848837,Graph Convolutional Neural Networks for Web-Scale Recommender Systems,"Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, Jure Leskovec","Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We also develop an efficient MapReduce model inference algorithm to generate embeddings using a trained model. We deploy PinSage at Pinterest and train it on 7.5 billion examples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embeddings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.",Advanced Graph Neural Networks,Artificial Intelligence,
6082,W2624431344,Inductive Representation Learning on Large Graphs,"William L. Hamilton, Rex Ying, Jure Leskovec","Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",Bioinformatics and Genomic Networks,Molecular Biology,
6083,W3105705953,LINE,"Jian Tang, Meng Qu, Wang Ming-zhe, Ming Zhang, Jun Yan, Qiaozhu Mei","This paper studies the problem of embedding very large information networks into low-dimensional vector spaces, which is useful in many tasks such as visualization, node classification, and link prediction. Most existing graph embedding methods do not scale for real world information networks which usually contain millions of nodes. In this paper, we propose a novel network embedding method called the ``LINE,'' which is suitable for arbitrary types of information networks: undirected, directed, and/or weighted. The method optimizes a carefully designed objective function that preserves both the local and global network structures. An edge-sampling algorithm is proposed that addresses the limitation of the classical stochastic gradient descent and improves both the effectiveness and the efficiency of the inference. Empirical experiments prove the effectiveness of the LINE on a variety of real-world information networks, including language networks, social networks, and citation networks. The algorithm is very efficient, which is able to learn the embedding of a network with millions of vertices and billions of edges in a few hours on a typical single machine. The source code of the LINE is available online\footnote{\url{https://github.com/tangjianpku/LINE}}.",Advanced Graph Neural Networks,Artificial Intelligence,
6084,W2393319904,Structural Deep Network Embedding,"Daixin Wang, Peng Cui, Wenwu Zhu","Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE. More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.",Advanced Graph Neural Networks,Artificial Intelligence,
6139,W2296283641,Neural Architectures for Named Entity Recognition,"Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer","Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, Chris Dyer. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",Topic Modeling,Artificial Intelligence,
6312,W1598003989,A novel use of statistical parsing to extract information from text,"S.L. Miller, Heidi Fox, Lance Ramshaw, Ralph Weischedel","Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",Natural Language Processing Techniques,Artificial Intelligence,
6381,W2127314673,Distributional clustering of English words,"Fernando Pereira, Naftali Tishby, Lillian Lee","We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical ""soft"" clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.",Bayesian Methods and Mixture Models,Artificial Intelligence,
6385,W2131744502,Distributed Representations of Sentences and Documents,"Quoc V. Le, Tomáš Mikolov","Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, powerful, strong and Paris are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",Topic Modeling,Artificial Intelligence,
6508,W130850236,Name Tagging with Word Clusters and Discriminative Training,"S.L. Miller, Jethran Guinness, Alex Zamanian","We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. Cluster membership is encoded in features that are incorporated in a discriminatively trained tagging model. Active learning is used to select training examples. We evaluate the technique for named-entity tagging. Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance. Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material.",Natural Language Processing Techniques,Artificial Intelligence,
6509,W1505083828,A maximum entropy approach to named entity recognition,"Ralph Grishman, Andrew Borthwick","This thesis describes a novel statistical named-entity (i.e. “proper name”) recognition system known as “MENE” (Maximum Entropy Named Entity). Named entity (N.E.) recognition is a form of information extraction in which we seek to classify every word in a document as being a person-name, organization, location, date, time, monetary value, percentage, or “none of the above”. The task has particular significance for Internet search engines, machine translation, the automatic indexing of documents, and as a foundation for work on more complex information extraction tasks. 
Two of the most significant problems facing the constructor of a named entity system are the questions of portability and system performance. A practical N.E. system will need to be ported frequently to new bodies of text and even to new languages. The challenge is to build a system which can be ported with minimal expense (in particular minimal programming by a computational linguist) while maintaining a high degree of accuracy in the new domains or languages. 
MENE attempts to address these issues through the use of maximum entropy probabilistic modeling. It utilizes a very flexible object-based architecture which allows it to make use of a broad range of knowledge sources in making its tagging decisions. In the DARPA-sponsored MUC-7 named entity evaluation, the system displayed an accuracy rate which was well-above the median, demonstrating that it can achieve the performance goal. In addition, we demonstrate that the system can be used as a post-processing tool to enhance the output of a hand-coded named entity recognizer through experiments in which MENE improved on the performance of N.E. systems from three different sites. Furthermore, when all three external recognizers are combined under MENE, we are able to achieve very strong results which, in some cases, appear to be competitive with human performance. 
Finally, we demonstrate the trans-lingual portability of the system. We ported the system to two Japanese-language named entity tasks, one of which involved a new named entity category, “artifact”. Our results on these tasks were competitive with the best systems built by native Japanese speakers despite the fact that the author speaks no Japanese.",Natural Language Processing Techniques,Artificial Intelligence,
6510,W1528321674,Mathematical structures of language,Zellig S. Harris,"This book attempts to show how one can arrive at an abstract system which characterizes precisely natural language. This is done by taking the data of language and finding within the data such relations as can be organized into a suitable model. The problem here was not to find a broad mathematical system in which the structure of language could be included, but to find what relations, or rather relations among relations, which is an interpretation of the model, can do the work of natural language.",Advanced Algebra and Logic,Computational Theory and Mathematics,
6511,W1531859941,The necessity of syntactic parsing for semantic role labeling,"Vasin Punyakanok, Dan Roth, Wen-tau Yih","We provide an experimental study of the role of syntactic parsing in semantic role labeling. Our conclusions demonstrate that syntactic parse information is clearly most relevant in the very first stage - the pruning stage. In addition, the quality of the pruning stage cannot be determined solely based on its recall and precision. Instead it depends on the characteristics of the output candidates that make downstream problems easier or harder. Motivated by this observation, we suggest an effective and simple approach of combining different semantic role labeling systems through joint inference, which significantly improves the performance.",Natural Language Processing Techniques,Artificial Intelligence,
6512,W1535015163,A maximum-entropy-inspired parser,Eugene Charniak,"We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] standard sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a maximum-entropy-inspired model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head.",Natural Language Processing Techniques,Artificial Intelligence,
6514,W1581941705,Ranking the Best Instances,"Stephan Clémençon, Nicolas Vayatis","We formulate a local form of the bipartite ranking problem where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We first state the problem of finding the best instances which can be cast as a classification problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where first, the best instances would be tentatively identified and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem.",Advanced Statistical Methods and Models,Statistics and Probability,
6516,W1773803948,A Maximum Entropy Model for Part-Of-Speech Tagging,Adwait Ratnaparkhi,This paper presents a statistical model which trains from a corpus annotated with Part Of Speech tags and assigns them to previously unseen text with state of the art accuracy The model can be classi ed as a Maximum Entropy model and simultaneously uses many contextual features to predict the POS tag Furthermore this paper demonstrates the use of specialized fea tures to model di cult tagging decisions discusses the corpus consistency problems discovered during the implementation of these features and proposes a training strategy that mitigates these problems,Natural Language Processing Techniques,Artificial Intelligence,
6517,W1902568950,Global training of document processing systems using graph transformer networks,"Léon Bottou, Yoshua Bengio, Y. Le Cun","We propose a new machine learning paradigm called Graph Transformer Networks that extends the applicability of gradient-based learning algorithms to systems composed of modules that take graphs as inputs and produce graphs as output. Training is performed by computing gradients of a global objective function with respect to all the parameters in the system using a kind of back-propagation procedure. A complete check reading system based on these concepts is described. The system uses convolutional neural network character recognizers, combined with global training techniques to provide record accuracy on business and personal checks. It is presently deployed commercially and reads million of checks per month.",Graph Theory and Algorithms,Computer Vision and Pattern Recognition,
6518,W1963993153,Comparing and combining finite-state and context-free parsers,"Kristy Hollingshead, Seeger Fisher, Brian Roark","In this paper, we look at comparing high-accuracy context-free parsers with high-accuracy finite-state (shallow) parsers on several shallow parsing tasks. We show that previously reported comparisons greatly under-estimated the performance of context-free parsers for these tasks. We also demonstrate that context-free parsers can train effectively on relatively little training data, and are more robust to domain shift for shallow parsing tasks than has been previously reported. Finally, we establish that combining the output of context-free and finite-state parsers gives much higher results than the previous-best published results, on several common tasks. While the efficiency benefit of finite-state models is inarguable, the results presented here show that the corresponding cost in accuracy is higher than previously thought.",Natural Language Processing Techniques,Artificial Intelligence,
6520,W1988995507,Chunking with support vector machines,"Taku Kudo, Yūji Matsumoto","We apply Support Vector Machines (SVMs) to identify English base phrases (chunks). SVMs are known to achieve high generalization performance even with input data of high dimensional feature spaces. Furthermore, by the Kernel principle, SVMs can carry out training with smaller computational overhead independent of their dimensionality. We apply weighted voting of 8 SVMs-based systems trained with distinct chunk representations. Experimental results show that our approach achieves higher accuracy than previous approaches.",Natural Language Processing Techniques,Artificial Intelligence,
6521,W1990005915,Continuous speech recognition by statistical methods,F. Jelinek,"Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.",Speech Recognition and Synthesis,Artificial Intelligence,
6522,W2792764867,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun","For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .",Topic Modeling,Artificial Intelligence,
6523,W2604319603,Efficient Processing of Deep Neural Networks: A Tutorial and Survey,"Vivienne Sze, Yu‐Hsin Chen, Tien-Ju Yang, Joel Emer","Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
6524,W2963674932,Learning both weights and connections for efficient neural networks,"Song Han, Jeff Pool, John Tran, William J. Dally","Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,
6554,W2123084125,Noun classification from predicate-argument structures,Donald Hindle,"A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.",Natural Language Processing Techniques,Artificial Intelligence,
6692,W2436001372,Word sense disambiguation,Roberto Navigli,"Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.",Natural Language Processing Techniques,Artificial Intelligence,
6723,W2103318667,Contextual correlates of semantic similarity,"George A. Miller, Walter G. Charles","Abstract The relationship between semantic and contextual similarity is investigated for pairs of nouns that vary from high to low semantic similarity. Semantic similarity is estimated by subjective ratings; contextual similarity is estimated by the method of sorting sentential contexts. The results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation, is obtained for two separate corpora of sentence contexts. It is concluded that, on average, for words in the same language drawn from the same syntactic and semantic categories, the more often two words can be substituted into the same contexts the more similar in meaning they are judged to be.",Language and cultural evolution,Cultural Studies,
6822,W1509045587,Training and scaling preference functions for disambiguation,"Hiyan Alshawi, David Carter","We present an automatic method for weighting the contributions of preference functions used in disambiguation. Initial scaling factors are derived as the solution to a least squares minimization problem, and improvements are then made by hill climbing. The method is applied to disambiguating sentences in the Air Travel Information System corpus, and the performance of the resulting scaling factors is compared with hand-tuned factors. We then focus on one class of preference function, those based on semantic lexical collocations. Experimental results are presented showing that such functions vary considerably in selecting correct analyses. In particular, we define a function that performs significantly better than ones based on mutual information and likelihood ratios of lexical associations.",Natural Language Processing Techniques,Artificial Intelligence,
6825,W1997841190,Information Retrieval: Data Structures and Algorithms,"William B. Frakes, Ricardo Baeza–Yates","An edited volume containing data structures and algorithms for information retrieved including a disk with examples written in C. For programmers and students interested in parsing text, automated indexing, its the first collection in book form of the basic data structures and algorithms that are critical to the storage and retrieval of documents.",Algorithms and Data Compression,Artificial Intelligence,
6826,W2004131797,Toward memory-based reasoning,"Craig Stanfill, David L. Waltz",The intensive use of memory to recall specific episodes from the past—rather than rules—should be the foundation of machine reasoning.,Artificial Intelligence in Games,Artificial Intelligence,
6829,W2038826915,Generalizing automatically generated selectional patterns,"Ralph Grishman, John Sterling","Frequency information on co-occurrence patterns can be automatically collected from a syntactically analyzed corpus; this information can then serve as the basis for selectional constraints when analyzing new text from the same domain. This information, however, is necessarily incomplete. We report on measurements of the degree of selectional coverage obtained with different sizes of corpora. We then describe a technique for using the corpus to identify selectionally similar terms, and for using this similarity to broaden the selectional coverage for a fixed corpus size.",Natural Language Processing Techniques,Artificial Intelligence,
6830,W2082889357,INFORMATION RETRIEVAL BASED ON CONCEPTUAL DISTANCE IN IS‐A HIERARCHIES,"Joon Ho Lee, Myoung Ho Kim, Yoon Joon Lee","There have been several document ranking methods to calculate the conceptual distance or closeness between a Boolean query and a document. Though they provide good retrieval effectiveness in many cases, they do not support effective weighting schemes for queries and documents and also have several problems resulting from inappropriate evaluation of Boolean operators. We propose a new method called Knowledge‐Based Extended Boolean Model (kb‐ebm) in which Salton's extended Boolean model is incorporated. kb‐ebm evaluates weighted queries and documents effectively, and avoids the problems of the previous methods. kb‐ebm provides high quality document rankings by using term dependence information from is‐a hierarchies The performance experiments show that the proposed method closely simulates human behaviour.",Data Management and Algorithms,Signal Processing,
6831,W2087739686,Development and application of a metric on semantic nets,"Roy Rada, Hafedh Mili, Ellen J. Bicknell, M. Blettner","Motivated by the properties of spreading activation and conceptual distance, the authors propose a metric, called distance, on the power set of nodes in a semantic net. Distance is the average minimum path length over all pairwise combinations of nodes between two subsets of nodes. Distance can be successfully used to assess the conceptual distance between sets of concepts when used on a semantic net of hierarchical relations. When other kinds of relationships, like 'cause', are used, distance must be amended but then can again be effective. The judgements of distance significantly correlate with the distance judgements that people make and help to determine whether one semantic net is better or worse than another. The authors focus on the mathematical characteristics of distance that presents novel cases and interpretations. Experiments in which distance is applied to pairs of concepts and to sets of concepts in a hierarchical knowledge base show the power of hierarchical relations in representing information about the conceptual distance between concepts.< <ETX xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">&gt;</ETX>",Semantic Web and Ontologies,Artificial Intelligence,
6832,W2102381086,Introduction to WordNet: An On-line Lexical Database<sup>*</sup>,"George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, Katherine Miller","WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.",Natural Language Processing Techniques,Artificial Intelligence,
6833,W2109779030,Principle-based parsing without overgeneration,Dekang Lin,"Overgeneration is the main source of computational complexity in previous principle-based parsers. This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem. This algorithm has been implemented in C++ and successfully tested with example sentences from (van Riemsdijk and Williams, 1986).","Logic, programming, and type systems",Artificial Intelligence,
6834,W2117805756,Using Information Content to Evaluate Semantic Similarity in a Taxonomy,Philip Resnik,"This paper presents a new measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. Experimental evaluation suggests that the measure performs encouragingly well (a correlation of r = 0.79 with a benchmark set of human similarity judgments, with an upper bound of r = 0.90 for human subjects performing the same task), and significantly better than the traditional edge counting approach (r = 0.66).",Topic Modeling,Artificial Intelligence,
6835,W2136480620,Verbs semantics and lexical selection,"Zhibiao Wu, Martha Palmer","This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT). Two groups of English and Chinese verbs are examined to show that lexical selection must be based on interpretation of the sentences as well as selection restrictions placed on the verb arguments. A novel representation scheme is suggested, and is compared to representations with selection restrictions used in transfer-based MT. We see our approach as closely aligned with knowledge-based MT approaches (KBMT), and as a separate component that could be incorporated into existing systems. Examples and experimental results will show that, using this scheme, inexact matches can achieve correct lexical selection.",Natural Language Processing Techniques,Artificial Intelligence,
6838,W1659833910,Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language,Philip Resnik,"This article presents a measure of semantic similarity in an IS-A taxonomy based on the notion of shared information content. Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge-counting approach. The article presents algorithms that take advantage of taxonomic similarity in resolving syntactic and semantic ambiguity, along with experimental results demonstrating their effectiveness.",Natural Language Processing Techniques,Artificial Intelligence,
6839,W2120779048,Computing semantic relatedness using Wikipedia-based explicit semantic analysis,"Evgeniy Gabrilovich, Shaul Markovitch","Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.",Topic Modeling,Artificial Intelligence,
6840,W2136930489,Evaluating WordNet-based Measures of Lexical Semantic Relatedness,"Alexander Budanitsky, Graeme Hirst","The quantification of lexical semantic relatedness has many applications in NLP, and many different measures have been proposed. We evaluate five of these measures, all of which use WordNet as their central resource, by comparing their performance in detecting and correcting real-word spelling errors. An information-content-based measure proposed by Jiang and Conrath is found superior to those proposed by Hirst and St-Onge, Leacock and Chodorow, Lin, and Resnik. In addition, we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness.",Natural Language Processing Techniques,Artificial Intelligence,
6841,W68733909,"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics","Micah Hodosh, Peter Young, Julia Hockenmaier","The ability to associate images with natural language sentences that describe what is depicted in them is a hallmark of image understanding, and a prerequisite for applications such as sentence-based image search. In analogy to image search, we propose to frame sentence-based image annotation as the task of ranking a given pool of captions. We introduce a new benchmark collection for sentence-based image description and search, consisting of 8,000 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. We introduce a number of systems that perform quite well on this task, even though they are only based on features that can be obtained with minimal supervision. Our results clearly indicate the importance of training on multiple captions per image, and of capturing syntactic (word order-based) and semantic features of these captions. We also perform an in-depth comparison of human and automatic evaluation metrics for this task, and propose strategies for collecting human judgments cheaply and on a very large scale, allowing us to augment our collection with additional relevance judgments of which captions describe which image. Our analysis shows that metrics that consider the ranked list of results for each query image or sentence are significantly more robust than metrics that are based on a single response per query. Moreover, our study suggests that the evaluation of ranking-based image description systems may be fully automated.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,
6842,W1566018662,Corpus-based and knowledge-based measures of text semantic similarity,"Rada Mihalcea, Courtney D. Corley, Carlo Strapparava","This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (e.g. text classification, information retrieval) or individual words (e.g. synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (e.g. abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring the semantic similarity of short texts. Through experiments performed on a paraphrase data set, we show that the semantic similarity method out-performs methods based on simple lexical matching, resulting in up to 13% error rate reduction with respect to the traditional vector-based similarity metric.",Topic Modeling,Artificial Intelligence,
7185,W2963073614,Image-to-Image Translation with Conditional Adversarial Networks,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros","We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Moreover, since the release of the pi×2pi× software associated with this paper, hundreds of twitter users have posted their own artistic experiments using our system. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without handengineering our loss functions either.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,1.0
7186,W2965373594,RoBERTa: A Robustly Optimized BERT Pretraining Approach,"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov","Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.",Topic Modeling,Artificial Intelligence,2.0
7189,W2962739339,Deep Contextualized Word Representations,"Matthew E. Peters, Mark E Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer","Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.",Topic Modeling,Artificial Intelligence,5.0
7191,W1514535095,"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention","Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, Yoshua Bengio","Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,8.0
7193,W2525778437,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,"Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Thomas Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg S. Corrado, Macduff Hughes, Jay B. Dean","Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",Natural Language Processing Techniques,Artificial Intelligence,10.0
7194,W1522301498,Adam: A Method for Stochastic Optimization,"Diederik P. Kingma, Jimmy Ba","We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.",Stochastic Gradient Optimization Techniques,Artificial Intelligence,11.0
7196,W1903029394,Fully convolutional networks for semantic segmentation,"Jonathan Long, Evan Shelhamer, Trevor Darrell","Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build ""fully convolutional"" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,15.0
7197,W1905829557,"Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture","David Eigen, Rob Fergus","In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.",Advanced Vision and Imaging,Computer Vision and Pattern Recognition,16.0
7201,W2019969451,Data-driven hallucination of different times of day from a single outdoor photo,"YiChang Shih, Sylvain Paris, Frédo Durand, William T. Freeman","We introduce ""time hallucination"": synthesizing a plausible image at a different time of day from an input image. This challenging task often requires dramatically altering the color appearance of the picture. In this paper, we introduce the first data-driven approach to automatically creating a plausible-looking photo that appears as though it were taken at a different time of day. The time of day is specified by a semantic time label, such as ""night"". Our approach relies on a database of time-lapse videos of various scenes. These videos provide rich information about the variations in color appearance of a scene throughout the day. Our method transfers the color appearance from videos with a similar scene as the input photo. We propose a locally affine model learned from the video for the transfer, allowing our model to synthesize new color data while retaining image details. We show that this model can hallucinate a wide range of different times of day. The model generates a large sparse linear system, which can be solved by off-the-shelf solvers. We validate our methods by synthesizing transforming photos of various outdoor scenes to four times of interest: daytime, the golden hour, the blue hour, and nighttime.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,20.0
7203,W2083366168,Transient attributes for high-level understanding and editing of outdoor scenes,"Pierre-Yves Laffont, Zhile Ren, Xiaofeng Tao, Chao Qian, James Hays","We live in a dynamic visual world where the appearance of scenes changes dramatically from hour to hour or season to season. In this work we study ""transient scene attributes"" -- high level properties which affect scene appearance, such as ""snow"", ""autumn"", ""dusk"", ""fog"". We define 40 transient attributes and use crowdsourcing to annotate thousands of images from 101 webcams. We use this ""transient attribute database"" to train regressors that can predict the presence of attributes in novel images. We demonstrate a photo organization method based on predicted attributes. Finally we propose a high-level image editing method which allows a user to adjust the attributes of a scene, e.g. change a scene to be ""snowy"" or ""sunset"". To support attribute manipulation we introduce a novel appearance transfer technique which is simple and fast yet competitive with the state-of-the-art. We show that we can convincingly modify many transient attributes in outdoor scenes.",Image Enhancement Techniques,Computer Vision and Pattern Recognition,22.0
7207,W2099471712,GAN（Generative Adversarial Nets）,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio","We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,26.0
7208,W2100495367,Reducing the Dimensionality of Data with Neural Networks,"Geoffrey E. Hinton, Ruslan Salakhutdinov","High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.",Neural Networks and Applications,Artificial Intelligence,27.0
7209,W2116013899,Texture synthesis by non-parametric sampling,"Alexei A. Efros, Thomas Leung","A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures.",Image Retrieval and Classification Techniques,Computer Vision and Pattern Recognition,28.0
7210,W2125389028,Conditional Generative Adversarial Nets,"Mehdi Mirza, Simon Osindero","Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,30.0
7211,W2962793481,Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks,"Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros","Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F(G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,31.0
7212,W2962785568,The Unreasonable Effectiveness of Deep Features as a Perceptual Metric,"Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang","While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called ""perceptual losses""? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,32.0
7213,W4312933868,High-Resolution Image Synthesis with Latent Diffusion Models,"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer","By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,33.0
7214,W2593414223,Least Squares Generative Adversarial Networks,"Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley","Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson X2 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on LSUN and CIFAR-10 datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,34.0
7215,W2603777577,Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization,"Xun Huang, Serge Belongie","Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,35.0
7216,W2963800363,High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,"Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan Catanzaro","We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 × 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,36.0
7217,W2963767194,StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation,"Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo","Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,37.0
7218,W2765811365,Generative Adversarial Networks: An Overview,"Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, Anil A. Bharath","Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,38.0
7221,W1810943226,Generating Sequences With Recurrent Neural Networks,Alex Graves,"This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",Handwritten Text Recognition Techniques,Computer Vision and Pattern Recognition,44.0
7226,W2194775991,Deep Residual Learning for Image Recognition,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun","Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,83.0
7227,W2752782242,Squeeze-and-Excitation Networks,"Jie Hu, Li Shen, Gang Sun","Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the ""Squeeze-and-Excitation"" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251%, achieving a ~25% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,86.0
7228,W2560023338,Pyramid Scene Parsing Network,"Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia","Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,90.0
7229,W1686810756,Very Deep Convolutional Networks for Large-Scale Image Recognition,"Karen Simonyan, Andrew Zisserman","In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",Advanced Vision and Imaging,Computer Vision and Pattern Recognition,97.0
7234,W2048695508,Weighted Nuclear Norm Minimization with Application to Image Denoising,"Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng","As a convex relaxation of the low rank matrix factorization problem, the nuclear norm minimization has been attracting significant research interest in recent years. The standard nuclear norm minimization regularizes each singular value equally to pursue the convexity of the objective function. However, this greatly restricts its capability and flexibility in dealing with many practical problems (e.g., denoising), where the singular values have clear physical meanings and should be treated differently. In this paper we study the weighted nuclear norm minimization (WNNM) problem, where the singular values are assigned different weights. The solutions of the WNNM problem are analyzed under different weighting conditions. We then apply the proposed WNNM algorithm to image denoising by exploiting the image nonlocal self-similarity. Experimental results clearly show that the proposed WNNM algorithm outperforms many state-of-the-art denoising algorithms such as BM3D in terms of both quantitative measure and visual perception quality.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,308.0
7237,W1978749115,Nonlocally Centralized Sparse Representation for Image Restoration,"Weisheng Dong, Lei Zhang, Guangming Shi, Xin Li","Sparse representation models code an image patch as a linear combination of a few atoms chosen out from an over-complete dictionary, and they have shown promising results in various image restoration applications. However, due to the degradation of the observed image (e.g., noisy, blurred, and/or down-sampled), the sparse representations by conventional models may not be accurate enough for a faithful reconstruction of the original image. To improve the performance of sparse representation-based image restoration, in this paper the concept of sparse coding noise is introduced, and the goal of image restoration turns to how to suppress the sparse coding noise. To this end, we exploit the image nonlocal self-similarity to obtain good estimates of the sparse coding coefficients of the original image, and then centralize the sparse coding coefficients of the observed image to those estimates. The so-called nonlocally centralized sparse representation (NCSR) model is as simple as the standard sparse representation model, while our extensive experiments on various types of image restoration problems, including denoising, deblurring and super-resolution, validate the generality and state-of-the-art performance of the proposed NCSR algorithm.",Sparse and Compressive Sensing Techniques,Computational Mechanics,335.0
7238,W1606347560,Theano: new features and speed improvements,"Frédéric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua Bengio","Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.",Parallel Computing and Optimization Techniques,Hardware and Architecture,342.0
7239,W1909320841,Stochastic Backpropagation and Approximate Inference in Deep Generative Models,"Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra","We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,346.0
7240,W1959608418,Auto-Encoding Variational Bayes,"Diederik P. Kingma, Max Welling","Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,347.0
7241,W2954996726,A survey on Image Data Augmentation for Deep Learning,"Connor Shorten, Taghi M. Khoshgoftaar","Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,360.0
7247,W3121661546,EnlightenGAN: Deep Light Enhancement Without Paired Supervision,"Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Fang Chen, Xiaohui Shen, Shuicheng Yan, Pan Zhou, Zhangyang Wang","Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and the attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. Our codes and pre-trained models are available at: https://github.com/VITA-Group/EnlightenGAN.",Image Enhancement Techniques,Computer Vision and Pattern Recognition,471.0
7249,W2533598788,Automated Flower Classification over a Large Number of Classes,"Maria-Elena Nilsback, Andrew Zisserman","We investigate to what extent combinations of features can improve classification performance on a large dataset of similar classes. To this end we introduce a 103 class flower dataset. We compute four different features for the flowers, each describing different aspects, namely the local shape/texture, the shape of the boundary, the overall spatial distribution of petals, and the colour. We combine the features using a multiple kernel framework with a SVM classifier. The weights for each class are learnt using the method of Varma and Ray, which has achieved state of the art performance on other large dataset, such as Caltech 101/256. Our dataset has a similar challenge in the number of classes, but with the added difficulty of large between class similarity and small within class similarity. Results show that learning the optimum kernel combination of multiple features vastly improves the performance, from 55.1% for the best single feature to 72.8% for the combination of all features.",Smart Agriculture and AI,Plant Science,549.0
7250,W131533222,Automatically Constructing a Corpus of Sentential Paraphrases.,"William B. Dolan, Chris Brockett","An obstacle to research in automatic paraphrase identification and generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases. This paper describes the creation of the recently-released Microsoft Research Paraphrase Corpus, which contains 5801 sentence pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase. The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data. These pairs were then submitted to human judges, who confirmed that 67% were in fact semantically equivalent. In addition to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.",Topic Modeling,Artificial Intelligence,553.0
7252,W1599016936,The Winograd schema challenge,"Hector J. Levesque, Ernest Davis, Leora Morgenstern","In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.",Natural Language Processing Techniques,Artificial Intelligence,555.0
7254,W2170973209,Semi-supervised Sequence Learning,"Andrew M. Dai, Quoc V. Le","We present two approaches to use unlabeled data to improve Sequence Learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a language model in NLP. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a algorithm for a later supervised sequence learning algorithm. In other words, the parameters obtained from the pretraining step can then be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to achieve strong performance in many classification tasks, such as text classification with IMDB, DBpedia or image recognition in CIFAR-10.",Topic Modeling,Artificial Intelligence,558.0
7255,W2251939518,Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank,"Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, Christopher Potts","Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.",Sentiment Analysis and Opinion Mining,Artificial Intelligence,559.0
7256,W2462831000,Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units,"Dan Hendrycks, Kevin Gimpel","We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.",Adversarial Robustness in Machine Learning,Artificial Intelligence,561.0
7257,W2784121710,Fine-tuned Language Models for Text Classification.,"Jeremy Howard, Sebastian Ruder","Transfer learning has revolutionized computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Fine-tuned Language Models (FitLaM), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a state-of-the-art language model. Our method significantly outperforms the state-of-the-art on five text classification tasks, reducing the error by 18-24% on the majority of datasets. We open-source our pretrained models and code to enable adoption by the community.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,563.0
7258,W2805206884,A Simple Method for Commonsense Reasoning,"Trieu H. Trinh, Quoc V. Le","Commonsense reasoning is a long-standing challenge for deep learning. For example, it is difficult to use neural networks to tackle the Winograd Schema dataset (Levesque et al., 2011). In this paper, we present a simple method for commonsense reasoning with neural networks, using unsupervised learning. Key to our method is the use of language models, trained on a massive amount of unlabled data, to score multiple choice questions posed by commonsense reasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges, our models outperform previous state-of-the-art methods by a large margin, without using expensive annotated knowledge bases or hand-engineered features. We train an array of large RNN language models that operate at word or character level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a customized corpus for this task and show that diversity of training data plays an important role in test performance. Further analysis also shows that our system successfully discovers important features of the context that decide the correct answer, indicating a good grasp of commonsense knowledge.",Natural Language Processing Techniques,Artificial Intelligence,565.0
7259,W2899663614,Gaussian Error Linear Units (GELUs),"Dan Hendrycks, Kevin Gimpel","We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\Phi(x)$, where $\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.",Anomaly Detection Techniques and Applications,Artificial Intelligence,566.0
7260,W2914120296,Cross-lingual Language Model Pretraining,"Guillaume Lample, Alexis Conneau","Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",Natural Language Processing Techniques,Artificial Intelligence,568.0
7261,W2914526845,Multi-Task Deep Neural Networks for Natural Language Understanding,"Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao","In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations in order to adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement). We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.",Topic Modeling,Artificial Intelligence,569.0
7262,W2920812691,Cloze-driven Pretraining of Self-attention Networks,"Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli","We present a new approach for pretraining a bi-directional transformer model that provides significant performance gains across a variety of language understanding problems. Our model solves a cloze-style word reconstruction task, where each word is ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with the concurrently introduced BERT model. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective.",Natural Language Processing Techniques,Artificial Intelligence,570.0
7263,W2930786691,Reducing BERT Pre-Training Time from 3 Days to 76 Minutes,"Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, Cho‐Jui Hsieh","Training large deep neural networks on massive datasets is very challenging.
One promising approach to tackle this issue is through the use of large batch
stochastic optimization. However, our understanding of this approach in the
context of deep learning is still very limited. Furthermore, the current
approaches in this direction are heavily hand-tuned. To this end, we first
study a general adaptation strategy to accelerate training of deep neural
networks using large minibatches. Using this strategy, we develop a new
layer-wise adaptive large batch optimization technique called LAMB. We also
provide a formal convergence analysis of LAMB as well as the previous published
layerwise optimizer LARS, showing convergence to a stationary point in general
nonconvex settings. Our empirical results demonstrate the superior performance
of LAMB for BERT and ResNet-50 training. In particular, for BERT training, our
optimization technique enables use of very large batches sizes of 32868;
thereby, requiring just 8599 iterations to train (as opposed to 1 million
iterations in the original paper). By increasing the batch size to the memory
limit of a TPUv3 pod, BERT training time can be reduced from 3 days to 76
minutes. Finally, we also demonstrate that LAMB outperforms previous
large-batch training algorithms for ResNet-50 on ImageNet; obtaining
state-of-the-art performance in just a few minutes.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,571.0
7268,W2978017171,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf","As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",Topic Modeling,Artificial Intelligence,576.0
7269,W2996428491,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,"Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut","Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.",Topic Modeling,Artificial Intelligence,577.0
7271,W2980282514,HuggingFace's Transformers: State-of-the-art Natural Language Processing,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clément Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew","Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",Topic Modeling,Artificial Intelligence,579.0
7276,W1527575280,Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,"Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel","Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - ""blue"" + ""red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,609.0
7277,W1753482797,Recurrent Continuous Translation Models,"Nal Kalchbrenner, Phil Blunsom","We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units. The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.",Topic Modeling,Artificial Intelligence,611.0
7278,W1905882502,Deep visual-semantic alignments for generating image descriptions,"Andrej Karpathy, Li Fei-Fei","We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,614.0
7279,W2064675550,Long Short-Term Memory,"Sepp Hochreiter, Jürgen Schmidhuber","Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",Neural Networks and Applications,Artificial Intelligence,620.0
7280,W1591801644,Recurrent Neural Network Regularization,"Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals","We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.",Neural Networks and Applications,Artificial Intelligence,675.0
7281,W1895577753,Show and tell: A neural image caption generator,"Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan","Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,680.0
7287,W4225672218,Restormer: Efficient Transformer for High-Resolution Image Restoration,"Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming–Hsuan Yang","Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,777.0
7288,W2158899491,Natural Language Processing (almost) from Scratch,"Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel P. Kuksa","We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",Natural Language Processing Techniques,Artificial Intelligence,801.0
7290,W1902237438,Effective Approaches to Attention-based Neural Machine Translation,"Thang Luong, Hieu Pham, Christopher D. Manning","An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there has been little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1",Natural Language Processing Techniques,Artificial Intelligence,841.0
7293,W2963403868,Attention is All you Need,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin","The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms. We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",Natural Language Processing Techniques,Artificial Intelligence,961.0
7294,W2121879602,Japanese and Korean voice search,"Mike Schuster, Kaisuke Nakajima","This paper describes challenges and solutions for building a successful voice search system as applied to Japanese and Korean at Google. We describe the techniques used to deal with an infinite vocabulary, how modeling completely in the written domain for language model and dictionary can avoid some system complexity, and how we built dictionaries, language and acoustic models in this framework. We show how to deal with the difficulty of scoring results for multiple script languages because of ambiguities. The development of voice search for these languages led to a significant simplification of the original process to build a system for any new language which in in parts became our default process for internationalization of voice search.",Natural Language Processing Techniques,Artificial Intelligence,968.0
7297,W2799269579,Exploring the Limits of Weakly Supervised Pretraining,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, Laurens van der Maaten","State-of-the-art visual perception models for a wide range of tasks rely on supervised pretraining. ImageNet classification is the de facto pretraining task for these models. Yet, ImageNet is now nearly ten years old and is by modern standards small. Even so, relatively little is known about the behavior of pretraining with datasets that are multiple orders of magnitude larger. The reasons are obvious: such datasets are difficult to collect and annotate. In this paper, we present a unique study of transfer learning with large convolutional networks trained to predict hashtags on billions of social media images. Our experiments demonstrate that training for large-scale hashtag prediction leads to excellent results. We show improvements on several image classification and object detection tasks, and report the highest ImageNet-1k single-crop, top-1 accuracy to date: 85.4% (97.6% top-5). We also perform extensive experiments that provide novel empirical data on the relationship between large-scale pretraining and transfer learning performance.",Video Surveillance and Tracking Methods,Computer Vision and Pattern Recognition,1012.0
7298,W2931316642,VideoBERT: A Joint Model for Video and Language Representation Learning,"Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, Cordelia Schmid","Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,1013.0
7299,W2937843571,Attention Augmented Convolutional Networks,"Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, Quoc V. Le","Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a $1.3\%$ top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1014.0
7301,W2948798935,Learning Deep Transformer Models for Machine Translation,"Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, Lidia S. Chao","Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English- German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4-2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big.",Natural Language Processing Techniques,Artificial Intelligence,1016.0
7303,W2950739196,Image Transformer,"Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, Dustin Tran","Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,1018.0
7304,W2962843773,Revisiting Unreasonable Effectiveness of Data in Deep Learning Era,"Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta","The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been significant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10 × or 100 × ? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between 'enormous data' and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) findings. First, we find that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pre-training) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-the-art results for different vision tasks including image classification, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1019.0
7305,W2963631907,Adaptive Input Representations for Neural Language Modeling,"Alexei Baevski, Michael Auli","We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity.",Topic Modeling,Artificial Intelligence,1020.0
7306,W4313156423,Masked Autoencoders Are Scalable Vision Learners,"Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick","This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3× or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,1021.0
7309,W3170841864,Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,"Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, Li Zhang","Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1024.0
7310,W3207918547,SwinIR: Image Restoration Using Swin Transformer,"Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte","Image restoration is a long-standing low-level vision problem that aims to restore high-quality images from low-quality images (e.g., downscaled, noisy and compressed images). While state-of-the-art image restoration methods are based on convolutional neural networks, few attempts have been made with Transformers which show impressive performance on high-level vision tasks. In this paper, we propose a strong baseline model SwinIR for image restoration based on the Swin Transformer. SwinIR consists of three parts: shallow feature extraction, deep feature extraction and high-quality image reconstruction. In particular, the deep feature extraction module is composed of several residual Swin Transformer blocks (RSTB), each of which has several Swin Transformer layers together with a residual connection. We conduct experiments on three representative tasks: image super-resolution (including classical, lightweight and real-world image super-resolution), image denoising (including grayscale and color image denoising) and JPEG compression artifact reduction. Experimental results demonstrate that SwinIR outperforms state-of-the-art methods on different tasks by up to 0.14∼0.45dB, while the total number of parameters can be reduced by up to 67%.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1025.0
7311,W3121523901,Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet,"Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis E. H. Tay, Jiashi Feng, Shuicheng Yan","Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet. <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">1</sup>",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,1026.0
7312,W1850742715,DRAW: A Recurrent Neural Network For Image Generation,"Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, Daan Wierstra","This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",Generative Adversarial Networks and Image Synthesis,Computer Vision and Pattern Recognition,1133.0
7313,W2963420686,Squeeze-and-Excitation Networks,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, Enhua Wu","The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the ""Squeeze-and-Excitation"" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of -25 percent. Models and code are available at https://github.com/hujie-frank/SENet.",Force Microscopy Techniques and Applications,"Atomic and Molecular Physics, and Optics",1159.0
7314,W2549139847,Aggregated Residual Transformations for Deep Neural Networks,"Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He","We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,1170.0
7316,W1947481528,Long-term recurrent convolutional networks for visual recognition and description,"Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Trevor Darrell, Kate Saenko","Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or ""temporally deep"", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are ""doubly deep"" in that they can be compositional in spatial and temporal ""layers"". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,1280.0
7317,W2037642501,Image denoising: Can plain neural networks compete with BM3D?,"H. Burger, Christian J. Schuler, Stefan Harmeling","Image denoising can be described as the problem of mapping from a noisy image to a noise-free image. The best currently available denoising methods approximate this mapping with cleverly engineered algorithms. In this work we attempt to learn this mapping directly with a plain multi layer perceptron (MLP) applied to image patches. While this has been done before, we will show that by training on large image databases we are able to compete with the current state-of-the-art image denoising methods. Furthermore, our approach is easily adapted to less extensively studied types of noise (by merely exchanging the training data), for which we achieve excellent results as well.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,1282.0
7318,W2056370875,Image Denoising by Sparse 3-D Transform-Domain Collaborative Filtering,"Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian","We propose a novel image denoising strategy based on an enhanced sparse representation in transform domain. The enhancement of the sparsity is achieved by grouping similar 2D image fragments (e.g., blocks) into 3D data arrays which we call ""groups."" Collaborative Altering is a special procedure developed to deal with these 3D groups. We realize it using the three successive steps: 3D transformation of a group, shrinkage of the transform spectrum, and inverse 3D transformation. The result is a 3D estimate that consists of the jointly filtered grouped image blocks. By attenuating the noise, the collaborative filtering reveals even the finest details shared by grouped blocks and, at the same time, it preserves the essential unique features of each individual block. The filtered blocks are then returned to their original positions. Because these blocks are overlapping, for each pixel, we obtain many different estimates which need to be combined. Aggregation is a particular averaging procedure which is exploited to take advantage of this redundancy. A significant improvement is obtained by a specially developed collaborative Wiener filtering. An algorithm based on this novel denoising strategy and its efficient implementation are presented in full detail; an extension to color-image denoising is also developed. The experimental results demonstrate that this computationally scalable algorithm achieves state-of-the-art denoising performance in terms of both peak signal-to-noise ratio and subjective visual quality.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,1283.0
7319,W2955058313,Dual Attention Network for Scene Segmentation,"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, Hanqing Lu","In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1285.0
7320,W3177052299,Coordinate Attention for Efficient Mobile Network Design,"Qibin Hou, Daquan Zhou, Jiashi Feng","Recent studies on mobile network design have demonstrated the remarkable effectiveness of channel attention (e.g., the Squeeze-and-Excitation attention) for lifting model performance, but they generally neglect the positional information, which is important for generating spatially selective attention maps. In this paper, we propose a novel attention mechanism for mobile networks by embedding positional information into channel attention, which we call ""coordinate attention"". Unlike channel attention that transforms a feature tensor to a single feature vector via 2D global pooling, the coordinate attention factorizes channel attention into two 1D feature encoding processes that aggregate features along the two spatial directions, respectively. In this way, long-range dependencies can be captured along one spatial direction and meanwhile precise positional information can be preserved along the other spatial direction. The resulting feature maps are then encoded separately into a pair of direction-aware and position-sensitive attention maps that can be complementarily applied to the input feature map to augment the representations of the objects of interest. Our coordinate attention is simple and can be flexibly plugged into classic mobile networks, such as MobileNetV2, MobileNeXt, and EfficientNet with nearly no computational overhead. Extensive experiments demonstrate that our coordinate attention is not only beneficial to ImageNet classification but more interestingly, behaves better in down-stream tasks, such as object detection and semantic segmentation. Code is available at https://github.com/Andrew-Qibin/CoordAttention.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1287.0
7324,W4212875960,UNETR: Transformers for 3D Medical Image Segmentation,"Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett A. Landman, Holger R. Roth, Daguang Xu","Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful ""U-shaped"" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard.",Radiomics and Machine Learning in Medical Imaging,"Radiology, Nuclear Medicine and Imaging",1311.0
7325,W2125215748,The Role of Context for Object Detection and Semantic Segmentation in the Wild,"Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong‐Whan Lee, Sanja Fidler, Raquel Urtasun, Alan Yuille","In this paper we study the role of context in existing state-of-the-art detection and segmentation approaches. Towards this goal, we label every pixel of PASCAL VOC 2010 detection challenge with a semantic category. We believe this data will provide plenty of challenges to the community, as it contains 520 additional classes for semantic segmentation and object detection. Our analysis shows that nearest neighbor based approaches perform poorly on semantic segmentation of contextual classes, showing the variability of PASCAL imagery. Furthermore, improvements of existing contextual models for detection is rather modest. In order to push forward the performance in this difficult scenario, we propose a novel deformable part-based model, which exploits both local context around each candidate detection as well as global context at the level of the scene. We show that this contextual reasoning significantly helps in detecting objects at all scales.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1345.0
7326,W2598666589,Large Kernel Matters — Improve Semantic Segmentation by Global Convolutional Network,"Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, Jian Sun","One of recent trends [31, 32, 14] in network architecture design is stacking small filters (e.g., 1×1 or 3×3) in the entire network because the stacked small filters is more efficient than a large kernel, given the same computational complexity. However, in the field of semantic segmentation, where we need to perform dense per-pixel prediction, we find that the large kernel (and effective receptive field) plays an important role when we have to perform the classification and localization tasks simultaneously. Following our design principle, we propose a Global Convolutional Network to address both the classification and localization issues for the semantic segmentation. We also suggest a residual-based boundary refinement to further refine the object boundaries. Our approach achieves state-of-art performance on two public benchmarks and significantly outperforms previous results, 82.2% (vs 80.2%) on PASCAL VOC 2012 dataset and 76.9% (vs 71.8%) on Cityscapes dataset.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1346.0
7327,W2630837129,Rethinking Atrous Convolution for Semantic Image Segmentation,"Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam","In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.",Image Retrieval and Classification Techniques,Computer Vision and Pattern Recognition,1347.0
7328,W2799213142,DenseASPP for Semantic Segmentation in Street Scenes,"Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, Kuiyuan Yang","Semantic image segmentation is a basic street scene understanding task in autonomous driving, where each pixel in a high resolution image is categorized into a set of semantic labels. Unlike other scenarios, objects in autonomous driving scene exhibit very large scale changes, which poses great challenges for high-level feature representation in a sense that multi-scale information must be correctly encoded. To remedy this problem, atrous convolution[14]was introduced to generate features with larger receptive fields without sacrificing spatial resolution. Built upon atrous convolution, Atrous Spatial Pyramid Pooling (ASPP)[2] was proposed to concatenate multiple atrous-convolved features using different dilation rates into a final feature representation. Although ASPP is able to generate multi-scale features, we argue the feature resolution in the scale-axis is not dense enough for the autonomous driving scenario. To this end, we propose Densely connected Atrous Spatial Pyramid Pooling (DenseASPP), which connects a set of atrous convolutional layers in a dense way, such that it generates multi-scale features that not only cover a larger scale range, but also cover that scale range densely, without significantly increasing the model size. We evaluate DenseASPP on the street scene benchmark Cityscapes[4] and achieve state-of-the-art performance.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,1348.0
7329,W1906770428,Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration,"Yunjin Chen, Thomas Pock","Image restoration is a long-standing problem in low-level computer vision with many interesting applications. We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems. By embodying recent improvements in nonlinear diffusion models, we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters (\ie, linear filters and influence functions). In contrast to previous nonlinear diffusion models, all the parameters, including the filters and the influence functions, are simultaneously learned from training data through a loss based approach. We call this approach TNRD -- \textit{Trainable Nonlinear Reaction Diffusion}. The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force. We demonstrate its capabilities with three representative applications, Gaussian image denoising, single image super resolution and JPEG deblocking. Experiments show that our trained nonlinear diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for the tested applications. Our trained models preserve the structural simplicity of diffusion models and take only a small number of diffusion steps, thus are highly efficient. Moreover, they are also well-suited for parallel computation on GPUs, which makes the inference procedure extremely fast.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1355.0
7331,W1811400895,Super-resolution from image sequences-a review,"Sean Borman, Robert Louis Stevenson",Growing interest in super-resolution (SR) restoration of video sequences and the closed related problem of construction of SR still images from image sequences has led to the emergence of several competing methodologies. We review the state of the art of SR techniques using a taxonomy of existing techniques. We critique these methods and identified areas which promise performance improvements.,Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1430.0
7334,W1910615622,Edge-directed interpolation,"Jan P. Allebach, Ping Wah Wong","We present a new method for digitally interpolating images to higher resolution. It consists of two phases: rendering and correction. The rendering phase is edge-directed. From the low resolution image data, we generate a high resolution edge map by first filtering with a rectangular center-on-surround-off filter and then performing piecewise linear interpolation between the zero crossings in the filter output. The rendering phase is based on bilinear interpolation modified to prevent interpolation across edges, as determined from the estimated high resolution edge map. During the correction phase, we modify the mesh values on which the rendering is based to account for the disparity between the true low resolution data, and that predicted by a sensor model operating on the high resolution output of the rendering phase. The overall process is repeated iteratively. We show experimental results which demonstrate the efficacy of our interpolation method.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1433.0
7335,W1949096787,Jointly Optimized Regressors for Image Super‐resolution,"Dengxin Dai, Radu Timofte, Luc Van Gool","Abstract Learning regressors from low‐resolution patches to high‐resolution patches has shown promising results for image super‐resolution. We observe that some regressors are better at dealing with certain cases, and others with different cases. In this paper, we jointly learn a collection of regressors, which collectively yield the smallest super‐resolving error for all training data. After training, each training sample is associated with a label to indicate its ‘best’ regressor, the one yielding the smallest error. During testing, our method bases on the concept of ‘adaptive selection’ to select the most appropriate regressor for each input patch. We assume that similar patches can be super‐resolved by the same regressor and use a fast, approximate kNN approach to transfer the labels of training patches to test patches. The method is conceptually simple and computationally efficient, yet very effective. Experiments on four datasets show that our method outperforms competing methods.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1434.0
7337,W1977581467,Single image super-resolution using Gaussian process regression,"He He, Wan-Chi Siu","In this paper we address the problem of producing a high-resolution image from a single low-resolution image without any external training set. We propose a framework for both magnification and deblurring using only the original low-resolution image and its blurred version. In our method, each pixel is predicted by its neighbors through the Gaussian process regression. We show that when using a proper covariance function, the Gaussian process regression can perform soft clustering of pixels based on their local structures. We further demonstrate that our algorithm can extract adequate information contained in a single low-resolution image to generate a high-resolution image with sharp edges, which is comparable to or even superior in quality to the performance of other edge-directed and example-based super-resolution algorithms. Experimental results also show that our approach maintains high-quality performance at large magnifications.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1437.0
7338,W1983781364,Very low resolution face recognition problem,"W. W. Zou Wilman, Pong C. Yuen","This paper addresses the very low resolution (VLR) problem in face recognition in which the resolution of face image to be recognized is lower than 16×16. The VLR problem happens in many surveillance camera-based applications and existing face recognition algorithms are not able to give satisfactory performance on VLR face image. While face super-resolution (SR) methods can be employed to enhance the resolution of the images, the existing learning-based face SR methods do not perform well on such a very low resolution face image. To overcome this problem, this paper models the SR problem under VLR case as a regression problem with two constraints. First, a new data constraint is design to perform the error measurement on high resolution image space which provides more detailed and discriminative information. Second, discriminative constraint is proposed and incorporated in the training stage so that the reconstructed HR image has higher discriminability. CMU-PIE, FRGC and surveillant camera face (SCface) databases are selected for experiments. Experimental results show that the proposed method outperforms the existing methods, in terms of image quality and recognition accuracy.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1438.0
7339,W1991867157,Landmark Image Super-Resolution by Retrieving Web Images,"Huanjing Yue, Xiaoyan Sun, Jingyu Yang, Feng Wu","This paper proposes a new super-resolution (SR) scheme for landmark images by retrieving correlated web images. Using correlated web images significantly improves the exemplar-based SR. Given a low-resolution (LR) image, we extract local descriptors from its up-sampled version and bundle the descriptors according to their spatial relationship to retrieve correlated high-resolution (HR) images from the web. Though similar in content, the retrieved images are usually taken with different illumination, focal lengths, and shot perspectives, resulting in uncertainty for the HR detail approximation. To solve this problem, we first propose aligning these images to the up-sampled LR image through a global registration, which identifies the corresponding regions in these images and reduces the mismatching. Second, we propose a structure-aware matching criterion and adaptive block sizes to improve the mapping accuracy between LR and HR patches. Finally, these matched HR patches are blended together by solving an energy minimization problem to recover the desired HR image. Experimental results demonstrate that our SR scheme achieves significant improvement compared with four state-of-the-art schemes in terms of both subjective and objective qualities.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1439.0
7340,W2035677848,Lanczos Filtering in One and Two Dimensions,Claude E. Duchon,"A Fourier method of filtering digital data called Lanczos filtering is described. Its principal feature is the use of “sigma factors” which significantly reduce the amplitude of the Gibbs oscillation. A pair of graphs is developed that can be used to determine filter response quality given the number of weights and the value of the cutoff frequency, the only two inputs required by the method. Examples of response functions in one and two dimensions are given and comparisons are made with response functions from other filters. The simplicity of calculating the weights and the adequate response make Lanczos filtering an attractive filtering method.",Scientific Research and Discoveries,Statistical and Nonlinear Physics,1440.0
7341,W2964101377,Residual Dense Network for Image Super-Resolution,"Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu","A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1442.0
7342,W2964125708,MemNet: A Persistent Memory Network for Image Restoration,"Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu","Recently, very deep convolutional neural networks (CNNs) have been attracting considerable attention in image restoration. However, as the depth grows, the longterm dependency problem is rarely realized for these very deep models, which results in the prior states/layers having little influence on the subsequent ones. Motivated by the fact that human thoughts have persistency, we propose a very deep persistent memory network (MemNet) that introduces a memory block, consisting of a recursive unit and a gate unit, to explicitly mine persistent memory through an adaptive learning process. The recursive unit learns multi-level representations of the current state under different receptive fields. The representations and the outputs from the previous memory blocks are concatenated and sent to the gate unit, which adaptively controls how much of the previous states should be reserved, and decides how much of the current state should be stored. We apply MemNet to three image restoration tasks, i.e., image denosing, super-resolution and JPEG deblocking. Comprehensive experiments demonstrate the necessity of the MemNet and its unanimous superiority on all three tasks over the state of the arts. Code is available at https://github.com/tyshiwo/MemNet.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1549.0
7343,W2739757502,NTIRE 2017 Challenge on Single Image Super-Resolution: Methods and Results,"Radu Timofte, Eirikur Agustsson, Luc Van Gool, Shuicheng Yan, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, Xintao Wang, Yapeng Tian, Ke Yu, Yulun Zhang, Shixiang Wu, Chao Dong, Liang Lin, Yu Qiao, Chen Change Loy, Woong Bae, Jaejun Yoo, Yoseob Han, Jong Chul Ye, Jae-Seok Choi, Munchurl Kim, Yuchen Fan, Jiahui Yu, Wei Han, Ding Liu, Haichao Yu, Shuicheng Yan, Humphrey Shi, Xinchao Wang, Thomas S. Huang, Yunjin Chen, Kai Zhang, Wangmeng Zuo, Zhimin Tang, Linkai Luo, Shaohui Li, Min Fu, Lei Cao, Wen Heng, Giang Bui, Truc Le, Ye Duan, Dacheng Tao, Ruxin Wang, Lin Xu, Jianxin Pang, Jinchang Xu, Yu Zhao, Xiangyu Xu, Jinshan Pan, Deqing Sun, Yu‐Jin Zhang, Xibin Song, Yuchao Dai, Xueying Qin, Xuan-Phung Huynh, Tiantong Guo, Hojjat Seyed Mousavi, Tiep H. Vu, Vishal Monga, Cristóvão Cruz, Karen Egiazarian, Vladimir Katkovnik, Rakesh Mehta, Arnav Jain, Abhinav Agarwalla, Ch V. Sai Praveen, Ruofan Zhou, Hongdiao Wen, Che Zhu, Zhiqiang Xia, Zhengtao Wang, Qi Guo","This paper reviews the first challenge on single image super-resolution (restoration of rich details in an low resolution image) with focus on proposed solutions and results. A new DIVerse 2K resolution image dataset (DIV2K) was employed. The challenge had 6 competitions divided into 2 tracks with 3 magnification factors each. Track 1 employed the standard bicubic downscaling setup, while Track 2 had unknown downscaling operators (blur kernel and decimation) but learnable through low and high res train images. Each competition had ∽100 registered participants and 20 teams competed in the final testing phase. They gauge the state-of-the-art in single image super-resolution.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1550.0
7344,W3171125843,Pre-Trained Image Processing Transformer,"Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao","As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,1679.0
7356,W2470673105,Hierarchical Attention Networks for Document Classification,"Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy","Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, Eduard Hovy. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.",Topic Modeling,Artificial Intelligence,1741.0
7357,W2282821441,"""Why Should I Trust You?""","Marco Túlio Ribeiro, Sameer Singh, Carlos Guestrin","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.",Explainable Artificial Intelligence (XAI),Artificial Intelligence,1902.0
7359,W1805346680,Cascades of Regression Tree Fields for Image Restoration,"Uwe Schmidt, Jeremy Jancsary, Sebastian Nowozin, Stefan Roth, Carsten Rother","Conditional random fields (CRFs) are popular discriminative models for computer vision and have been successfully applied in the domain of image restoration, especially to image denoising. For image deblurring, however, discriminative approaches have been mostly lacking. We posit two reasons for this: First, the blur kernel is often only known at test time, requiring any discriminative approach to cope with considerable variability. Second, given this variability it is quite difficult to construct suitable features for discriminative prediction. To address these challenges we first show a connection between common half-quadratic inference for generative image priors and Gaussian CRFs. Based on this analysis, we then propose a cascade model for image restoration that consists of a Gaussian CRF at each stage. Each stage of our cascade is semi-parametric, i.e., it depends on the instance-specific parameters of the restoration problem, such as the blur kernel. We train our model by loss minimization with synthetically generated training data. Our experiments show that when applied to non-blind image deblurring, the proposed approach is efficient and yields state-of-the-art restoration quality on images corrupted with synthetic and real blur. Moreover, we demonstrate its suitability for image denoising, where we achieve competitive results for grayscale and color images.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,2089.0
7360,W1915360731,On learning optimized reaction diffusion processes for effective image restoration,"Yunjin Chen, Wei Yu, Thomas Pock","For several decades, image restoration remains an active research topic in low-level computer vision and hence new approaches are constantly emerging. However, many recently proposed algorithms achieve state-of-the-art performance only at the expense of very high computation time, which clearly limits their practical relevance. In this work, we propose a simple but effective approach with both high computational efficiency and high restoration quality. We extend conventional nonlinear reaction diffusion models by several parametrized linear filters as well as several parametrized influence functions. We propose to train the parameters of the filters and the influence functions through a loss based approach. Experiments show that our trained nonlinear reaction diffusion models largely benefit from the training of the parameters and finally lead to the best reported performance on common test datasets for image restoration. Due to their structural simplicity, our trained models are highly efficient and are also well-suited for parallel computation on GPUs.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,2090.0
7361,W1963882359,MatConvNet,"Andrea Vedaldi, Karel Lenc","MatConvNet is an open source implementation of Convolutional Neural Networks (CNNs) with a deep integration in the MATLAB environment. The toolbox is designed with an emphasis on simplicity and flexibility. It exposes the building blocks of CNNs as easy-to-use MATLAB functions, providing routines for computing convolutions with filter banks, feature pooling, normalisation, and much more. MatConvNet can be easily extended, often using only MATLAB code, allowing fast prototyping of new CNN architectures. At the same time, it supports efficient computation on CPU and GPU, allowing to train complex models on large datasets such as ImageNet ILSVRC containing millions of training examples",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,2091.0
7362,W1969210895,Residual interpolation for color image demosaicking,"Daisuke Kiku, Yusuke Monno, Masayuki Tanaka, Masatoshi Okutomi","A color difference interpolation technique is widely used for color image demosaicking. In this paper, we propose residual interpolation as an alternative to the color difference interpolation, where the residual is a difference between an observed and a tentatively estimated pixel value. We incorporate the proposed residual interpolation into the gradient based threshold free (GBTF) algorithm, which is one of current state-of-the-art demosaicking algorithms. Experimental results demonstrate that our proposed demosaicking algorithm using the residual interpolation can give state-of-the-art performance for the 30 images of Kodak and IMAX datasets.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,2092.0
7363,W2011181254,An Iterative Regularization Method for Total Variation-Based Image Restoration,"Stanley Osher, Martin Burger, Donald Goldfarb, Jinjun Xu, Wotao Yin","We introduce a new iterative regularization procedure for inverse problems based on the use of Bregman distances, with particular focus on problems arising in image processing. We are motivated by the problem of restoring noisy and blurry images via variational methods by using total variation regularization. We obtain rigorous convergence results and effective stopping criteria for the general procedure. The numerical results for denoising appear to give significant improvement over standard models, and preliminary results for deblurring/denoising are very encouraging.",Numerical methods in inverse problems,Mathematical Physics,2093.0
7364,W2037133587,Natural image denoising: Optimality and inherent bounds,"Anat Levin, Boaz Nadler","The goal of natural image denoising is to estimate a clean version of a given noisy image, utilizing prior knowledge on the statistics of natural images. The problem has been studied intensively with considerable progress made in recent years. However, it seems that image denoising algorithms are starting to converge and recent algorithms improve over previous ones by only fractional dB values. It is thus important to understand how much more can we still improve natural image denoising algorithms and what are the inherent limits imposed by the actual statistics of the data. The challenge in evaluating such limits is that constructing proper models of natural image statistics is a long standing and yet unsolved problem. To overcome the absence of accurate image priors, this paper takes a non parametric approach and represents the distribution of natural images using a huge set of 10 <sup xmlns:mml=""http://www.w3.org/1998/Math/MathML"" xmlns:xlink=""http://www.w3.org/1999/xlink"">10</sup> patches. We then derive a simple statistical measure which provides a lower bound on the optimal Bayesian minimum mean square error (MMSE). This imposes a limit on the best possible results of denoising algorithms which utilize a fixed support around a denoised pixel and a generic natural image prior. Our findings suggest that for small windows, state of the art denoising algorithms are approaching optimality and cannot be further improved beyond ~ 0.1dB values.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,2095.0
7365,W2051834767,Separable Markov Random Field Model and Its Applications in Low Level Vision,"Jian Sun, Marshall F. Tappen","This brief proposes a continuously-valued Markov random field (MRF) model with separable filter bank, denoted as MRFSepa, which significantly reduces the computational complexity in the MRF modeling. In this framework, we design a novel gradient-based discriminative learning method to learn the potential functions and separable filter banks. We learn MRFSepa models with 2-D and 3-D separable filter banks for the applications of gray-scale/color image denoising and color image demosaicing. By implementing MRFSepa model on graphics processing unit, we achieve real-time image denoising and fast image demosaicing with high-quality results.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,2096.0
7366,W2574952845,Deep Convolutional Neural Network for Inverse Problems in Imaging,"Kyong Hwan Jin, Michael T. McCann, Emmanuel Froustey, Michaël Unser","In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyper parameter selection. The starting point of our work is the observation that unrolled iterative methods have the form of a CNN (filtering followed by point-wise non-linearity) when the normal operator (H*H, the adjoint of H times H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill-posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512 x 512 image on GPU.",Image and Signal Denoising Methods,Computer Vision and Pattern Recognition,2097.0
7367,W3170697543,Multi-Stage Progressive Image Restoration,"Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming–Hsuan Yang, Ling Shao","Image restoration tasks demand a complex balance between spatial details and high-level contextualized information while recovering images. In this paper, we propose a novel synergistic design that can optimally balance these competing goals. Our main proposal is a multi-stage architecture, that progressively learns restoration functions for the degraded inputs, thereby breaking down the overall recovery process into more manageable steps. Specifically, our model first learns the contextualized features using encoder-decoder architectures and later combines them with a high-resolution branch that retains local information. At each stage, we introduce a novel per-pixel adaptive design that leverages in-situ supervised attention to reweight the local features. A key ingredient in such a multi-stage architecture is the information exchange between different stages. To this end, we propose a two-faceted approach where the information is not only exchanged sequentially from early to late stages, but lateral connections between feature processing blocks also exist to avoid any loss of information. The resulting tightly interlinked multi-stage architecture, named as MPRNet, delivers strong performance gains on ten datasets across a range of tasks including image deraining, deblurring, and denoising. The source code and pre-trained models are available at https://github.com/swz30/MPRNet.",Advanced Image Processing Techniques,Computer Vision and Pattern Recognition,2098.0
7368,W1586939924,Describing Videos by Exploiting Temporal Structure,"Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville","Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,2391.0
7369,W1687846465,Composing Simple Image Descriptions using Web-scale N-grams,"Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, Yejin Choi","Studying natural language, and especially how people describe the world around them can help us better understand the visual world. In turn, it can also help us in the quest to generate natural language that describes this world in a human manner. We present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams. Unlike most previous work that summarizes or retrieves pre-existing text relevant to an image, our method composes sentences entirely from scratch. Experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image, while permitting creativity in the description -- making for more human-like annotations than previous approaches.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,2392.0
7370,W1858383477,Corpus-Guided Sentence Generation of Natural Images,"Yezhou Yang, Ching L. Teo, Hal Daumé, Yiannis Aloimonos","We propose a sentence generation strategy that describes images by predicting the most likely nouns, verbs, scenes and prepositions that make up the core sentence structure. The input are initial noisy estimates of the objects and scenes detected in the image using state of the art trained detectors. As predicting actions from still images directly is unreliable, we use a language model trained from the English Gigaword corpus to obtain their estimates; together with probabilities of co-located nouns, scenes and prepositions. We use these estimates as parameters on a HMM that models the sentence generation process, with hidden nodes as sentence components and image detections as the emissions. Experimental results show that our strategy of combining vision and language produces readable and descriptive sentences compared to naive strategies that use vision alone.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,2393.0
7371,W1931639407,From captions to visual concepts and back,"Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K. Srivastava, Li Deng, Piotr Dollár, Jianfeng Gao, Xiaodong He, Margaret Mitchell, John Platt, C. Lawrence Zitnick, Geoffrey Zweig","This paper presents a novel approach for automatically generating image descriptions: visual detectors, language models, and multimodal similarity models learnt directly from a dataset of image captions. We use multiple instance learning to train visual detectors for words that commonly occur in captions, including many different parts of speech such as nouns, verbs, and adjectives. The word detector outputs serve as conditional inputs to a maximum-entropy language model. The language model learns from a set of over 400,000 image descriptions to capture the statistics of word usage. We capture global semantics by re-ranking caption candidates using sentence-level features and a deep multimodal similarity model. Our system is state-of-the-art on the official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When human judges compare the system captions to ones written by other people on our held-out test set, the system captions have equal or better quality 34% of the time.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,2394.0
7372,W1969616664,BabyTalk: Understanding and Generating Simple Image Descriptions,"Girish Kulkarni, Visruth Premraj, Vicente Ordóñez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, Tamara L. Berg","We present a system to automatically generate natural language descriptions from images. This system consists of two parts. The first part, content planning, smooths the output of computer vision-based detection and recognition algorithms with statistics mined from large pools of visually descriptive text to determine the best content words to use to describe an image. The second step, surface realization, chooses words to construct natural language sentences based on the predicted content and general statistics from natural language. We present multiple approaches for the surface realization step and evaluate each using automatic measures of similarity to human generated reference descriptions. We also collect forced choice human evaluations between descriptions from the proposed generation system and descriptions from competing approaches. The proposed system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,2395.0
7373,W603908379,Spatial transformer networks,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, Koray Kavukcuoglu","Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,2397.0
7374,W1485009520,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,"Xingjian Shi, Zhourong Chen, Hao Wang, Dit‐Yan Yeung, Wai Kin Wong, Wang‐chun Woo","The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",Meteorological Phenomena and Simulations,Atmospheric Science,2398.0
7375,W2745461083,Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering,"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Jay Gould, Lei Zhang","Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.",Multimodal Machine Learning Applications,Computer Vision and Pattern Recognition,2399.0
7376,W2144898279,Approximate Bayesian Inference for Latent Gaussian models by using Integrated Nested Laplace Approximations,"Håvard Rue, Sara Martino, Nicolás Chopin","Summary Structured additive regression models are perhaps the most commonly used class of models in statistical applications. It includes, among others, (generalized) linear models, (generalized) additive models, smoothing spline models, state space models, semiparametric regression, spatial and spatiotemporal models, log-Gaussian Cox processes and geostatistical and geoadditive models. We consider approximate Bayesian inference in a popular subset of structured additive regression models, latent Gaussian models, where the latent field is Gaussian, controlled by a few hyperparameters and with non-Gaussian response variables. The posterior marginals are not available in closed form owing to the non-Gaussian response variables. For such models, Markov chain Monte Carlo methods can be implemented, but they are not without problems, in terms of both convergence and computational time. In some practical applications, the extent of these problems is such that Markov chain Monte Carlo sampling is simply not an appropriate tool for routine analysis. We show that, by using an integrated nested Laplace approximation and its simplified version, we can directly compute very accurate approximations to the posterior marginals. The main benefit of these approximations is computational: where Markov chain Monte Carlo algorithms need hours or days to run, our approximations provide more precise estimates in seconds or minutes. Another advantage with our approach is its generality, which makes it possible to perform Bayesian analysis in an automatic, streamlined way, and to compute model comparison criteria and various predictive measures so that models can be compared and the model under study can be challenged.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,2529.0
7378,W4210257598,A Comprehensive Survey on Graph Neural Networks,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S. Yu","Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial-temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",Advanced Graph Neural Networks,Artificial Intelligence,2630.0
7380,W1968594024,Learning Recursive Distributed Representations for Holistic Computation,Lonnie Chrisman,"A number of connectionist models capable of representing data with compositional structure have recently appeared. These new models suggest the intriguing possibility of performing holistic structure-sensitive computations with distributed representations. Two possible forms of holistic inference, transformational inference and confluent inference, are identified and compared. Transformational inference was successfully demonstrated by Chalmers; however, the pure transformational approach does not consider the eventual inference tasks during the process of learning its representations. Confluent inference is introduced as a method for achieving a tight coupling between the distributed representations of a problem and the solution for the given inference task while the net is still learning its representations. A dual-ported RAAM architecture based on Pollack's Recursive Auto-Associative Memory is implemented and demonstrated in the domain of Natural Language translation.",Neural Networks and Applications,Artificial Intelligence,2693.0
7381,W2251743902,Multi-Task Learning for Multiple Language Translation,"Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, Haifeng Wang","Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, Haifeng Wang. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",Natural Language Processing Techniques,Artificial Intelligence,2694.0
7382,W2402144811,TensorFlow: a system for large-scale machine learning,"Martı́n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jay B. Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G. Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, Xiaoqiang Zheng","TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous parameter server designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,2695.0
7385,W2753160622,Optimization as a Model for Few-Shot Learning,"Sachin Ravi, Hugo Larochelle","Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning.",Domain Adaptation and Few-Shot Learning,Artificial Intelligence,2698.0
7386,W4385245566,Attention Is All You Need,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",Natural Language Processing Techniques,Artificial Intelligence,2836.0
7387,W1909234690,Scene labeling with LSTM recurrent neural networks,"Wonmin Byeon, Thomas M. Breuel, Federico Raue, Marcus Liwicki","This paper addresses the problem of pixel-level segmentation and classification of scene images with an entirely learning-based approach using Long Short Term Memory (LSTM) recurrent neural networks, which are commonly used for sequence classification. We investigate two-dimensional (2D) LSTM networks for natural scene images taking into account the complex spatial dependencies of labels. Prior methods generally have required separate classification and image segmentation stages and/or pre- and post-processing. In our approach, classification, segmentation, and context integration are all carried out by 2D LSTM networks, allowing texture and spatial model parameters to be learned within a single model. The networks efficiently capture local and global contextual information over raw RGB values and adapt well for complex scene images. Our approach, which has a much lower computational complexity than prior methods, achieved state-of-the-art performance over the Stanford Background and the SIFT Flow datasets. In fact, if no pre- or post-processing is applied, LSTM networks outperform other state-of-the-art approaches. Hence, only with a single-core Central Processing Unit (CPU), the running time of our approach is equivalent or better than the compared state-of-the-art approaches which use a Graphics Processing Unit (GPU). Finally, our networks' ability to visualize feature maps from each layer supports the hypothesis that LSTM networks are overall suited for image processing tasks.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,2868.0
7388,W1961891967,RGB-D Object Recognition via Incorporating Latent Data Structure and Prior Knowledge,"Jinhui Tang, Lu Jin, Zechao Li, Shenghua Gao","For the task of RGB-D object recognition, it is important to identify suitable representations of images, which can boost the performance of object recognition. In this work, we propose a novel representation learning method for RGB-D images by jointly incorporating the underlying data structure and the prior knowledge of the data. Specifically, the convolutional neural networks (CNN) are employed to learn image representation by exploiting the underlying data structure. To handle the problem of the limited RGB and depth images for object recognition, the multi-level hierarchies of features trained on ImageNet from the CNN are transferred to learn rich generic feature representation for RGB and depth images while the labeled images are leveraged. On the other hand, we propose a novel deep auto-encoders (DAE) to exploit the prior knowledge, which can overcome the expensive computational cost of optimization in feature encoding. The expected representations of images are obtained by integrating the two types of image representations. To verify the effectiveness of the proposed method, we thoroughly conduct extensive experiments on two publicly available RGB-D datasets. The encouraging experimental results compared with the state-of-the-art approaches demonstrate the advantages of the proposed method.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,2869.0
7389,W2563705555,RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation,"Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid","Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,2871.0
7391,W2597655663,A Structured Self-attentive Sentence Embedding,"Zhouhan Lin, Minwei Feng, Cícero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, Yoshua Bengio","This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.",Topic Modeling,Artificial Intelligence,2873.0
7392,W2623546809,Scene Segmentation with DAG-Recurrent Neural Networks,"Bing Shuai, Zhen Zuo, Bing Wang, Gang Wang","In this paper, we address the challenging task of scene segmentation. In order to capture the rich contextual dependencies over image regions, we propose Directed Acyclic Graph-Recurrent Neural Networks (DAG-RNN) to perform context aggregation over locally connected feature maps. More specifically, DAG-RNN is placed on top of pre-trained CNN (feature extractor) to embed context into local features so that their representative capability can be enhanced. In comparison with plain CNN (as in Fully Convolutional Networks-FCN), DAG-RNN is empirically found to be significantly more effective at aggregating context. Therefore, DAG-RNN demonstrates noticeably performance superiority over FCNs on scene segmentation. Besides, DAG-RNN entails dramatically less parameters as well as demands fewer computation operations, which makes DAG-RNN more favorable to be potentially applied on resource-constrained embedded devices. Meanwhile, the class occurrence frequencies are extremely imbalanced in scene segmentation, so we propose a novel class-weighted loss to train the segmentation network. The loss distributes reasonably higher attention weights to infrequent classes during network training, which is essential to boost their parsing performance. We evaluate our segmentation network on three challenging public scene segmentation benchmarks: Sift Flow, Pascal Context and COCO Stuff. On top of them, we achieve very impressive segmentation performance.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,2874.0
7393,W2745943519,Stacked Deconvolutional Network for Semantic Segmentation,"Jun Fu, Jing Liu, Yuhang Wang, Hanqing Lu","Recent progress in semantic segmentation has been driven by improving the spatial resolution under Fully Convolutional Networks (FCNs). To address this problem, we propose a Stacked Deconvolutional Network (SDN) for semantic segmentation. In SDN, multiple shallow deconvolutional networks, which are called as SDN units, are stacked one by one to integrate contextual information and guarantee the fine recovery of localization information. Meanwhile, inter-unit and intra-unit connections are designed to assist network training and enhance feature fusion since the connections improve the flow of information and gradient propagation throughout the network. Besides, hierarchical supervision is applied during the upsampling process of each SDN unit, which guarantees the discrimination of feature representations and benefits the network optimization. We carry out comprehensive experiments and achieve the new state-of-the-art results on three datasets, including PASCAL VOC 2012, CamVid, GATECH. In particular, our best model without CRF post-processing achieves an intersection-over-union score of 86.6% in the test set.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,2875.0
7394,W2798791840,Context Contrasted Feature and Gated Multi-scale Aggregation for Scene Segmentation,"Henghui Ding, Xudong Jiang, Bing Shuai, A. Q. Liu, Gang Wang","Scene segmentation is a challenging task as it need label every pixel in the image. It is crucial to exploit discriminative context and aggregate multi-scale features to achieve better segmentation. In this paper, we first propose a novel context contrasted local feature that not only leverages the informative context but also spotlights the local information in contrast to the context. The proposed context contrasted local feature greatly improves the parsing performance, especially for inconspicuous objects and background stuff. Furthermore, we propose a scheme of gated sum to selectively aggregate multi-scale features for each spatial position. The gates in this scheme control the information flow of different scale features. Their values are generated from the testing image by the proposed network learnt from the training data so that they are adaptive not only to the training data, but also to the specific testing image. Without bells and whistles, the proposed approach achieves the state-of-the-arts consistently on the three popular scene segmentation datasets, Pascal Context, SUN-RGBD and COCO Stuff.",Advanced Image and Video Retrieval Techniques,Computer Vision and Pattern Recognition,2876.0
7396,W3014641072,Deep High-Resolution Representation Learning for Visual Recognition,"Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, Bin Xiao","High-resolution representations are essential for position-sensitive vision problems, such as human pose estimation, semantic segmentation, and object detection. Existing state-of-the-art frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions <i>in series</i> (e.g., ResNet, VGGNet), and then recover the high-resolution representation from the encoded low-resolution representation. Instead, our proposed network, named as High-Resolution Network (HRNet), maintains high-resolution representations through the whole process. There are two key characteristics: (i) Connect the high-to-low resolution convolution streams <i>in parallel</i> and (ii) repeatedly exchange the information across resolutions. The benefit is that the resulting representation is semantically richer and spatially more precise. We show the superiority of the proposed HRNet in a wide range of applications, including human pose estimation, semantic segmentation, and object detection, suggesting that the HRNet is a stronger backbone for computer vision problems. All the codes are available at <uri>https://github.com/HRNet</uri>.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,2878.0
7397,W2981689412,CCNet: Criss-Cross Attention for Semantic Segmentation,"Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu","Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github.com/speedinghzl/CCNet.",Advanced Neural Network Applications,Computer Vision and Pattern Recognition,2879.0
7400,W2158196600,Multimodel Inference,"Kenneth P. Burnham, David R. Anderson","The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a “savvy” prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging.",Bayesian Modeling and Causal Inference,Artificial Intelligence,3068.0
7401,W2120611093,jModelTest: Phylogenetic Model Averaging,David Posada,"jModelTest is a new program for the statistical selection of models of nucleotide substitution based on ""Phyml"" (Guindon and Gascuel 2003. A simple, fast, and accurate algorithm to estimate large phylogenies by maximum likelihood. Syst Biol. 52:696–704.). It implements 5 different selection strategies, including ""hierarchical and dynamical likelihood ratio tests,"" the ""Akaike information criterion,"" the ""Bayesian information criterion,"" and a ""decision-theoretic performance-based"" approach. This program also calculates the relative importance and model-averaged estimates of substitution parameters, including a model-averaged estimate of the phylogeny. jModelTest is written in Java and runs under Mac OSX, Windows, and Unix systems with a Java Runtime Environment installed. The program, including documentation, can be freely downloaded from the software section at http://darwin.uvigo.es.",Genomics and Phylogenetic Studies,Molecular Biology,3069.0
7403,W2112315008,Novel methods improve prediction of species’ distributions from occurrence data,"Jane Elith, Catherine H. Graham, Robert P. Anderson, Miroslav Dudı́k, Simon Ferrier, Antoine Guisan, Robert J. Hijmans, Falk Huettmann, John R. Leathwick, Anthony Lehmann, Jin Li, Lúcia G. Lohmann, Bette A. Loiselle, Glenn Manion, Craig Moritz, Miguel Nakamura, Yoshinori Nakazawa, Jacob McC. Overton, A. Townsend Peterson, Steven J. Phillips, Karen Richardson, Ricardo Scachetti‐Pereira, Robert E. Schapire, Jorge Soberón, Stephen E. Williams, Mary S. Wisz, Niklaus E. Zimmermann","Prediction of species’ distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence‐only data to fit models, and independent presence‐absence data to evaluate the predictions. Along with well‐established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species’ distributions. These include machine‐learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species’ occurrence data. Presence‐only data were effective for modelling species’ distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.",Species Distribution and Climate Change,Ecological Modeling,3071.0
7404,W2111684210,A protocol for data exploration to avoid common statistical problems,"Alain F. Zuur, Elena N. Ieno, Chris S. Elphick","1. While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a random sample of their work (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniques employed. 2. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially troublesome in applied ecology, where management and policy decisions are often at stake. 3. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. 4. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance of making wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses.",Forest ecology and management,Nature and Landscape Conservation,3074.0
7405,W2097601813,Species Distribution Models: Ecological Explanation and Prediction Across Space and Time,"Jane Elith, John R. Leathwick","Species distribution models (SDMs) are numerical tools that combine observations of species occurrence or abundance with environmental estimates. They are used to gain ecological and evolutionary insights and to predict distributions across landscapes, sometimes requiring extrapolation in space and time. SDMs are now widely used across terrestrial, freshwater, and marine realms. Differences in methods between disciplines reflect both differences in species mobility and in “established use.” Model realism and robustness is influenced by selection of relevant predictors and modeling method, consideration of scale, how the interplay between environmental and geographic factors is handled, and the extent of extrapolation. Current linkages between SDM practice and ecological theory are often weak, hindering progress. Remaining challenges include: improvement of methods for modeling presence-only data and for model selection and evaluation; accounting for biotic interactions; and assessing model uncertainty.",Species Distribution and Climate Change,Ecological Modeling,3075.0
7406,W2135695572,A working guide to boosted regression trees,"Jane Elith, John R. Leathwick, Trevor Hastie","1 Ecologists use statistical models for both explanation and prediction, and need techniques that are flexible enough to express typical features of their data, such as nonlinearities and interactions. 2 This study provides a working guide to boosted regression trees (BRT), an ensemble method for fitting statistical models that differs fundamentally from conventional techniques that aim to fit a single parsimonious model. Boosted regression trees combine the strengths of two algorithms: regression trees (models that relate a response to their predictors by recursive binary splits) and boosting (an adaptive method for combining many simple models to give improved predictive performance). The final BRT model can be understood as an additive regression model in which individual terms are simple trees, fitted in a forward, stagewise fashion. 3 Boosted regression trees incorporate important advantages of tree-based methods, handling different types of predictor variables and accommodating missing data. They have no need for prior data transformation or elimination of outliers, can fit complex nonlinear relationships, and automatically handle interaction effects between predictors. Fitting multiple trees in BRT overcomes the biggest drawback of single tree models: their relatively poor predictive performance. Although BRT models are complex, they can be summarized in ways that give powerful ecological insight, and their predictive performance is superior to most traditional modelling methods. 4 The unique features of BRT raise a number of practical issues in model fitting. We demonstrate the practicalities and advantages of using BRT through a distributional analysis of the short-finned eel (Anguilla australis Richardson), a native freshwater fish of New Zealand. We use a data set of over 13 000 sites to illustrate effects of several settings, and then fit and interpret a model using a subset of the data. We provide code and a tutorial to enable the wider use of BRT by ecologists.",Fish Ecology and Management Studies,Nature and Landscape Conservation,3076.0
7407,W1217949068,Bayesian model assessment and selection using expected utilities,Aki Vehtari,"In this work, we discuss practical methods for the Bayesian model assessment and selection based on expected utilities, and propose several new methods and techniques for the analysis of the models. The Bayesian approach offers a consistent way to use probabilities to quantify uncertainty in inference resulting in a probability distribution expressing our beliefs regarding how likely the different predictions are. The use of Bayesian models in increasingly complex problems is facilitated by advances in Markov chain Monte Carlo methods and computing power. A natural way to assess the goodness of the model is to estimate its future predictive capability by estimating expected utilities. With application specific utilities, the expected benefit or the cost of using the model can be readily computed. We propose an approach using cross-validation predictive densities to compute the expected utilities and Bayesian bootstrap to obtain samples from their distributions. Instead of just making a point estimate, it is important to estimate the distribution of the expected utility, as it describes the uncertainty in the estimate. The distributions of the expected utilities can also be used to compare models, for example, by computing the probability of one model having a better expected utility than some other model. The expected utilities take into account how the model predictions are going to be used and thus may reveal that even the best model selected may be inadequate or not practically better than the previously used models. To make the model easier to analyse, or to reduce the cost of making measurements or computation, it may be useful to select a smaller set of input variables. Computing the cross-validation predictive densities for all possible input combinations easily becomes computationally prohibitive. We propose to use a variable dimension Markov chain Monte Carlo method to find out potentially useful input combinations, for which the final model choice and assessment is done using the cross-validation predictive densities. We demonstrate the usefulness of the presented approaches with MLP neural networks and Gaussian process models in three challenging real-world case problems.",Gaussian Processes and Bayesian Inference,Artificial Intelligence,3077.0
7408,W1223765834,Nested Hypothesis Testing: The Bayesian Reference Criterion,José M. Bernardo,"Abstract It is argued that hypothesis testing problems are best considered as decision problems concerning the choice of a usejul probability model. Decision theory, information measures and reference analysis, are combined to propose a non-subjective Bayesian approach to nested hypothesis testing, the Bayesian Reference Criterion (BRC). The results are compared both with frequentist based procedures, and with the use of Bayes factors. The theory is illustrated with stylized examples, where alternative approaches may easily be compared.",Advanced Statistical Methods and Models,Statistics and Probability,3078.0
7409,W128740895,Bayes Estimates for the Linear Model,"D. V. Lindley, A. F. M. Smith",Summary The usual linear statistical model is reanalyzed using Bayesian methods and the concept of exchangeability. The general method is illustrated by applications to two-factor experimental designs and multiple regression.,Advanced Statistical Methods and Models,Statistics and Probability,3079.0
7410,W141760394,Bayesian “Confidence Intervals” for the Cross-Validated Smoothing Spline,Grace Wahba,"SUMMARY We consider the model Y(ti)=g(ti) + ∈i, i = 1, 2, …, n, where g(t), t ∈ [0, 1] is a smooth function and the {∈i} are independent N(0, σ2) errors with σ2 unknown. The cross-validated smoothing spline can be used to estimate g non-parametrically from observations on Y(ti), i= 1, 2, …, n, and the purpose of this paper is to study confidence intervals for this estimate. Properties of smoothing splines as Bayes estimates are used to derive confidence intervals based on the posterior covariance function of the estimate. A small Monte Carlo study with the cubic smoothing spline is carried out to suggest by example to what extent the resulting 95 per cent confidence intervals can be expected to cover about 95 per cent of the true (but in practice unknown) values of g(ti), i= 1,2,…,n. The method was also applied to one example of a two-dimensional thin plate smoothing spline. An asymptotic theoretical argument is presented to explain why the method can be expected to work on fixed smooth functions (like those tried), which are “smoother” than the sample functions from the prior distributions on which the confidence interval theory is based.",Statistical Methods and Inference,Statistics and Probability,3080.0
7411,W14369269,Fisherian Inference in Likelihood and Prequential Frames of Reference,A. P. Dawid,"SUMMARY In celebration of the centenary of the birth of Sir Ronald Fisher, this paper explores Fisher's conception of statistical inference, with special attention to the importance he placed on choosing an appropriate frame of reference to define the inferential model. In particular, we investigate inferential models which respect the likelihood principle or the prequential principle, and argue that these will typically have an asymptotic sampling theory justification.",Advanced Statistical Methods and Models,Statistics and Probability,3081.0
7412,W1494853941,Bayes and Empirical Bayes Methods for Data Analysis,"Bradley P. Carlin, Thomas A. Louis",Approaches for Statistical Inference. The Bayes Approach. The Empirical Bayes Approach. Performance of Bayes Procedures. Bayesian Computation. Model Criticism and Selection. Special Methods and Models. Case Studies. Appendices.,Neural Networks and Applications,Artificial Intelligence,3082.0
7414,W1606684850,Bayesian Wavelet Analysis with a Model Complexity Prior,"Chris Holmes, D. G. T. Denison","Abstract Wavelet analysis has quickly established itself as a standard method for the analysis and smoothing of time series. Of particular importance is the application to the denoising and compression of signals (e.g. Bruce and Gao, 1996).",Fault Detection and Control Systems,Control and Systems Engineering,3088.0
7416,W1963648150,The Bayesian analysis of a pivotal pharmacokinetic study,"Nargis Rahman, J. C. Wakefield, David A. Stephens, Christine Falcoz","The aim of this paper is to carry out a detailed Bayesian population pharmacokinetic analysis of a three-period cross-over study of the drug fluticasone propionate carried out in 12 healthy male volunteers. The study was carried out to characterize the pharmacokinetics of the drug, in particular to investigate dose proportionality. We examine the appropriateness of modelling assumptions via a variety of diagnostic techniques. We also examine the effect of deleting time points at which the concentration was recorded as below the limit of quantification, as opposed to including these points as censored observations. We assess dose proportionality before carrying out a final combined analysis of data from all three doses.",Statistical Methods in Clinical Trials,Statistics and Probability,3090.0
7417,W1967992668,Limiting Behavior of Posterior Distributions when the Model is Incorrect,Robert H. Berk,"The large sample behavior of posterior distributions is examined without the assumption that the model is correct. Under certain conditions it is shown that asymptotically, the posterior distribution for a parameter $\theta$ is confined to a set (called the asymptotic carrier) which may, in general, contain more than one point. The asymptotic carrier depends on the model, the carrier of the prior distribution and the actual distribution of the observations. An example shows that, in general, there need be no convergence (in any sense) of the posterior distribution to a limiting distribution over the asymptotic carrier. This is in contrast to the (known) asymptotic behavior when the model is correct; see e.g. [7], p. 304: the asymptotic carrier then contains only one point, the ""true value"" of $\theta$ and the posterior distribution converges in distribution to the distribution degenerate at the ""true value.""",Statistical Methods and Bayesian Inference,Statistics and Probability,3093.0
7418,W1969175201,Bayesian Inference for Prevalence in Longitudinal Two‐Phase Studies,"Alaattin Erkanli, Refik Soyer, E. Jane Costello",Summary. We consider Bayesian inference and model selection for prevalence estimation using a longitudinal two‐phase design in which subjects initially receive a low‐cost screening test followed by an expensive diagnostic test conducted on several occasions. The change in the subject's diagnostic probability over time is described using four mixed‐effects probit models in which the subject‐specific effects are captured by latent variables. The computations are performed using Markov chain Monte Carlo methods. These models are then compared using the deviance information criterion. The methodology is illustrated with an analysis of alcohol and drug use in adolescents using data from the Great Smoky Mountains Study.,Statistical Methods and Bayesian Inference,Statistics and Probability,3094.0
7419,W1973464501,A note on the generalized information criterion for choice of a model,Anthony C. Atkinson,"Journal Article A note on the generalized information criterion for choice of a model Get access A. C. ATKINSON A. C. ATKINSON Department of Mathematics, Imperial CollegeLondon Search for other works by this author on: Oxford Academic Google Scholar Biometrika, Volume 67, Issue 2, 1980, Pages 413–418, https://doi.org/10.1093/biomet/67.2.413 Published: 01 August 1980 Article history Received: 01 August 1979 Revision received: 01 November 1979 Published: 01 August 1980",Statistical Methods and Inference,Statistics and Probability,3095.0
7420,W1974315578,Markov Chain Monte Carlo Methods for Computing Bayes Factors,"Cong Han, Bradley P. Carlin","The problem of calculating posterior probabilities for a collection of competing models and associated Bayes factors continues to be a formidable challenge for applied Bayesian statisticians. Current approaches that take advantage of modern Markov chain Monte Carlo computing methods include those that attempt to sample over some form of the joint space created by the model indicators and the parameters for each model, others that sample over the model space alone, and still others that attempt to estimate the marginal likelihood of each model directly (because the collection of these is equivalent to the collection of model probabilities themselves). We review several methods and compare them in the context of three examples: a simple regression example, a more challenging hierarchical longitudinal model, and a binary data latent variable model. We find that the joint model-parameter space search methods perform adequately but can be difficult to program and tune, whereas the marginal likelihood methods often are less troublesome and require less additional coding. Our results suggest that the latter methods may be most appropriate for practitioners working in many standard model choice settings, but the former remain important for comparing models of varying dimension (e.g., multiple changepoint models) or models whose parameters cannot easily be updated in relatively few blocks. We caution, however, that all methods we compared require significant human and computer effort, and this suggests that less formal Bayesian model choice methods may offer a more realistic alternative in many cases.",Bayesian Methods and Mixture Models,Artificial Intelligence,3096.0
7422,W1617328014,The PRISMA Extension Statement for Reporting of Systematic Reviews Incorporating Network Meta-analyses of Health Care Interventions: Checklist and Explanations,"Brian Hutton, Georgia Salanti, Deborah M Caldwell, Anna Chaimani, Christopher H. Schmid, Chris Cameron, John P. A. Ioannidis, Sharon E. Straus, Kristian Thorlund, Jeroen P. Jansen, Cynthia D. Mulrow, Ferrán Catalá-López, Peter C Gøtzsche, Kay Dickersin, Isabelle Boutron, Douglas G. Altman, David Moher","The PRISMA statement is a reporting guideline designed to improve the completeness of reporting of systematic reviews and meta-analyses. Authors have used this guideline worldwide to prepare their reviews for publication. In the past, these reports typically compared 2 treatment alternatives. With the evolution of systematic reviews that compare multiple treatments, some of them only indirectly, authors face novel challenges for conducting and reporting their reviews. This extension of the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-analyses) statement was developed specifically to improve the reporting of systematic reviews incorporating network meta-analyses. A group of experts participated in a systematic review, Delphi survey, and face-to-face discussion and consensus meeting to establish new checklist items for this extension statement. Current PRISMA items were also clarified. A modified, 32-item PRISMA extension checklist was developed to address what the group considered to be immediately relevant to the reporting of network meta-analyses. This document presents the extension and provides examples of good reporting, as well as elaborations regarding the rationale for new checklist items and the modification of previously existing items from the PRISMA statement. It also highlights educational information related to key considerations in the practice of network meta-analysis. The target audience includes authors and readers of network meta-analyses, as well as journal editors and peer reviewers.",Meta-analysis and systematic reviews,"Statistics, Probability and Uncertainty",3098.0
7424,W2144673831,MCMC Methods for Multi-Response Generalized Linear Mixed Models: The<b>MCMCglmm</b><i>R</i>Package,Jarrod D. Hadfield,"Generalized linear mixed models provide a flexible framework for modeling a range of data, although with non-Gaussian response variables the likelihood cannot be obtained in closed form. Markov chain Monte Carlo methods solve this problem by sampling from a series of simpler conditional distributions that can be evaluated. The R package <b>MCMCglmm</b> implements such an algorithm for a range of model fitting problems. More than one response variable can be analyzed simultaneously, and these variables are allowed to follow Gaussian, Poisson, multi(bi)nominal, exponential, zero-inflated and censored distributions. A range of variance structures are permitted for the random effects, including interactions with categorical or continuous variables (i.e., random regression), and more complicated variance structures that arise through shared ancestry, either through a pedigree or through a phylogeny. Missing values are permitted in the response variable(s) and data can be known up to some level of measurement error as in meta-analysis. All simu- lation is done in C/ C++ using the <b>CSparse</b> library for sparse linear systems.",Statistical Methods and Bayesian Inference,Statistics and Probability,3100.0
7427,W2034562813,Applied Regression Analysis,"Norman R. Draper, Harry Smith","Basic Prerequisite Knowledge. Fitting a Straight Line by Least Squares. Checking the Straight Line Fit. Fitting Straight Lines: Special Topics. Regression in Matrix Terms: Straight Line Case. The General Regression Situation. Extra Sums of Squares and Tests for Several Parameters Being Zero. Serial Correlation in the Residuals and the Durbin--Watson Test. More of Checking Fitted Models. Multiple Regression: Special Topics. Bias in Regression Estimates, and Expected Values of Mean Squares and Sums of Squares. On Worthwhile Regressions, Big F's, and R 2 . Models Containing Functions of the Predictors, Including Polynomial Models. Transformation of the Response Variable. Dummy Variables. Selecting the Best Regression Equation. Ill--Conditioning in Regression Data. Ridge Regression. Generalized Linear Models (GLIM). Mixture Ingredients as Predictor Variables. The Geometry of Least Squares. More Geometry of Least Squares. Orthogonal Polynomials and Summary Data. Multiple Regression Applied to Analysis of Variance Problems. An Introduction to Nonlinear Estimation. Robust Regression. Resampling Procedures (Bootstrapping). Bibliography. True/False Questions. Answers to Exercises. Tables. Indexes.",Multidisciplinary Science and Engineering Research,"Statistics, Probability and Uncertainty",3399.0
7431,W1584343945,Multimodel inference in ecology and evolution: challenges and solutions,"Catherine E. Grueber, Shinichi Nakagawa, Rebecca Laws, Ian G. Jamieson","Information theoretic approaches and model averaging are increasing in popularity, but this approach can be difficult to apply to the realistic, complex models that typify many ecological and evolutionary analyses. This is especially true for those researchers without a formal background in information theory. Here, we highlight a number of practical obstacles to model averaging complex models. Although not meant to be an exhaustive review, we identify several important issues with tentative solutions where they exist (e.g. dealing with collinearity amongst predictors; how to compute model-averaged parameters) and highlight areas for future research where solutions are not clear (e.g. when to use random intercepts or slopes; which information criteria to use when random factors are involved). We also provide a worked example of a mixed model analysis of inbreeding depression in a wild population. By providing an overview of these issues, we hope that this approach will become more accessible to those investigating any process where multiple variables impact an evolutionary or ecological response.",Ecology and Vegetation Dynamics Studies,Nature and Landscape Conservation,3435.0
7434,W1998025025,Collinearity: a review of methods to deal with it and a simulation study evaluating their performance,"Carsten F. Dormann, Jane Elith, Sven Bacher, Carsten M. Buchmann, Gudrun Carl, Gabriel Carré, Jaime Márquez, Bernd Gruber, Bruno Lafourcade, Pedro J. Leitão, Tamara Münkemüller, Colin J. McClean, Patrick E. Osborne, Björn Reineking, Boris Schröder, Andrew K. Skidmore, Damaris Zurell, Sven Lautenbach","Collinearity refers to the non independence of predictor variables, usually in a regression‐type analysis. It is a common feature of any descriptive ecological data set and can be a problem for parameter estimation because it inflates the variance of regression parameters and hence potentially leads to the wrong identification of relevant predictors in a statistical model. Collinearity is a severe problem when a model is trained on data from one region or time, and predicted to another with a different or unknown structure of collinearity. To demonstrate the reach of the problem of collinearity in ecology, we show how relationships among predictors differ between biomes, change over spatial scales and through time. Across disciplines, different approaches to addressing collinearity problems have been developed, ranging from clustering of predictors, threshold‐based pre‐selection, through latent variable methods, to shrinkage and regularisation. Using simulated data with five predictor‐response relationships of increasing complexity and eight levels of collinearity we compared ways to address collinearity with standard multiple regression and machine‐learning approaches. We assessed the performance of each approach by testing its impact on prediction to new data. In the extreme, we tested whether the methods were able to identify the true underlying relationship in a training dataset with strong collinearity by evaluating its performance on a test dataset without any collinearity. We found that methods specifically designed for collinearity, such as latent variable methods and tree based models, did not outperform the traditional GLM and threshold‐based pre‐selection. Our results highlight the value of GLM in combination with penalised methods (particularly ridge) and threshold‐based pre‐selection when omitted variables are considered in the final interpretation. However, all approaches tested yielded degraded predictions under change in collinearity structure and the ‘folk lore’‐thresholds of correlation coefficients between predictor variables of |r| &gt;0.7 was an appropriate indicator for when collinearity begins to severely distort model estimation and subsequent prediction. The use of ecological understanding of the system in pre‐analysis variable selection and the choice of the least sensitive statistical approaches reduce the problems of collinearity, but cannot ultimately solve them.",Ecology and Vegetation Dynamics Studies,Nature and Landscape Conservation,3482.0
7435,W1568201516,A statistical explanation of MaxEnt for ecologists,"Jane Elith, Steven J. Phillips, Trevor Hastie, Miroslav Dudı́k, Yung En Chee, Colin J. Yates","MaxEnt is a program for modelling species distributions from presence-only species records. This paper is written for ecologists and describes the MaxEnt model from a statistical perspective, making explicit links between the structure of the model, decisions required in producing a modelled distribution, and knowledge about the species and the data that might affect those decisions. To begin we discuss the characteristics of presence-only data, highlighting implications for modelling distributions. We particularly focus on the problems of sample bias and lack of information on species prevalence. The keystone of the paper is a new statistical explanation of MaxEnt which shows that the model minimizes the relative entropy between two probability densities (one estimated from the presence data and one, from the landscape) defined in covariate space. For many users, this viewpoint is likely to be a more accessible way to understand the model than previous ones that rely on machine learning concepts. We then step through a detailed explanation of MaxEnt describing key components (e.g. covariates and features, and definition of the landscape extent), the mechanics of model fitting (e.g. feature selection, constraints and regularization) and outputs. Using case studies for a Banksia species native to south-west Australia and a riverine fish, we fit models and interpret them, exploring why certain choices affect the result and what this means. The fish example illustrates use of the model with vector data for linear river segments rather than raster (gridded) data. Appropriate treatments for survey bias, unprojected data, locally restricted species, and predicting to environments outside the range of the training data are demonstrated, and new capabilities discussed. Online appendices include additional details of the model and the mathematical links between previous explanations and this one, example code and data, and further information on the case studies.",Species Distribution and Climate Change,Ecological Modeling,3484.0
7437,W1996835973,BIONJ: an improved version of the NJ algorithm based on a simple model of sequence data,Olivier Gascuel,"We propose an improved version of the neighbor-joining (NJ) algorithm of Saitou and Nei. This new algorithm, BIONJ, follows the same agglomerative scheme as NJ, which consists of iteratively picking a pair of taxa, creating a new mode which represents the cluster of these taxa, and reducing the distance matrix by replacing both taxa by this node. Moreover, BIONJ uses a simple first-order model of the variances and covariances of evolutionary distance estimates. This model is well adapted when these estimates are obtained from aligned sequences. At each step it permits the selection, from the class of admissible reductions, of the reduction which minimizes the variance of the new distance matrix. In this way, we obtain better estimates to choose the pair of taxa to be agglomerated during the next steps. Moreover, in comparison with NJ's estimates, these estimates become better and better as the algorithm proceeds. BIONJ retains the good properties of NJ--especially its low run time. Computer simulations have been performed with 12-taxon model trees to determine BIONJ's efficiency. When the substitution rates are low (maximum pairwise divergence approximately 0.1 substitutions per site) or when they are constant among lineages, BIONJ is only slightly better than NJ. When the substitution rates are higher and vary among lineages,BIONJ clearly has better topological accuracy. In the latter case, for the model trees and the conditions of evolution tested, the topological error reduction is on the average around 20%. With highly-varying-rate trees and with high substitution rates (maximum pairwise divergence approximately 1.0 substitutions per site), the error reduction may even rise above 50%, while the probability of finding the correct tree may be augmented by as much as 15%.",Metaheuristic Optimization Algorithms Research,Artificial Intelligence,3650.0
7438,W2013184405,Empirical Problems of the Hierarchical Likelihood Ratio Test for Model Selection,Diego Pol,"Advocates of maximum likelihood (ML) approaches to phylogenetics commonly cite as one of their primary advantages the use of objective statistical criteria for model selection. Currently, a particular implementation of the likelihood ratio test (LRT) is the most commonly used model-selection criterion in phylogenetics. This approach requires the choice of a starting point and a parameter addition (or removal) sequence that can affect all ML inferences (i.e., topology, model, and all evolutionary parameters). Here, several alternative starting points and parameter sequences are tested in empirical data sets to assess their influence on model selection and optimal topology. In the studied data sets, varying model-selection protocols leads to selection of different models that, in some cases, lead to different ML trees. Given the sensitivity of the LRT, some possible solutions to model selection (within the hypothesis testing approach) are outlined, and alternative model-selection criteria are discussed. Some of the suggested alternatives seem to lack these problems, although their behavior and adequacy for phylogenetics needs to be further explored.",Evolution and Paleontology Studies,Paleontology,3653.0
7439,W2023998798,Estimation of evolutionary distances between homologous nucleotide sequences.,Makoto Kimura,"By using two models of evolutionary base substitutions--""three-substitution-type"" and ""two-frequency-class"" models--some formulae are derived which permit a simple estimation of the evolutionary distances (and also the evolutionary rates when the divergence times are known) through comparative studies of DNA (and RNA) sequences. These formulae are applied to estimate the base substitution rates at the first, second, and third positions of codons in genes for presomatotropins, preproinsulins, and alpha- and beta-globins (using comparisons involving mammals). Also, formulae for estimating the synonymous component (at the third codon position) and the standard errors are obtained. It is pointed out that the rates of synonymous base substitutions not only are very high but also are roughly equal to each other between genes even when amino acid-altering substitution rates are quite different and that this is consistent with the neutral mutation-random drift hypothesis of molecular evolution.",Genomics and Phylogenetic Studies,Molecular Biology,3654.0
7440,W2082928585,MODELTEST: testing the model of DNA substitution.,"David Posada, Keith A. Crandall","The program MODELTEST uses log likelihood scores to establish the model of DNA evolution that best fits the data.The MODELTEST package, including the source code and some documentation is available at http://bioag.byu. edu/zoology/crandall_lab/modeltest.html.",Genomics and Phylogenetic Studies,Molecular Biology,3656.0
7441,W2099364951,Comparative Performance of Bayesian and AIC-Based Measures of Phylogenetic Model Uncertainty,"Michael E. Alfaro, John P. Huelsenbeck","Reversible-jump Markov chain Monte Carlo (RJ-MCMC) is a technique for simultaneously evaluating multiple related (but not necessarily nested) statistical models that has recently been applied to the problem of phylogenetic model selection. Here we use a simulation approach to assess the performance of this method and compare it to Akaike weights, a measure of model uncertainty that is based on the Akaike information criterion. Under conditions where the assumptions of the candidate models matched the generating conditions, both Bayesian and AIC-based methods perform well. The 95% credible interval contained the generating model close to 95% of the time. However, the size of the credible interval differed with the Bayesian credible set containing approximately 25% to 50% fewer models than an AIC-based credible interval. The posterior probability was a better indicator of the correct model than the Akaike weight when all assumptions were met but both measures performed similarly when some model assumptions were violated. Models in the Bayesian posterior distribution were also more similar to the generating model in their number of parameters and were less biased in their complexity. In contrast, Akaike-weighted models were more distant from the generating model and biased towards slightly greater complexity. The AIC–based credible interval appeared to be more robust to the violation of the rate homogeneity assumption. Both AIC and Bayesian approaches suggest that substantial uncertainty can accompany the choice of model for phylogenetic analyses, suggesting that alternative candidate models should be examined in analysis of phylogenetic data.",Evolution and Paleontology Studies,Paleontology,3658.0
7442,W2101846955,Model Selection and Model Averaging in Phylogenetics: Advantages of Akaike Information Criterion and Bayesian Approaches Over Likelihood Ratio Tests,"David Posada, Thomas R. Buckley","Model selection is a topic of special relevance in molecular phylogenetics that affects many, if not all, stages of phylogenetic inference. Here we discuss some fundamental concepts and techniques of model selection in the context of phylogenetics. We start by reviewing different aspects of the selection of substitution models in phylogenetics from a theoretical, philosophical and practical point of view, and summarize this comparison in table format. We argue that the most commonly implemented model selection approach, the hierarchical likelihood ratio test, is not the optimal strategy for model selection in phylogenetics, and that approaches like the Akaike Information Criterion (AIC) and Bayesian methods offer important advantages. In particular, the latter two methods are able to simultaneously compare multiple nested or nonnested models, assess model selection uncertainty, and allow for the estimation of phylogenies and model parameters using all available models (model-averaged inference or multimodel inference). We also describe how the relative importance of the different parameters included in substitution models can be depicted. To illustrate some of these points, we have applied AIC-based model averaging to 37 mitochondrial DNA sequences from the subgenus Ohomopterus (genus Carabus) ground beetles described by Sota and Vogler (2001).",Genetic diversity and population structure,Genetics,3659.0
7443,W2148698435,MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space,"Fredrik Ronquist, Maxim Teslenko, Paul van der Mark, Daniel L. Ayres, Aaron E. Darling, Sebastian Höhna, Bret Larget, Liang Liu, Marc A. Suchard, John P. Huelsenbeck","Since its introduction in 2001, MrBayes has grown in popularity as a software package for Bayesian phylogenetic inference using Markov chain Monte Carlo (MCMC) methods. With this note, we announce the release of version 3.2, a major upgrade to the latest official release presented in 2003. The new version provides convergence diagnostics and allows multiple analyses to be run in parallel with convergence progress monitored on the fly. The introduction of new proposals and automatic optimization of tuning parameters has improved convergence for many problems. The new version also sports significantly faster likelihood calculations through streaming single-instruction-multiple-data extensions (SSE) and support of the BEAGLE library, allowing likelihood calculations to be delegated to graphics processing units (GPUs) on compatible hardware. Speedup factors range from around 2 with SSE code to more than 50 with BEAGLE for codon problems. Checkpointing across all models allows long runs to be completed even when an analysis is prematurely terminated. New models include relaxed clocks, dating, model averaging across time-reversible substitution models, and support for hard, negative, and partial (backbone) tree constraints. Inference of species trees from gene trees is supported by full incorporation of the Bayesian estimation of species trees (BEST) algorithms. Marginal model likelihoods for Bayes factor tests can be estimated accurately across the entire model space using the stepping stone method. The new version provides more output options than previously, including samples of ancestral states, site rates, site dN/dS rations, branch rates, and node dates. A wide range of statistics on tree parameters can also be output for visualization in FigTree and compatible software.",Genomics and Phylogenetic Studies,Molecular Biology,3661.0
7444,W2133870991,PartitionFinder: Combined Selection of Partitioning Schemes and Substitution Models for Phylogenetic Analyses,"Robert Lanfear, Brett Calcott, Simon Y. W. Ho, Stéphane Guindon","In phylogenetic analyses of molecular sequence data, partitioning involves estimating independent models of molecular evolution for different sets of sites in a sequence alignment. Choosing an appropriate partitioning scheme is an important step in most analyses because it can affect the accuracy of phylogenetic reconstruction. Despite this, partitioning schemes are often chosen without explicit statistical justification. Here, we describe two new objective methods for the combined selection of best-fit partitioning schemes and nucleotide substitution models. These methods allow millions of partitioning schemes to be compared in realistic time frames and so permit the objective selection of partitioning schemes even for large multilocus DNA data sets. We demonstrate that these methods significantly outperform previous approaches, including both the ad hoc selection of partitioning schemes (e.g., partitioning by gene or codon position) and a recently proposed hierarchical clustering method. We have implemented these methods in an open-source program, PartitionFinder. This program allows users to select partitioning schemes and substitution models using a range of information-theoretic metrics (e.g., the Bayesian information criterion, akaike information criterion [AIC], and corrected AIC). We hope that PartitionFinder will encourage the objective selection of partitioning schemes and thus lead to improvements in phylogenetic analyses. PartitionFinder is written in Python and runs under Mac OSX 10.4 and above. The program, source code, and a detailed manual are freely available from www.robertlanfear.com/partitionfinder.",Genomics and Phylogenetic Studies,Molecular Biology,3664.0
7445,W2117801354,phangorn: phylogenetic analysis in R,Klaus Schliep,"Abstract Summary: phangorn is a package for phylogenetic reconstruction and analysis in the R language. Previously it was only possible to estimate phylogenetic trees with distance methods in R. phangorn, now offers the possibility of reconstructing phylogenies with distance based methods, maximum parsimony or maximum likelihood (ML) and performing Hadamard conjugation. Extending the general ML framework, this package provides the possibility of estimating mixture and partition models. Furthermore, phangorn offers several functions for comparing trees, phylogenetic models or splits, simulating character data and performing congruence analyses. Availability: phangorn can be obtained through the CRAN homepage http://cran.r-project.org/web/packages/phangorn/index.html. phangorn is licensed under GPL 2. Contact: klaus.kschliep@snv.jussieu.fr Supplementary information: Supplementary data are available at Bioinformatics online.",Genomics and Phylogenetic Studies,Molecular Biology,3665.0
7446,W2969866109,ModelTest-NG: A New and Scalable Tool for the Selection of DNA and Protein Evolutionary Models,"Diego Darriba, David Posada, Alexey M. Kozlov, Alexandros Stamatakis, Benoît Morel, Tomáš Flouri","ModelTest-NG is a reimplementation from scratch of jModelTest and ProtTest, two popular tools for selecting the best-fit nucleotide and amino acid substitution models, respectively. ModelTest-NG is one to two orders of magnitude faster than jModelTest and ProtTest but equally accurate and introduces several new features, such as ascertainment bias correction, mixture, and free-rate models, or the automatic processing of single partitions. ModelTest-NG is available under a GNU GPL3 license at https://github.com/ddarriba/modeltest , last accessed September 2, 2019.",Genomics and Phylogenetic Studies,Molecular Biology,3666.0
7447,W2036839190,The Colletotrichum gloeosporioides species complex,"Bevan Weir, Peter R. Johnston, Ulrike Damm","The limit of the Colletotrichum gloeosporioides species complex is defined genetically, based on a strongly supported clade within the Colletotrichum ITS gene tree. All taxa accepted within this clade are morphologically more or less typical of the broadly defined C. gloeosporioides, as it has been applied in the literature for the past 50 years. We accept 22 species plus one subspecies within the C. gloeosporioides complex. These include C. asianum, C. cordylinicola, C. fructicola, C. gloeosporioides, C. horii, C. kahawae subsp. kahawae, C. musae, C. nupharicola, C. psidii, C. siamense, C. theobromicola, C. tropicale, and C. xanthorrhoeae, along with the taxa described here as new, C. aenigma, C. aeschynomenes, C. alatae, C. alienum, C. aotearoa, C. clidemiae, C. kahawae subsp. ciggaro, C. salsolae, and C. ti, plus the nom. nov. C. queenslandicum (for C. gloeosporioides var. minus). All of the taxa are defined genetically on the basis of multi-gene phylogenies. Brief morphological descriptions are provided for species where no modern description is available. Many of the species are unable to be reliably distinguished using ITS, the official barcoding gene for fungi. Particularly problematic are a set of species genetically close to C. musae and another set of species genetically close to C. kahawae, referred to here as the Musae clade and the Kahawae clade, respectively. Each clade contains several species that are phylogenetically well supported in multi-gene analyses, but within the clades branch lengths are short because of the small number of phylogenetically informative characters, and in a few cases individual gene trees are incongruent. Some single genes or combinations of genes, such as glyceraldehyde-3-phosphate dehydrogenase and glutamine synthetase, can be used to reliably distinguish most taxa and will need to be developed as secondary barcodes for species level identification, which is important because many of these fungi are of biosecurity significance. In addition to the accepted species, notes are provided for names where a possible close relationship with C. gloeosporioides sensu lato has been suggested in the recent literature, along with all subspecific taxa and formae speciales within C. gloeosporioides and its putative teleomorph Glomerella cingulata.",Plant Pathogens and Fungal Diseases,Cell Biology,3667.0
7448,W2161787041,The online database Maarj<i>AM</i> reveals global and ecosystemic distribution patterns in arbuscular mycorrhizal fungi (Glomeromycota),"Maarja Öpik, Alo Vanatoa, Elise Vanatoa, Mari Moora, John Davison, Jesse M. Kalwij, Ülle Reier, Martin Zobel","• Here, we describe a new database, MaarjAM, that summarizes publicly available Glomeromycota DNA sequence data and associated metadata. The goal of the database is to facilitate the description of distribution and richness patterns in this group of fungi. • Small subunit (SSU) rRNA gene sequences and available metadata were collated from all suitable taxonomic and ecological publications. These data have been made accessible in an open-access database (http://maarjam.botany.ut.ee). • Two hundred and eighty-two SSU rRNA gene virtual taxa (VT) were described based on a comprehensive phylogenetic analysis of all collated Glomeromycota sequences. Two-thirds of VT showed limited distribution ranges, occurring in single current or historic continents or climatic zones. Those VT that associated with a taxonomically wide range of host plants also tended to have a wide geographical distribution, and vice versa. No relationships were detected between VT richness and latitude, elevation or vascular plant richness. • The collated Glomeromycota molecular diversity data suggest limited distribution ranges in most Glomeromycota taxa and a positive relationship between the width of a taxon's geographical range and its host taxonomic range. Inconsistencies between molecular and traditional taxonomy of Glomeromycota, and shortage of data from major continents and ecosystems, are highlighted.",Mycorrhizal Fungi and Plant Interactions,Plant Science,3668.0
7449,W2588207325,Fungal Identification Using Molecular Tools: A Primer for the Natural Products Research Community,"Huzefa A. Raja, Andrew N. Miller, Cedric J. Pearce, Nicholas H. Oberlies","Fungi are morphologically, ecologically, metabolically, and phylogenetically diverse. They are known to produce numerous bioactive molecules, which makes them very useful for natural products researchers in their pursuit of discovering new chemical diversity with agricultural, industrial, and pharmaceutical applications. Despite their importance in natural products chemistry, identification of fungi remains a daunting task for chemists, especially those who do not work with a trained mycologist. The purpose of this review is to update natural products researchers about the tools available for molecular identification of fungi. In particular, we discuss (1) problems of using morphology alone in the identification of fungi to the species level; (2) the three nuclear ribosomal genes most commonly used in fungal identification and the potential advantages and limitations of the ITS region, which is the official DNA barcoding marker for species-level identification of fungi; (3) how to use NCBI-BLAST search for DNA barcoding, with a cautionary note regarding its limitations; (4) the numerous curated molecular databases containing fungal sequences; (5) the various protein-coding genes used to augment or supplant ITS in species-level identification of certain fungal groups; and (6) methods used in the construction of phylogenetic trees from DNA sequences to facilitate fungal species identification. We recommend that, whenever possible, both morphology and molecular data be used for fungal identification. Our goal is that this review will provide a set of standardized procedures for the molecular identification of fungi that can be utilized by the natural products research community.",Plant Pathogens and Fungal Diseases,Cell Biology,3669.0
7450,W2951214599,bModelTest: Bayesian phylogenetic site model averaging and model comparison,"Remco Bouckaert, Alexei J. Drummond","Reconstructing phylogenies through Bayesian methods has many benefits, which include providing a mathematically sound framework, providing realistic estimates of uncertainty and being able to incorporate different sources of information based on formal principles. Bayesian phylogenetic analyses are popular for interpreting nucleotide sequence data, however for such studies one needs to specify a site model and associated substitution model. Often, the parameters of the site model is of no interest and an ad-hoc or additional likelihood based analysis is used to select a single site model. bModelTest allows for a Bayesian approach to inferring and marginalizing site models in a phylogenetic analysis. It is based on trans-dimensional Markov chain Monte Carlo (MCMC) proposals that allow switching between substitution models as well as estimating the posterior probability for gamma-distributed rate heterogeneity, a proportion of invariable sites and unequal base frequencies. The model can be used with the full set of time-reversible models on nucleotides, but we also introduce and demonstrate the use of two subsets of time-reversible substitution models. With the new method the site model can be inferred (and marginalized) during the MCMC analysis and does not need to be pre-determined, as is now often the case in practice, by likelihood-based methods. The method is implemented in the bModelTest package of the popular BEAST 2 software, which is open source, licensed under the GNU Lesser General Public License and allows joint site model and tree inference under a wide range of models.",Evolution and Paleontology Studies,Paleontology,3670.0
7451,W1969423031,Subset Selection in Regression,Alan J. Miller,"OBJECTIVES Prediction, Explanation, Elimination or What? How Many Variables in the Prediction Formula? Alternatives to Using Subsets 'Black Box' Use of Best-Subsets Techniques LEAST-SQUARES COMPUTATIONS Using Sums of Squares and Products Matrices Orthogonal Reduction Methods Gauss-Jordan v. Orthogonal Reduction Methods Interpretation of Projections Appendix A: Operation Counts for All-Subsets Regression FINDING SUBSETS WHICH FIT WELL Objectives and Limitations of this Chapter Forward Selection Efroymson's Algorithm Backward Elimination Sequential Replacement Algorithm Replacing Two Variables at a Time Generating All Subsets Using Branch-and-Bound Techniques Grouping Variables Ridge Regression and Other Alternatives The Non-Negative Garrote and the Lasso Some Examples Conclusions and Recommendations HYPOTHESIS TESTING Is There any Information in the Remaining Variables? Is One Subset Better than Another? Appendix A: Spjftvoll's Method - Detailed Description WHEN TO STOP? What Criterion Should We Use? Prediction Criteria Cross-Validation and the PRESS Statistic Bootstrapping Likelihood and Information-Based Stopping Rules Appendix A. Approximate Equivalence of Stopping Rules ESTIMATION OF REGRESSION COEFFICIENTS Selection Bias Choice Between Two Variables Selection Bias in the General Case, and its Reduction Conditional Likelihood Estimation Estimation of Population Means Estimating Least-Squares Projections Appendix A: Changing Projections to Equate Sums of Squares BAYESIAN METHODS Bayesian Introduction 'Spike and Slab' Prior Normal prior for Regression Coefficients Model Averaging Picking the Best Model CONCLUSIONS AND SOME RECOMMENDATIONS REFERENCES INDEX",Advanced Data Processing Techniques,Control and Systems Engineering,3724.0
7452,W1489838709,Experimental Design and Data Analysis for Biologists,"Gerry P. Quinn, Michael J. Keough","An essential textbook for any student or researcher in biology needing to design experiments, sample programs or analyse the resulting data. The text begins with a revision of estimation and hypothesis testing methods, covering both classical and Bayesian philosophies, before advancing to the analysis of linear and generalized linear models. Topics covered include linear and logistic regression, simple and complex ANOVA models (for factorial, nested, block, split-plot and repeated measures and covariance designs), and log-linear models. Multivariate techniques, including classification and ordination, are then introduced. Special emphasis is placed on checking assumptions, exploratory data analysis and presentation of results. The main analyses are illustrated with many examples from published papers and there is an extensive reference list to both the statistical and biological literature. The book is supported by a website that provides all data sets, questions for each chapter and links to software.","Genetics, Bioinformatics, and Biomedical Research",Molecular Biology,4003.0
7453,W1492075988,Applied Longitudinal Analysis,"Garrett M. Fitzmaurice, Nan M. Laird, James H. Ware",Preface.Acknowledgments.PART I: INTRODUCTION TO LONGITUDINAL AND CLUSTERED DATA.1. Longitudinal and Clustered Data.2. Longitudinal Data: Basic Concepts.PART II: LINEAR MODELS FOR LONGITUDINAL CONTINUOUS DATA.3. Overview of Linear Models for Longitudinal Data.4. Estimation and Statistical Inference.5. Modelling the Mean: Analyzing Response Profiles.6. Modelling the Mean: Parametric Curves.7. Modelling the Covariance.8. Linear Mixed Effects Models.9. Residual Analyses and Diagnostics.PART III: GENERALIZED LINEAR MODELS FOR LONGITUDINAL DATA.10. Review of Generalized Linear Models.11. Marginal Models: Generalized Estimating Equations (GEE).12. Generalized Linear Mixed Effects Models.13. Contrasting Marginal and Mixed Effects Models.PART IV: ADVANCED TOPICS FOR LONGITUDINAL AND CLUSTERED DATA.14. Missing Data and Dropout.15. Some Aspects of the Design of Longitudinal Studies.16. Repeated Measures and Related Designs.17. Multilevel Models.Appendix A: Gentle Introduction to Vectors and Matrices.Appendix B: Properties of Expectations and Variances.Appendix C: Critical Points for a 50:50 Mixture of Chi-Squared Distributions.References.Index.,Advanced Statistical Methods and Models,Statistics and Probability,4004.0
7454,W1512882103,Measurement error in nonlinear models: a modern perspective,Raymond J. Carroll,Guide to Notation Introduction The Double/Triple-Whammy of Measurement Error Classical Measurement Error A Nutrition Example Measurement Error Examples Radiation Epidemiology and Berkson Errors Classical Measurement Error Model Extensions Other Examples of Measurement Error Models Checking The Classical Error Model Loss of Power A Brief Tour Bibliographic Notes Important Concepts Functional and Structural Models Models for Measurement Error Sources of Data Is There an Exact Predictor? What is Truth? Differential and Nondifferential Error Prediction Bibliographic Notes Linear Regression and Attenuation Introduction Bias Caused by Measurement Error Multiple and Orthogonal Regression Correcting for Bias Bias Versus Variance Attenuation in General Problems Bibliographic Notes Regression Calibration Overview The Regression Calibration Algorithm NHANES Example Estimating the Calibration Function Parameters Multiplicative Measurement Error Standard Errors Expanded Regression Calibration Models Examples of the Approximations Theoretical Examples Bibliographic Notes and Software Simulation Extrapolation Overview Simulation Extrapolation Heuristics The SIMEX Algorithm Applications SIMEX in Some Important Special Cases Extensions and Related Methods Bibliographic Notes Instrumental Variables Overview Instrumental Variables in Linear Models Approximate Instrumental Variable Estimation Adjusted Score Method Examples Other Methodologies Bibliographic Notes Score Function Methods Overview Linear and Logistic Regression Conditional Score Functions Corrected Score Functions Computation and Asymptotic Approximations Comparison of Conditional and Corrected Scores Bibliographic Notes Likelihood and Quasilikelihood Introduction Steps 2 and 3: Constructing Likelihoods Step 4: Numerical Computation of Likelihoods Cervical Cancer and Herpes Framingham Data Nevada Test Site Reanalysis Bronchitis Example Quasilikelihood and Variance Function Models Bibliographic Notes Bayesian Methods Overview The Gibbs Sampler Metropolis-Hastings Algorithm Linear Regression Nonlinear Models Logistic Regression Berkson Errors Automatic implementation Cervical Cancer and Herpes Framingham Data OPEN Data: A Variance Components Model Bibliographic Notes Hypothesis Testing Overview The Regression Calibration Approximation Illustration: OPEN Data Hypotheses about Sub-Vectors of ssx and ssz Efficient Score Tests of H0 : ssx = 0 Bibliographic Notes Longitudinal Data and Mixed Models Mixed Models for Longitudinal Data Mixed Measurement Error Models A Bias Corrected Estimator SIMEX for GLMMEMs Regression Calibration for GLMMs Maximum Likelihood Estimation Joint Modeling Other Models and Applications Example: The CHOICE Study Bibliographic Notes Nonparametric Estimation Deconvolution Nonparametric Regression Baseline Change Example Bibliographic Notes Semiparametric Regression Overview Additive Models MCMC for Additive Spline Models Monte-Carlo EM-Algorithm Simulation with Classical Errors Simulation with Berkson Errors Semiparametrics: X Modeled Parametrically Parametric Models: No Assumptions on X Bibliographic Notes Survival Data Notation and Assumptions Induced Hazard Function Regression Calibration for Survival Analysis SIMEX for Survival Analysis Chronic Kidney Disease Progression Semi and Nonparametric Methods Likelihood Inference for Frailty Models Bibliographic Notes Response Variable Error Response Error and Linear Regression Other Forms of Additive Response Error Logistic Regression with Response Error Likelihood Methods Use of Complete Data Only Semiparametric Methods for Validation Data Bibliographic Notes Appendix A: Background Material Overview Normal and Lognormal Distributions Gamma and Inverse Gamma Distributions Best and Best Linear Prediction and Regression Likelihood Methods Unbiased Estimating Equations Quasilikelihood and Variance Function Models (QVF) Generalized Linear Models Bootstrap Methods Appendix B: Technical Details Appendix to Chapter 1: Power in Berkson and Classical Error Models Appendix to Chapter 3: Linear Regression and Attenuation Regression Calibration SIMEX Instrumental Variables Score Function Methods Likelihood and Quasilikelihood Bayesian Methods References Applications and Examples Index Index,Scientific Measurement and Uncertainty Evaluation,"Statistics, Probability and Uncertainty",4005.0
7456,W1561032708,A Beginner's Guide to R,"Alain F. Zuur, Elena N. Ieno, Erik H. Meesters","Based on their extensive experience with teaching R and statistics to applied scientists, the authors provide a beginner's guide to R. To avoid the difficulty of teaching R and statistics at the same time, statistical methods are kept to a minimum. The text covers how to download and install R, import and manage data, elementary plotting, an introduction to functions, advanced plotting, and common beginner mistakes. This book contains everything you need to know to get started with R. Its biggest advantage is that it aims only to teach R...It organizes R commands very efficiently, with much teaching guidance included. I would describe this book as being handy--its the kind of book that you want to keep in your jacket pocket or backpack all the time, ready for use, like a Swiss Army knife. (Loveday Conquest, University of Washington) Whilst several books focus on learning statistics in R..., the authors of this book fill a gap in the market by focusing on learning R whilst almost completely avoiding any statistical jargon...The fact that the authors have very extensive experience of teaching R to absolute beginners shines throughout. (Mark Mainwaring, Lancaster University) Exactly what is needed...This is great, nice work. I love the ecological/biological examples; they will be an enormous help. (Andrew J. Tyne, University of Nebraska-Lincoln)",Data Analysis with R,Artificial Intelligence,4009.0
7457,W1611898144,Data Analysis and Regression: A Second Course in Statistics,"Frederick Mosteller, John W. Tukey",The assumption is made in this volume devoted to data analysis and regression that the student has had a 1st course in statistics. Attitudes and approaches are more important than the techniques this book can teach. Readers can learn to identify at least the following attitudes understanding and approaches: an approach to the formulation of statistical and data analytical problems such that for example the students shortcut to inference can be properly understood and the role of vague concepts becomes clear; the role of indications (of pointers to behavior not necessarily on prechosen scales) in contrast to conclusions or decisions about prechosen quantities or alternatives; the importance of displays and the value of graphs in forcing the unexpected upon the reader; the importance of re-expression; the need to seek out the real uncertainty as a nontrivial task; the importance of iterated calculation; how the ideas of robustness and resistance can change both what one does and what one thinks; what regression is all about; what regression coefficient can and cannot do; that the behavior of ones data can often be used to guide its analysis; the importance of looking at and drawing information from residuals; and the idea that data analysis can profit from repeated starts and fresh approaches and that there is not just a single analysis for a substantial problem. The 16 chapters of this book include the following: some practical philosophy for data analysis; a background for simple linear regression; the nature and importance of re-expression; a method of direct assessment; the direct and flexible approach to 2 way tables; a review of resistant/robust techniques in the simpler applications; standardization; regression and regression coefficients; a mathematical approach to understanding regression; guided regression and examining regression residuals. Among the special features of this volume are the following: an introduction to stem and leaf displays; use of running medians for smoothing; the ladder of re-expression for straightening curves; methods of re-expression for analysis; special tables to make re-expression easy in hand calculations; robust and resistant measures of location and scale; and regression with errors of measurement.,Sensor Technology and Measurement Systems,Computer Networks and Communications,4010.0
7458,W1668720348,Contemporary statistical models for the plant and soil sciences,"Oliver Schabenberger, Francis J. Pierce","Statistical Models Mathematical and Statistical Models Functional Aspects of Models The Inferential Steps o Estimation and Testing t-Tests in Terms of Statistical Models Embedding Hypotheses Hypothesis and Significance Testing o Interpretation of the p-Value Classes of Statistical Models Data Structures Introduction Classification by Response Type Classification by Study Type Clustered Data Autocorrelated Data From Independent to Spatial Data o A Progression of Clustering Linear Algebra Tools Introduction Matrices and Vectors Basic Matrix Operations Matrix Inversion o Regular and Generalized Inverse Mean, Variance, and Covariance of Random Vectors The Trace and Expectation of Quadratic Forms The Multivariate Gaussian Distribution Matrix and Vector Differentiation Using Matrix Algebra to Specify Models The Classical Linear Model: Least Squares and Alternatives Introduction Least Squares Estimation and Partitioning of Variation Factorial Classification Diagnosing Regression Models Diagnosing Classification Models Robust Estimation Nonparametric Regression Nonlinear Models Introduction Models as Laws or Tools Linear Polynomials Approximate Nonlinear Models Fitting a Nonlinear Model to Data Hypothesis Tests and Confidence Intervals Transformations Parameterization of Nonlinear Models Applications Generalized Linear Models Introduction Components of a Generalized Linear Model Grouped and Ungrouped Data Parameter Estimation and Inference Modeling an Ordinal Response Overdispersion Applications Linear Mixed Models for Clustered Data Introduction The Laird-Ware Model Choosing the Inference Space Estimation and Inference Correlations in Mixed Models Applications Nonlinear Models for Clustered Data Introduction Nonlinear and Generalized Linear Mixed Models Towards an Approximate Objective Function Applications Statistical Models for Spatial Data Changing the Mindset Semivariogram Analysis and Estimation The Spatial Model Spatial Prediction and the Kriging Paradigm Spatial Regression and Classification Models Autoregressive Models for Lattice Data Analyzing Mapped Spatial Point Patterns Applications Bibliography",Leaf Properties and Growth Measurement,Plant Science,4011.0
7459,W1973628995,Regression Analysis of Count Data,"A. Colin Cameron, Pravin K. Trivedi","Students in both social and natural sciences often seek regression methods to explain the frequency of events, such as visits to a doctor, auto accidents, or new patents awarded. This book, now in its second edition, provides the most comprehensive and up-to-date account of models and methods to interpret such data. The authors combine theory and practice to make sophisticated methods of analysis accessible to researchers and practitioners working with widely different types of data and software in areas such as applied statistics, econometrics, marketing, operations research, actuarial studies, demography, biostatistics and quantitative social sciences. The new material includes new theoretical topics, an updated and expanded treatment of cross-section models, coverage of bootstrap-based and simulation-based inference, expanded treatment of time series, multivariate and panel data, expanded treatment of endogenous regressors, coverage of quantile count regression, and a new chapter on Bayesian methods.",Forecasting Techniques and Applications,Management Science and Operations Research,4012.0
7460,W1987515496,Correcting for spatial autocorrelation in sequential sampling,"Andrew P. Robinson, Jeff D. Hamann","1 Sequential sampling is attractive because it permits the user to choose, and efficiently achieve, desired confidence interval lengths. Sequential sampling has been broadly applied in the inventory of ecological resources. 2 Using case studies and simulations, we demonstrate that estimates of the population mean that are derived from sequential sampling can be overconfident if the population is autocorrelated, that is, that the confidence intervals are too short. We test a model-based correction designed to ameliorate this effect of autocorrelation upon the estimates of confidence intervals from sequential sampling. 3 The correction is useful in realistic situations. Among the scenarios we tested, better confidence interval coverage was achieved with larger sample sizes, and coverage rates were poor at smaller sample sizes. Nominal coverage could be attained even when the wrong model was used, although only at the cost of requiring a much higher average sample size. 4 Synthesis and applications. If sequential sampling is naively applied in a population that has autocorrelation, then confidence intervals for population parameters will be too short, and the usefulness of the sample will be overestimated. We recommend using a correction to lengthen the estimated confidence intervals. Our results suggest that this correction requires a substantial sample size, up to several hundred units, in order to provide nominal coverage. Sequential sampling seems risky in autocorrelated populations if the realized sample size is small.",Statistical Methods and Bayesian Inference,Statistics and Probability,4015.0
7461,W1994040168,HOW WELL CAN WE MODEL NUMBERS AND PRODUCTIVITY OF SALTMARSH SHARP-TAILED SPARROWS (<i>AMMODRAMUS CAUDACUTUS</i>) USING HABITAT FEATURES?,"Carina Gjerdrum, Chris S. Elphick, Margaret A. Rubega","Habitat models are often used to describe species distributions, but they need to be tested to evaluate their predictive performance. We investigated the importance of model evaluation in a study of habitat selection by Saltmarsh Sharp-tailed Sparrows (Ammodramus caudacutus), a species of conservation concern in eastern North America. We estimated the number of birds, nests, and fledglings produced in thirty 1-ha study plots spread across multiple marshes and used an information-theoretic approach to select among explanatory habitat models. Model performance was evaluated using both the original data set and data from another 30 plots from the same set of marshes. Our models indicate that both female and male Saltmarsh Sharp-tailed Sparrows increase in abundance as one moves away from the marsh's upland edge and in areas where there is deep thatch and uniform vegetation height. In contrast to the results for adult birds, number of nests and production of fledglings were positively associated with Saltmeadow Rush (Juncus gerardii). These models suggest that where adults spend the most time is influenced by the vertical structure of the vegetation but that nesting activity is determined more by plant compositions that indicate subtle variations in marsh elevation. Despite the fact that we found good internal consistency for our models during model evaluation, model performance worsened considerably when used to make predictions about new sites. Thus, although our study identifies several new factors influencing habitat selection in Saltmarsh Sharp-tailed Sparrows, it also highlights the need to be cautious when making predictions from habitat models.",Species Distribution and Climate Change,Ecological Modeling,4016.0
7462,W2020301958,Generalized estimating equations and generalized linear mixed‐effects models for modelling resource selection,"Nicola Koper, Micheline Manseau","Summary Accurate resource selection functions (RSFs) are important for managing animal populations. Developing RSFs using data from GPS telemetry can be problematic due to serial autocorrelation, but modern analytical techniques can help to compensate for this correlation. We used telemetry locations from 18 woodland caribou Rangifer tarandus caribou in Saskatchewan, Canada, to compare marginal (population‐specific) generalized estimating equations (GEEs), and conditional (subject‐specific) generalized linear mixed‐effects models (GLMMs), for developing resource selection functions at two spatial scales. We evaluated the use of empirical standard errors, which are robust to misspecification of the correlation structure. We compared these approaches with destructive sampling. Statistical significance was strongly influenced by the use of empirical vs. model‐based standard errors, and marginal (GEE) and conditional (GLMM) results differed. Destructive sampling reduced apparent habitat selection. k ‐fold cross‐validation results differed for GEE and GLMM, as it must be applied differently for each model. Synthesis and applications . Due to their different interpretations, marginal models (e.g. generalized estimating equations, GEEs) may be better for landscape and population management, while conditional models (e.g. generalized linear mixed‐effects models, GLMMs) may be better for management of endangered species and individuals. Destructive sampling may lead to inaccurate resource selection functions (RSFs), but GEEs and GLMMs can be used for developing RSFs when used with empirical standard errors.",Wildlife Ecology and Conservation,Ecology,4017.0
7463,W2021811323,"APPLIED MIXED MODELS IN MEDICINE, 2ND ED.",Sharon M. Homan,"APPLIED MIXED MODELS IN MEDICINE, 2ND ED. Authors: Helen Brown; Robin Prescott Bibliographic Data: John Wiley & Sons, Inc., 2006. ISBN: 978-0-470-02356-3, Series Title: Statistics in Practice, 473 pages, hard cover, $130.00. Reviewer's Expert Opinion Description: Mixed models, also known as multilevel models in the social sciences, allow both fixed and random variables within a statistical analysis. The general linear model assumption that the error terms are independent and identically distributed is relaxed in mixed models so that observations can be correlated (e.g., repeated measures, cross-over trial, etc.). This second edition describes current methods and advanced SAS techniques for applying mixed models. The first edition was published in 1999. Purpose: The authors present the theory and application of mixed models in medical research, including the latest developments in bioequivalence, cross-over trials, and cluster randomized trials. Their purpose is to make mixed modeling easily accessible to practitioners such as medical statisticians and economists. The book is well written, thorough, and highly applicable. The examples, complete with SAS code and output, are outstanding. The authors meet their objectives. Audience: This is written for applied statisticians working in medical research and the pharmaceutical industry, as well as teachers and students of statistic courses in mixed models. The authors are experienced statisticians and highly credible scholars from the U.K. The book is part of the Statistics in Practice international series of books that provide statistical support for professionals and researchers. Features: Beginning by describing the capabilities of mixed models, the authors introduce readers to the general linear model for fitting normally distributed data and then extend the general linear model to general linear mixed models. The authors then examine how mixed models can be applied with categorical outcome variables. Chapters 5 to 7 are devoted to practical application of mixed models using particular designs. Chapter 8 includes a new section on bioequivalence studies and cluster randomized trials. Chapter 9 concludes by describing software options, including SAS and the PROC GLIMIX and PROC GENMOD procedures. The reference pages on mixed model notation, the glossary of terms, and the detailed SAS programming code and annotation greatly enhance the book's usefulness. Assessment: This is an excellent resource for biostatisticians and medical researchers. It provides the reader with a thorough understanding of the concepts of mixed models. There are many social science texts on mixed modeling (also called multilevel modeling), but few that clearly link mixed models to clinical research designs. The second edition uses the 9th version of SAS and expands the coverage of categorical outcomes. Reviewer: Sharon M. Homan (St. Louis University School of Public Health, St. Louis, MO)",Machine Learning in Healthcare,Artificial Intelligence,4018.0
7464,W1590050327,mvabund– an R package for model‐based analysis of multivariate abundance data,"Yi Wang, Ulrike Naumann, Stephen Wright, David I. Warton","Summary 1. The mvabund package for R provides tools for model‐based analysis of multivariate abundance data in ecology. 2. This includes methods for visualising data, fitting predictive models, checking model assumptions, as well as testing hypotheses about the community–environment association. 3. This paper briefly introduces the package and demonstrates its functionality by example.",Ecology and Vegetation Dynamics Studies,Nature and Landscape Conservation,4019.0
7465,W1644145788,Do not log‐transform count data,"Robert B. O’Hara, D. Johan Kotze","Summary 1. Ecological count data (e.g. number of individuals or species) are often log‐transformed to satisfy parametric test assumptions. 2. Apart from the fact that generalized linear models are better suited in dealing with count data, a log‐transformation of counts has the additional quandary in how to deal with zero observations. With just one zero observation (if this observation represents a sampling unit), the whole data set needs to be fudged by adding a value (usually 1) before transformation. 3. Simulating data from a negative binomial distribution, we compared the outcome of fitting models that were transformed in various ways (log, square root) with results from fitting models using quasi‐Poisson and negative binomial models to untransformed count data. 4. We found that the transformations performed poorly, except when the dispersion was small and the mean counts were large. The quasi‐Poisson and negative binomial models consistently performed well, with little bias. 5. We recommend that count data should not be analysed by log‐transforming it, but instead models based on Poisson and negative binomial distributions should be used.",Statistical Methods and Bayesian Inference,Statistics and Probability,4020.0
7466,W2784805535,Moving in the Anthropocene: Global reductions in terrestrial mammalian movements,"Marlee A. Tucker, Katrin Böhning‐Gaese, William F. Fagan, John M. Fryxell, Bram Van Moorter, Susan C. Alberts, Abdullahi H. Ali, Andrew M. Allen, Nina Attias, Tal Avgar, Hattie L. A. Bartlam‐Brooks, Bayarbaatar Buuveibaatar, Jerrold L. Belant, Alessandra Bertassoni, Dean E. Beyer, Laura R. Bidner, Floris M. van Beest, Stephen Blake, Niels Blaum, Chloe Bracis, Danielle D. Brown, P J Nico de Bruyn, Francesca Cagnacci, Justin M. Calabrese, Constança Camilo-Alves, Simon Chamaillé‐Jammes, André Chiaradia, Sarah C. Davidson, Todd E. Dennis, Stephen DeStefano, Duane R. Diefenbach, Iain Douglas‐Hamilton, Julian Fennessy, Claudia Fichtel, Wolfgang Fiedler, Christina Fischer, Ilya R. Fischhoff, Christen H. Fleming, Adam T. Ford, Susanne A. Fritz, Benedikt Gehr, Jacob R. Goheen, Eliezer Gurarie, Mark Hebblewhite, Marco Heurich, A. J. Mark Hewison, Christian Hof, Edward Hurme, Lynne A. Isbell, René Janssen, Florian Jeltsch, Petra Kaczensky, Adam Kane, Peter M. Kappeler, Matthew J. Kauffman, Roland Kays, Duncan M. Kimuyu, Flávia Koch, Bart Kranstauber, Scott LaPoint, Peter Leimgruber, John D. C. Linnell, Pascual López‐López, A. Catherine Markham, Jenny Mattisson, Emília Patrícia Medici, Ugo Mellone, Evelyn H. Merrill, Guilherme Mourão, Ronaldo Gonçalves Morato, Nicolas Morellet, Thomas A. Morrison, Samuel L. Díaz‐Muñoz, Atle Mysterud, Nandintsetseg Dejid, Ran Nathan, Aidin Niamir, John Oddén, Robert B. O’Hara, Luiz Gustavo Rodrigues Oliveira‐Santos, Kirk A. Olson, Bruce D. Patterson, Rogério Cunha de Paula, Luca Pedrotti, Björn Reineking, Martin Rimmler, Tracey L. Rogers, Christer M. Rolandsen, Christopher S. Rosenberry, Daniel I. Rubenstein, Kamran Safi, Sonia Saı̈d, Nir Sapir, Hall Sawyer, Niels Martin Schmidt, Nuria Selva, Agnieszka Sergiel, Enkhtuvshin Shiilegdamba, João Paulo Silva, Navinder J. Singh, Erling J. Solberg, Orr Spiegel, Olav Strand, Siva R. Sundaresan, Wiebke Ullmann, Ulrich Voigt, Jake Wall, David W. Wattles, Martin Wikelski, Christopher C. Wilmers, John W. Wilson, George Wittemyer, Filip Zięba, Tomasz Zwijacz‐Kozica, Thomas Mueller","Animal movement is fundamental for ecosystem functioning and species survival, yet the effects of the anthropogenic footprint on animal movements have not been estimated across species. Using a unique GPS-tracking database of 803 individuals across 57 species, we found that movements of mammals in areas with a comparatively high human footprint were on average one-half to one-third the extent of their movements in areas with a low human footprint. We attribute this reduction to behavioral changes of individual animals and to the exclusion of species with long-range movements from areas with higher human impact. Global loss of vagility alters a key ecological trait of animals that affects not only population persistence but also ecosystem processes such as predator-prey interactions, nutrient cycling, and disease transmission.",Wildlife Ecology and Conservation,Ecology,4021.0
7467,W3035663944,Robustness of linear mixed‐effects models to violations of distributional assumptions,"Holger Schielzeth, Niels J. Dingemanse, Shinichi Nakagawa, David F. Westneat, Hassen Allegue, Céline Teplitsky, Denis Réale, Ned A. Dochtermann, László Zsolt Garamszegi, Yimen G. Araya‐Ajoy","Abstract Linear mixed‐effects models are powerful tools for analysing complex datasets with repeated or clustered observations, a common data structure in ecology and evolution. Mixed‐effects models involve complex fitting procedures and make several assumptions, in particular about the distribution of residual and random effects. Violations of these assumptions are common in real datasets, yet it is not always clear how much these violations matter to accurate and unbiased estimation. Here we address the consequences of violations in distributional assumptions and the impact of missing random effect components on model estimates. In particular, we evaluate the effects of skewed, bimodal and heteroscedastic random effect and residual variances, of missing random effect terms and of correlated fixed effect predictors. We focus on bias and prediction error on estimates of fixed and random effects. Model estimates were usually robust to violations of assumptions, with the exception of slight upward biases in estimates of random effect variance if the generating distribution was bimodal but was modelled by Gaussian error distributions. Further, estimates for (random effect) components that violated distributional assumptions became less precise but remained unbiased. However, this particular problem did not affect other parameters of the model. The same pattern was found for strongly correlated fixed effects, which led to imprecise, but unbiased estimates, with uncertainty estimates reflecting imprecision. Unmodelled sources of random effect variance had predictable effects on variance component estimates. The pattern is best viewed as a cascade of hierarchical grouping factors. Variances trickle down the hierarchy such that missing higher‐level random effect variances pool at lower levels and missing lower‐level and crossed random effect variances manifest as residual variance. Overall, our results show remarkable robustness of mixed‐effects models that should allow researchers to use mixed‐effects models even if the distributional assumptions are objectively violated. However, this does not free researchers from careful evaluation of the model. Estimates that are based on data that show clear violations of key assumptions should be treated with caution because individual datasets might give highly imprecise estimates, even if they will be unbiased on average across datasets.",Ecology and Vegetation Dynamics Studies,Nature and Landscape Conservation,4023.0
7468,W2187279102,Non-bee insects are important contributors to global crop pollination,"Romina Rader, Ígnasi Bartomeus, Lucas A. Garibaldi, Michael P. D. Garratt, Brad G. Howlett, Rachael Winfree, Saul A. Cunningham, Margaret M. Mayfield, Anthony D. Arthur, Georg K.S. Andersson, Riccardo Bommarco, Claire Brittain, Luísa G. Carvalheiro, Natacha P. Chacoff, Martin H. Entling, Benjamin Foully, Breno Magalhães Freitas, Barbara Gemmill‐Herren, Jaboury Ghazoul, Sean R. Griffin, C. L. Gross, Lina Herbertsson, Félix Herzog, Juliana Hipólito, Sue Jaggar, Frank Jauker, Alexandra‐Maria Klein, David Kleijn, Smitha Krishnan, Camila Queiroz Lemos, Sandra Lindström, Yael Mandelik, Victor Magalhães Monteiro, Warrick Nelson, L. Anders Nilsson, David E. Pattemore, Natália de Oliveira Pereira, Gideon Pisanty, Simon G. Potts, Menno Reemer, Maj Rundlöf, Cory S. Sheffield, Jeroen Scheper, Christof Schüepp, Henrik G. Smith, Dara A. Stanley, Jane C. Stout, Hajnalka Szentgyörgyi, Hisatomo Taki, Carlos Vergara, Blandina Felipe Viana, Michał Woyciechowski","Significance Many of the world’s crops are pollinated by insects, and bees are often assumed to be the most important pollinators. To our knowledge, our study is the first quantitative evaluation of the relative contribution of non-bee pollinators to global pollinator-dependent crops. Across 39 studies we show that insects other than bees are efficient pollinators providing 39% of visits to crop flowers. A shift in perspective from a bee-only focus is needed for assessments of crop pollinator biodiversity and the economic value of pollination. These studies should also consider the services provided by other types of insects, such as flies, wasps, beetles, and butterflies—important pollinators that are currently overlooked.",Plant and animal studies,"Ecology, Evolution, Behavior and Systematics",4025.0
7469,W2015628143,Building Statistical Models To Analyze Species Distributions,"Andrew M. Latimer, Shanshan Wu, Alan E. Gelfand, John A. Silander","Models of the geographic distributions of species have wide application in ecology. But the nonspatial, single-level, regression models that ecologists have often employed do not deal with problems of irregular sampling intensity or spatial dependence, and do not adequately quantify uncertainty. We show here how to build statistical models that can handle these features of spatial prediction and provide richer, more powerful inference about species niche relations, distributions, and the effects of human disturbance. We begin with a familiar generalized linear model and build in additional features, including spatial random effects and hierarchical levels. Since these models are fully specified statistical models, we show that it is possible to add complexity without sacrificing interpretability. This step-by-step approach, together with attached code that implements a simple, spatially explicit, regression model, is structured to facilitate self-teaching. All models are developed in a Bayesian framework. We assess the performance of the models by using them to predict the distributions of two plant species (Proteaceae) from South Africa's Cape Floristic Region. We demonstrate that making distribution models spatially explicit can be essential for accurately characterizing the environmental response of species, predicting their probability of occurrence, and assessing uncertainty in the model results. Adding hierarchical levels to the models has further advantages in allowing human transformation of the landscape to be taken into account, as well as additional features of the sampling process.",Species Distribution and Climate Change,Ecological Modeling,4131.0
7470,W2002900408,Resource Selection by Animals : Statistical design and analysis for field studies,"Bryan F. J. Manly, Lyman L. McDonald, Dana L. Thomas",Preface. List of Symbols. 1. Introduction to Resource Selection Studies. 2. Statistical Modelling Procedures. 3. Examples of the Use of Resource Selection Functions. 4. Studies with Resources Defined by Several Categories. 5. Resource Selection Functions from Logistic Regression. 6. Resource Selection over Several Time Periods. 7. Log-Linear Modelling. 8. Discrete Choice Models with Changing Availability. 9. Applications Using Geographic Information Systems. 10. Discriminant Function Analysis. 11. Analysis of the Amount of Use. 12. Some Other Types of Analysis. 13. Risk Assessment and Population Size Estimation. 14. Computing. References. Name Index. Subject Index.,Animal Ecology and Behavior Studies,Ecology,4158.0
7471,W2012735201,Modelling distribution and abundance with presence‐only data,"Jennie Pearce, Mark S. Boyce","Summary Presence‐only data, for which there is no information on locations where the species is absent, are common in both animal and plant studies. In many situations, these may be the only data available on a species. We need effective ways to use these data to explore species distribution or species use of habitat. Many analytical approaches have been used to model presence‐only data, some inappropriately. We provide a synthesis and critique of statistical methods currently in use to both estimate and evaluate these models, and discuss the critical importance of study design in models where only presence can be identified Profile or envelope methods exist to characterize environmental covariates that describe the locations where organisms are found. Predictions from profile approaches are generally coarse, but may be useful when species records, environmental predictors and biological understanding are scarce. Alternatively, one can build models to contrast environmental attributes associated with known locations with a sample of random landscape locations, termed either ‘pseudo‐absences’ or ‘available’. Great care needs to be taken when selecting random landscape locations, because the way in which they are selected determines the modelling techniques that can be applied. Regression‐based models can provide predictions of the relative likelihood of occurrence, and in some situations predictions of the probability of occurrence. The logistic model is frequently applied, but can rarely be used directly to estimate these models; instead, case–control or logistic discrimination should be used depending on the sample design. Cross‐validation can be used to evaluate model performance and to assess how effectively the model reflects a quantity proportional to the probability of occurrence. However, more research is needed to develop a single measure or statistic that summarizes model performance for presence‐only data. Synthesis and applications. A number of statistical procedures are available to explore patterns in presence‐only data; the choice among them depends on the quality of the presence‐only data. Presence‐only records can provide insight into the vulnerability, historical distribution and conservation status of species. Models developed using these data can inform management. Our caveat is that researchers must be mindful of study design and the biases inherent in presence data, and be cautious in the interpretation of model predictions.",Species Distribution and Climate Change,Ecological Modeling,4160.0
7472,W2168113371,Where is positional uncertainty a problem for species distribution modelling?,"Babak Naimi, Nicholas Hamm, T.A. Groen, Andrew K. Skidmore, Albertus G. Toxopeus","Species data held in museum and herbaria, survey data and opportunistically observed data are a substantial information resource. A key challenge in using these data is the uncertainty about where an observation is located. This is important when the data are used for species distribution modelling (SDM), because the coordinates are used to extract the environmental variables and thus, positional error may lead to inaccurate estimation of the species–environment relationship. The magnitude of this effect is related to the level of spatial autocorrelation in the environmental variables. Using local spatial association can be relevant because it can lead to the identification of the specific occurrence records that cause the largest drop in SDM accuracy. Therefore, in this study, we tested whether the SDM predictions are more affected by positional uncertainty originating from locations that have lower local spatial association in their predictors. We performed this experiment for Spain and the Netherlands, using simulated datasets derived from well known species distribution models (SDMs). We used the K statistic to quantify the local spatial association in the predictors at each species occurrence location. A probabilistic approach using Monte Carlo simulations was employed to introduce the error in the species locations. The results revealed that positional uncertainty in species occurrence data at locations with low local spatial association in predictors reduced the prediction accuracy of the SDMs. We propose that local spatial association is a way to identify the species occurrence records that require treatment for positional uncertainty. We also developed and present a tool in the R environment to target observations that are likely to create error in the output from SDMs as a result of positional uncertainty.",Species Distribution and Climate Change,Ecological Modeling,4212.0
7473,W2109735604,The importance of correcting for sampling bias in MaxEnt species distribution models,"Stephanie Kramer‐Schadt, Jürgen Niedballa, John D. Pilgrim, Boris Schröder, Jana Lindenborn, Vanessa Reinfelder, Milena Stillfried, Ilja Heckmann, Anne K. Scharf, Dave M. Augeri, Susan M. Cheyne, Andrew J. Hearn, Joanna Ross, David W. Macdonald, John Mathai, James A. Eaton, Andrew J. Marshall, Gono Semiadi, Rustam Rustam, Henry Bernard, Raymond Alfred, Hiromitsu Samejima, J. W. Duckworth, Christine Breitenmoser‐Würsten, Jerrold L. Belant, Heribert Hofer, Andreas Wilting","Abstract Aim Advancement in ecological methods predicting species distributions is a crucial precondition for deriving sound management actions. Maximum entropy (MaxEnt) models are a popular tool to predict species distributions, as they are considered able to cope well with sparse, irregularly sampled data and minor location errors. Although a fundamental assumption of MaxEnt is that the entire area of interest has been systematically sampled, in practice, MaxEnt models are usually built from occurrence records that are spatially biased towards better‐surveyed areas. Two common, yet not compared, strategies to cope with uneven sampling effort are spatial filtering of occurrence data and background manipulation using environmental data with the same spatial bias as occurrence data. We tested these strategies using simulated data and a recently collated dataset on Malay civet V iverra tangalunga in Borneo. Location Borneo, Southeast Asia. Methods We collated 504 occurrence records of Malay civets from Borneo of which 291 records were from 2001 to 2011 and used them in the MaxEnt analysis (baseline scenario) together with 25 environmental input variables. We simulated datasets for two virtual species (similar to a range‐restricted highland and a lowland species) using the same number of records for model building. As occurrence records were biased towards north‐eastern Borneo, we investigated the efficacy of spatial filtering versus background manipulation to reduce overprediction or underprediction in specific areas. Results Spatial filtering minimized omission errors (false negatives) and commission errors (false positives). We recommend that when sample size is insufficient to allow spatial filtering, manipulation of the background dataset is preferable to not correcting for sampling bias, although predictions were comparatively weak and commission errors increased. Main Conclusions We conclude that a substantial improvement in the quality of model predictions can be achieved if uneven sampling effort is taken into account, thereby improving the efficacy of species conservation planning.",Species Distribution and Climate Change,Ecological Modeling,4214.0
7474,W1599304806,Why do we still use stepwise modelling in ecology and behaviour?,"Mark J. Whittingham, Philip A. Stephens, Richard B. Bradbury, Robert P. Freckleton","Summary The biases and shortcomings of stepwise multiple regression are well established within the statistical literature. However, an examination of papers published in 2004 by three leading ecological and behavioural journals suggested that the use of this technique remains widespread: of 65 papers in which a multiple regression approach was used, 57% of studies used a stepwise procedure. The principal drawbacks of stepwise multiple regression include bias in parameter estimation, inconsistencies among model selection algorithms, an inherent (but often overlooked) problem of multiple hypothesis testing, and an inappropriate focus or reliance on a single best model. We discuss each of these issues with examples. We use a worked example of data on yellowhammer distribution collected over 4 years to highlight the pitfalls of stepwise regression. We show that stepwise regression allows models containing significant predictors to be obtained from each year's data. In spite of the significance of the selected models, they vary substantially between years and suggest patterns that are at odds with those determined by analysing the full, 4‐year data set. An information theoretic (IT) analysis of the yellowhammer data set illustrates why the varying outcomes of stepwise analyses arise. In particular, the IT approach identifies large numbers of competing models that could describe the data equally well, showing that no one model should be relied upon for inference.",Ecology and Vegetation Dynamics Studies,Nature and Landscape Conservation,4223.0
7475,W2035983506,Generalized Linear Models (2nd ed.).,"Terry M. Therneau, Peter McCullagh, J. A. Nelder","Addresses a class of statistical models that generalizes classical linear models-extending them to include many other models useful in statistical analysis. Incorporates numerous exercises, both theoretical and data-analytic Discusses quasi-likelihood functions and estimating equations, models for dispersion effect, components of dispersion, and conditional likelihoods Holds particular interest for statisticians in medicine, biology, agriculture, social science, and engineering",,,4288.0
7476,W1844182505,Towards a hierarchical framework for modelling the spatial distribution of animals,"Brendan Mackey, David B. Lindenmayer","Aim A hierarchical framework is presented for modelling the spatial distribution of terrestrial vertebrate animals. Location The location of the study is the montane ash forests of the Central Highlands of Victoria, south‐eastern Australia. Methods The framework is illustrated using as a case study the distribution of Leadbeater’s Possum [ Gymnobelideus leadbeateri McCoy, 1867, ( Marsupialia: Petauridae )], a small arboreal marsupial. The framework is based upon quantifying the environmental response of a species in terms of a five‐level environmental hierarchy defined by scales (global‐, meso‐, topo‐, micro‐ and nano‐scales) that represent natural breaks in the distribution and availability of the primary environmental resources. Animal response is examined in terms of a species’ distribution as observed in four biological units (the species in toto , meta‐population/population, group/colony, and individual organism). We define the spatial occurrence and abundance of the target species in each of these units as its ‘distributional behaviour’. Results Predictions of the potential spatial distribution of Leadbeater’s Possum are presented at meso‐, topo‐, micro‐ and nano‐scales. These spatial predictions utilize Geographical Information System (GIS)‐based spatial models of long term mean monthly climate and terrain‐modified surface radiation, together with vegetation cover and individual tree attributes from air‐photo interpretation and field survey. Main conclusions Ideally, species’ responses at each level in the environmental hierarchy should be empirically derived using statistical models based on field observation of a species’ distribution and abundance. Spatial modelling of species’ responses becomes problematic at finer scales because of the lack of suitable environmental data. The key characteristics of the modelling framework are generic, but the influence of additional scales and processes will be important in other ecosystems and species.",Wildlife Ecology and Conservation,Ecology,4379.0
7477,W1967360417,Grinnellian and Eltonian niches and geographic distributions of species,Jorge Soberón,"In the recent past, availability of large data sets of species presences has increased by orders of magnitude. This, together with developments in geographical information systems and statistical methods, has enabled scientists to calculate, for thousands of species, the environmental conditions of their distributional areas. The profiles thus obtained are obviously related to niche concepts in the Grinnell tradition, and separated from those in Elton's tradition. I argue that it is useful to define Grinnellian and Eltonian niches on the basis of the types of variables used to calculate them, the natural spatial scale at which they can be measured, and the dispersal of the individuals over the environment. I use set theory notation and analogies derived from population ecology theory to obtain formal definitions of areas of distribution and several types of niches. This brings clarity to several practical and fundamental questions in macroecology and biogeography.",Plant and animal studies,"Ecology, Evolution, Behavior and Systematics",4381.0
7478,W1979467831,Constraints to projecting the effects of climate change on mammals,"Dominique Berteaux, MM Humphries, CJ Krebs, Maurício Lima, Andrew G. McAdam, Nathalie Pettorelli, Denis Réale, Takashi Saitoh, Emil Tkadlec, RB Weladji, Nils Chr. Stenseth","CR Climate Research Contact the journal Facebook Twitter RSS Mailing List Subscribe to our mailing list via Mailchimp HomeLatest VolumeAbout the JournalEditorsSpecials CR 32:151-158 (2006) - doi:10.3354/cr032151 Constraints to projecting the effects of climate change on mammals D. Berteaux1,*, M. M. Humphries2, C. J. Krebs3, M. Lima4, A. G. McAdam5, N. Pettorelli6, D. Réale7, T. Saitoh8, E. Tkadlec9, R. B. Weladji6, N. Chr. Stenseth10 1Canada Research Chair in Conservation of Northern Ecosystems and Centre d'études nordiques, Université du Québec à Rimouski, 300 allée des Ursulines, Rimouski, Québec G5L 381, Canada 2Department of Natural Resource Sciences, Macdonald Campus, McGill University, 21-111 Lakeshore Road, Ste-Anne-de-Bellevue, Québec H9X 3V9, Canada 3Department of Zoology, University of British Columbia, 6270 University Boulevard, Vancouver, British Columbia V6T 1Z4, Canada 4Center for Advanced Studies in Ecology and Biodiversity, Pontificia Universidad Catolica de Chile, Casilla 114-D, Santiago CP 6513677, Chile 5Department of Fisheries & Wildlife, Department of Zoology, 13 Natural Resources Building, Michigan State University, East Lansing, Michigan 48824, USA 6Département de Biologie, Université Laval, Québec G1K 7P4, Canada 7Canada Research Chair in Behavioural Ecology, Université du Québec à Montréal, CP 8888-succursalle centre-ville, Montréal, Québec H3C 3P8, Canada 8Field Science Center, Hokkaido University, North 11, West 10, Sapporo 060-0811, Japan 9Palacky University, Department of Ecology and Environmental Sciences, Tr. Svobody 26, 771 46 Olomouc and Institute of Vertebrate Biology, Academy of Sciences of the Czech Republic, 675 02 Studenec, Czech Republic 10Centre for Ecological and Evolutionary Synthesis (CEES), Department of Biology, University of Oslo, PO Box 1066, Blindern, 0316 Oslo, Norway *Email: dominique_berteaux@uqar.qc.ca ABSTRACT: Ecologists are under pressure to anticipate the ecological effects of climate change. Therefore many ecological publications (and most grant proposals) related to climate claim relevance to the projection of future climate change effects. Yet the steps leading from ecological description and understanding to reliable projection are rarely explicit. A good understanding of the factors which allow the ecological effects of climate change to be effectively anticipated is critical to both the quality of basic science and its application to public policy. We used research performed on mammals to explore scientific approaches to anticipation of climate change effects. We distinguished forecasting models based on correlations from predictive models based on cause-effect relationships. These categories represent extremes along a continuous gradient between pattern description and causal understanding. We suggest that the constraints to our capacity to anticipate fall into 6 broad categories rooted in the development and application of forecasting and predictive models. These categories help to identify the conditions that allow or prevent projection of the effects of climate change on ecosystems. This approach should also help to identify which research avenues will likely be most fruitful. KEY WORDS: Cause-effect relationships · Climate change · Ecology · Forecasting · Mammals · Models · Prediction · Scientific projection · Time series Full text in pdf format PreviousExport citation RSS - Facebook - Tweet - linkedIn Cited by Published in CR Vol. 32, No. 2. Online publication date: October 13, 2006 Print ISSN: 0936-577X; Online ISSN: 1616-1572 Copyright © 2006 Inter-Research.",Species Distribution and Climate Change,Ecological Modeling,4383.0
7479,W1983436597,Modeling and Predicting Species Occurrence Using Broad‐Scale Environmental Variables: an Example with Butterflies of the Great Basin,"Erica Fleishman, Ralph Mac Nally, John P. Fay, Dennis D. Murphy","Abstract: If occurrence of individual species can be modeled as a function of easily quantified environmental variables (e.g., derived from a geographic information system [GIS]) and the predictions of these models are demonstrably successful, then the scientific foundation for management planning will be strengthened. We used Bayesian logistic regression to develop predictive models for resident butterflies in the central Great Basin of western North America. Species inventory data and values for 14 environmental variables from 49 locations (segments of canyons) in the Toquima Range ( Nevada, U.S.A.) were used to build the models. Squares of the environmental variables were also used to accommodate possibly nonmonotonic responses. We obtained statistically significant models for 36 of 56 (64%) resident species of butterflies. The models explained 8–72% of the deviance in occurrence of those species. Each of the independent variables was significant in at least one model, and squared versions of five variables contributed to models. Elevation was included in more than half of the models. Models included one to four variables; only one variable was significant in about half the models. We conducted preliminary tests of two of our models by using an existing set of data on the occurrence of butterflies in the neighboring Toiyabe Range. We compared conventional logistic classification with posterior probability distributions derived from Bayesian modeling. For the latter, we restricted our predictions to locations with a high ( 70%) probability of predicted presence or absence. We will perform further tests after conducting inventories at new locations in the Toquima Range and nearby Shoshone Mountains, for which we have computed environmental variables by using remotely acquired topographic data, digital‐terrain and microclimatic models, and GIS computation.",Species Distribution and Climate Change,Ecological Modeling,4385.0
7480,W1989594892,Identifying the effects of oceanographic features and zooplankton on prespawning herring abundance using generalized additive models,"Christos D. Maravelias, David G. Reid","MEPS Marine Ecology Progress Series Contact the journal Facebook Twitter RSS Mailing List Subscribe to our mailing list via Mailchimp HomeLatest VolumeAbout the JournalEditorsTheme Sections MEPS 147:1-9 (1997) - doi:10.3354/meps147001 Identifying the effects of oceanographic features and zooplankton on prespawning herring abundance using generalized additive models Maravelias CD, Reid DG Spatial distribution patterns of prespawning herring were analyzed in relation to zooplankton biomass, sea-surface temperature and salinity, temperature and depth of the thermocline (fronts) and the temperature difference between surface and bottom water. Data were collected in mid-summer 1995 during the ICES coordinated herring acoustic survey of the ICES Division IVa. We used generalized additive models (GAMs), nonparametric generalization of multiple linear regression, to test the hypothesis that prespawning herring distribution is related to zooplankton availability and the oceanography around the Shetland Islands (UK), with particular reference to inflows from the Slope Current. The results of this study supported this hypothesis. We found that zooplankton biomass and the location of ocean fronts influence the distribution of prespawning herring. Mean herring abundance was consistently highest in areas having a surface salinity of 35.1 ppt and where the zooplankton abundances were higher. Results indicated that herring appeared to prefer the well-mixed waters and transition zones and avoided the stratified and frontal areas. The present results also suggested that prespawning aggregations of herring followed the movements of zooplankton to deeper and cooler waters beneath the thermocline during summer. Waters with specific salinity and temperature properties are attractive to herring due to the process of frontal mixing which enhances primary and secondary production. These waters are ultimately linked with the Slope Current which is responsible for the advection of warm, nutrient-rich, saline water into the North Sea ecosystem. Herring · Oceanography · Zooplankton · North Sea · Generalized additive models Full text in pdf format NextExport citation RSS - Facebook - Tweet - linkedIn Cited by Published in MEPS Vol. 147. Publication date: February 27, 1997 Print ISSN:0171-8630; Online ISSN:1616-1599 Copyright © 1997 Inter-Research.",Marine and fisheries research,Global and Planetary Change,4386.0
7481,W1996132275,"SPATIOTEMPORAL PREDICTION MODELS OF CETACEAN HABITATS IN THE MID‐WESTERN NORTH ATLANTIC OCEAN (FROM CAPE HATTERAS, NORTH CAROLINA, U.S.A. TO NOVA SCOTIA, CANADA)",Toshihide Hamazaki,"A bstract Habitat prediction models were developed for 13 cetacean species of the mid‐western North Atlantic Ocean: beaked whale, fin whale, humpback whale, minke whale, pilot whale, sperm whale, bottlenose dolphin, common dolphin, Risso's dolphin, spotted dolphin, whitesided dolphin, and harbor porpoise. Using the multiple logistic regression, sightings of cetaceans during the 1990–1996 summer (June‐September) surveys were modeled with oceanographic (sea surface temperature, monthly probability of front occurrence) and topographic (depth, slope) variables for the same period. Predicted habitat maps for June and August were created for each species using a Geographical Information System. The predicted habitat locations matched with current and historic cetacean sighting locations. The model also predicted habitat shifts for some species associated with oceanographic changes. The correct classification rate of the prediction models with 1997–1998 summer survey data ranged from 44% to 70%, of which most of the misclassifications were caused by false positives ( i.e. , absence of sightings at locations where the models predicted).",Marine animal studies overview,Ecology,4387.0
7482,W2020867281,"Climate change and the origin of migratory pathways in the Swainson's thrush,<i>Catharus ustulatus</i>","Kristen Ruegg, Robert J. Hijmans, Craig Moritz","Abstract Aim To provide a spatially explicit model of how geographic distributions at the last glacial maximum (LGM) and post‐glacial colonization routes shaped current migratory pathways in the Swainson's thrush, Catharus ustulatus , a long‐distance migratory bird. Location The Swainson's thrush breeds in boreal forest regions of the United States and Canada as well as in riparian woodlands along the Pacific coast of North America. Methods Palaeodistribution modelling is combined with mtDNA phylogeography to predict the breeding range of the Swainson's thrush at the LGM. Quantitative environmental analysis and bioclimatic modelling are used to reconstruct the most likely post‐glacial colonization pathways. A maximum likelihood method for estimating growth rates is used to approximate the relative change in population size since the LGM. Main conclusions The palaeodistribution models are concordant with the Swainson's thrush mtDNA phylogeography, suggesting that the inland and coastal groups were geographically isolated in eastern (inland) and western (coastal) regions at the LGM. Estimates of change in population size based on genetic data are remarkably consistent with estimates of change in range size, suggesting that the coastal group has undergone a 2‐ to 3‐fold demographic and range expansion, while the inland group has undergone a 6‐ to 12‐fold demographic and range expansion since the LGM. Bioclimatic analyses strongly support the hypothesis that populations expanding out of the east into previously glaciated areas in the west were undergoing a natural extension of their range by tracking the changes in climatic conditions. The combination of bioclimatic and molecular analyses is consistent with the idea that coastal and inland groups expanded from separate eastern and western regions after the LGM and that the current migratory pathway of the inland group retraces its post‐glacial colonization route.",Species Distribution and Climate Change,Ecological Modeling,4390.0
7483,W2100533358,The art of modelling range-shifting species,"Jane Elith, Michael Kearney, Steven Phillips","1. Species are shifting their ranges at an unprecedented rate through human transportation and environmental change. Correlative species distribution models (SDMs) are frequently applied for predicting potential future distributions of range-shifting species, despite these models' assumptions that species are at equilibrium with the environments used to train (fit) the models, and that the training data are representative of conditions to which the models are predicted. Here we explore modelling approaches that aim to minimize extrapolation errors and assess predictions against prior biological knowledge. Our aim was to promote methods appropriate to range-shifting species. 2. We use an invasive species, the cane toad in Australia, as an example, predicting potential distributions under both current and climate change scenarios. We use four SDM methods, and trial weighting schemes and choice of background samples appropriate for species in a state of spread. We also test two methods for including information from a mechanistic model. Throughout, we explore graphical techniques for understanding model behaviour and reliability, including the extent of extrapolation. 3. Predictions varied with modelling method and data treatment, particularly with regard to the use and treatment of absence data. Models that performed similarly under current climatic conditions deviated widely when transferred to a novel climatic scenario. 4. The results highlight problems with using SDMs for extrapolation, and demonstrate the need for methods and tools to understand models and predictions. We have made progress in this direction and have implemented exploratory techniques as new options in the free modelling software, MaxEnt. Our results also show that deliberately controlling the fit of models and integrating information from mechanistic models can enhance the reliability of correlative predictions of species in non-equilibrium and novel settings. 5. Implications. The biodiversity of many regions in the world is experiencing novel threats created by species invasions and climate change. Predictions of future species distributions are required for management, but there are acknowledged problems with many current methods, and relatively few advances in techniques for understanding or overcoming these. The methods presented in this manuscript and made accessible in MaxEnt provide a forward step.",Species Distribution and Climate Change,Ecological Modeling,4392.0
7484,W2104024599,The global distribution of the arbovirus vectors Aedes aegypti and Ae. albopictus,"Moritz U. G. Kraemer, Marianne Sinka, Kirsten A. Duda, Adrian Mylne, Freya M. Shearer, Christopher M. Barker, Chester G. Moore, Roberta Gomes Carvalho, Giovanini Evelim Coelho, Wim Van Bortel, Guy Hendrickx, Francis Schaffner, Iqbal Elyazar, Hwa‐Jen Teng, Oliver J. Brady, Jane P. Messina, David M. Pigott, Thomas W. Scott, David L. Smith, GR William Wint, Nick Golding, Simon I Hay","Dengue and chikungunya are increasing global public health concerns due to their rapid geographical spread and increasing disease burden. Knowledge of the contemporary distribution of their shared vectors, Aedes aegypti and Aedes albopictus remains incomplete and is complicated by an ongoing range expansion fuelled by increased global trade and travel. Mapping the global distribution of these vectors and the geographical determinants of their ranges is essential for public health planning. Here we compile the largest contemporary database for both species and pair it with relevant environmental variables predicting their global distribution. We show Aedes distributions to be the widest ever recorded; now extensive in all continents, including North America and Europe. These maps will help define the spatial limits of current autochthonous transmission of dengue and chikungunya viruses. It is only with this kind of rigorous entomological baseline that we can hope to project future health impacts of these viruses.",Mosquito-borne diseases and control,"Public Health, Environmental and Occupational Health",4393.0
7485,W2127367934,Predicting species distributions for conservation decisions,"Antoine Guisan, Reid Tingley, John B. Baumgartner, Ilona Naujokaitis‐Lewis, Patricia Sutcliffe, Ayesha Tulloch, Tracey J. Regan, Lluı́s Brotons, Eve McDonald‐Madden, Chrystal Mantyka‐Pringle, Tara G. Martin, Jonathan R. Rhodes, Ramona Maggini, Samantha A. Setterfield, Jane Elith, Mark W. Schwartz, Brendan A. Wintle, Olivier Broennimann, Mike P. Austin, Simon Ferrier, Michael Kearney, Hugh P. Possingham, Yvonne M. Buckley","Species distribution models (SDMs) are increasingly proposed to support conservation decision making. However, evidence of SDMs supporting solutions for on-ground conservation problems is still scarce in the scientific literature. Here, we show that successful examples exist but are still largely hidden in the grey literature, and thus less accessible for analysis and learning. Furthermore, the decision framework within which SDMs are used is rarely made explicit. Using case studies from biological invasions, identification of critical habitats, reserve selection and translocation of endangered species, we propose that SDMs may be tailored to suit a range of decision-making contexts when used within a structured and transparent decision-making process. To construct appropriate SDMs to more effectively guide conservation actions, modellers need to better understand the decision process, and decision makers need to provide feedback to modellers regarding the actual use of SDMs to support conservation decisions. This could be facilitated by individuals or institutions playing the role of 'translators' between modellers and decision makers. We encourage species distribution modellers to get involved in real decision-making processes that will benefit from their technical input; this strategy has the potential to better bridge theory and practice, and contribute to improve both scientific knowledge and conservation outcomes.",Species Distribution and Climate Change,Ecological Modeling,4394.0
7486,W1969126720,spThin: an R package for spatial thinning of species occurrence records for use in ecological niche models,"Matthew E. Aiello‐Lammens, Robert A. Boria, Aleksandar Radosavljević, Bruno Vilela, Robert P. Anderson","Spatial thinning of species occurrence records can help address problems associated with spatial sampling biases. Ideally, thinning removes the fewest records necessary to substantially reduce the effects of sampling bias, while simultaneously retaining the greatest amount of useful information. Spatial thinning can be done manually; however, this is prohibitively time consuming for large datasets. Using a randomization approach, the ‘thin’ function in the spThin R package returns a dataset with the maximum number of records for a given thinning distance, when run for sufficient iterations. We here provide a worked example for the Caribbean spiny pocket mouse, where the results obtained match those of manual thinning.",Species Distribution and Climate Change,Ecological Modeling,4395.0
7487,W1530451520,Refugia: identifying and understanding safe havens for biodiversity under climate change,"Gunnar Keppel, Kimberly P. Van Niel, Grant Wardell‐Johnson, Colin J. Yates, Margaret Byrne, Ladislav Mucina, A.G.T. Schut, Stephen D. Hopper, Steven E. Franklin","Aim Identifying and protecting refugia is a priority for conservation under projected anthropogenic climate change, because of their demonstrated ability to facilitate the survival of biota under adverse conditions. Refugia are habitats that components of biodiversity retreat to, persist in and can potentially expand from under changing environmental conditions. However, the study and discussion of refugia has often been ad hoc and descriptive in nature. We therefore: (1) provide a habitat-based concept of refugia, and (2) evaluate methods for the identification of refugia. Location Global. Methods We present a simple conceptual framework for refugia and examine the factors that describe them. We then demonstrate how different disciplines are contributing to our understanding of refugia, and the tools that they provide for identifying and quantifying refugia. Results Current understanding of refugia is largely based on Quaternary phylogeographic studies on organisms in North America and Europe during significant temperature fluctuations. This has resulted in gaps in our understanding of refugia, particularly when attempting to apply current theory to forecast anthropogenic climate change. Refugia are environmental habitats with space and time dimensions that operate on evolutionary time-scales and have facilitated the survival of biota under changing environmental conditions for millennia. Therefore, they offer the best chances for survival under climate change for many taxa, making their identification important for conservation under anthropogenic climate change. Several methods from various disciplines provide viable options for achieving this goal. Main conclusions The framework developed for refugia allows the identification and description of refugia in any environment. Various methods provide important contributions but each is limited in scope; urging a more integrated approach to identify, define and conserve refugia. Such an approach will facilitate better understanding of refugia and their capacity to act as safe havens under projected anthropogenic climate change.",Species Distribution and Climate Change,Ecological Modeling,4397.0
7488,W2052583540,Mapping Species Distributions with MAXENT Using a Geographically Biased Sample of Presence Data: A Performance Assessment of Methods for Correcting Sampling Bias,"Yoan Fourcade, Jan O. Engler, Dennis Rödder, Jean Secondi","MAXENT is now a common species distribution modeling (SDM) tool used by conservation practitioners for predicting the distribution of a species from a set of records and environmental predictors. However, datasets of species occurrence used to train the model are often biased in the geographical space because of unequal sampling effort across the study area. This bias may be a source of strong inaccuracy in the resulting model and could lead to incorrect predictions. Although a number of sampling bias correction methods have been proposed, there is no consensual guideline to account for it. We compared here the performance of five methods of bias correction on three datasets of species occurrence: one ""virtual"" derived from a land cover map, and two actual datasets for a turtle (Chrysemys picta) and a salamander (Plethodon cylindraceus). We subjected these datasets to four types of sampling biases corresponding to potential types of empirical biases. We applied five correction methods to the biased samples and compared the outputs of distribution models to unbiased datasets to assess the overall correction performance of each method. The results revealed that the ability of methods to correct the initial sampling bias varied greatly depending on bias type, bias intensity and species. However, the simple systematic sampling of records consistently ranked among the best performing across the range of conditions tested, whereas other methods performed more poorly in most cases. The strong effect of initial conditions on correction performance highlights the need for further research to develop a step-by-step guideline to account for sampling bias. However, this method seems to be the most efficient in correcting sampling bias and should be advised in most cases.",Species Distribution and Climate Change,Ecological Modeling,4398.0
7489,W191383808,"Generalized Linear Models, 2nd Edn.","P. J. Cheek, P. McCullagh, J. A. Nelder","Generalized Linear Models, 2nd edn. By P. McCullagh and J. A. Nelder. ISBN 0 412 31760 5. Chapman and Hall, London, 1989. xx + 512 pp. £30.",Soil Geostatistics and Mapping,Environmental Engineering,4406.0
7490,W2100226004,Uses and misuses of bioclimatic envelope modeling,"Miguel B. Araújo, A. Townsend Peterson","Bioclimatic envelope models use associations between aspects of climate and species' occurrences to estimate the conditions that are suitable to maintain viable populations. Once bioclimatic envelopes are characterized, they can be applied to a variety of questions in ecology, evolution, and conservation. However, some have questioned the usefulness of these models, because they may be based on implausible assumptions or may be contradicted by empirical evidence. We review these areas of contention, and suggest that criticism has often been misplaced, resulting from confusion between what the models actually deliver and what users wish that they would express. Although improvements in data and methods will have some effect, the usefulness of these models is contingent on their appropriate use, and they will improve mainly via better awareness of their conceptual basis, strengths, and limitations.",Species Distribution and Climate Change,Ecological Modeling,4613.0
7493,W1599871777,Machine Learning Benchmarks and Random Forest Regression,Mark R. Segal,"Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called ‘random forests’. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.",Neural Networks and Applications,Artificial Intelligence,4824.0
7494,W1680392829,A study of cross-validation and bootstrap for accuracy estimation and model selection,Ron Kohavi,"We review accuracy estimation methods and compare the two most common methods crossvalidation and bootstrap. Recent experimental results on artificial data and theoretical re cults in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algrithms on real-world datasets. For crossvalidation we vary the number of folds and whether the folds are stratified or not, for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, The best method to use for model selection is ten fold stratified cross validation even if computation power allows using more folds.",Algorithms and Data Compression,Artificial Intelligence,4825.0
7495,W1932590463,One hundred years of eruptions of house mice in Australia - a natural biological curio,"Grant R. Singleton, Peter Brown, Roger P. Pech, Jens Jacob, Greg Mutze, Charles J. Krebs","The house mouse has adapted well to the cereal crops of south-eastern Australia where populations show aperiodic outbreaks over large areas. A 20-year population study has provided a wealth of information on breeding ecology, demographic changes, spatial behaviour and epidemiology. The breeding season can be as short as 4.5 months and as long as 10 months with litter size changing seasonally from high values in spring to low values in autumn. There are marked changes in litter size between years. Rates of increase of populations also vary between years. The rate of change of populations during the breeding season is independent of density effects, but if the population density is high at the commencement of breeding then the litter size is depressed throughout that breeding season. There are density-dependent effects on survival during the non-breeding season. Rates of increase of populations over spring and summer are highly correlated with accumulated rainfall from the previous winter–spring (April–October). Studies of helminths and viruses indicate that Australian mice carry only a subset of the helminths found in Europe. There have been no published studies on murine viruses in Europe. Perhaps a reduced diversity of diseases partially accounts for the ability of mice to increase rapidly to extreme population densities in cereal-growing areas of south-eastern Australia.",Animal Ecology and Behavior Studies,Ecology,4826.0
7496,W1963998078,Scaling of metabolic rate with body mass and temperature in teleost fish,"Andrew Clarke, Nadine M. Johnston","Summary 1. We examined published studies relating resting oxygen consumption to body mass and temperature in post‐larval teleost fish. The resulting database comprised 138 studies of 69 species (representing 28 families and 12 orders) living over a temperature range of c. 40 °C. 2. Resting metabolic rate ( R b ; mmol oxygen gas h –1 ) was related to body mass ( M; wet mass, g) by R b = a M b , where a is a constant and b the scaling exponent. The model was fitted by least squares linear regression after logarithmic transformation of both variables. The mean value of scaling exponent, b, for the 69 individual species was 0·79 (SE 0·11). The general equation for all teleost fish was 1n R b = 0·80(1nM) – 5·43. 3. The relationship between resting oxygen consumption and environmental temperature for a 50‐g fish was curvilinear. A typical tropical fish at 30°C requires approximately six times as much oxygen for resting metabolism as does a polar fish at 0°C. This relationship could be fitted by several statistical models, of which the Arrhenius model is probably the most appropriate. The Arrhenius model for the resting metabolism of 69 species of teleost fish, corrected to a standard body mass of 50 g, was 1n R b = 15·7 – 5·02. T –1 , where T is absolute temperature (10 3 × K). 4. The Arrhenius model fitted to all 69 species exhibited a lower thermal sensitivity of resting metabolism (mean Q 10 = 1·83 over the range 0–30 °C) than typical within‐species acclimation studies (median Q 10 = 2·40, n = 14). This suggests that evolutionary adaptation has reduced the overall thermal sensitivity of resting metabolism across species. Analysis of covariance indicated that the relationships between resting metabolic rate and temperature for various taxa (orders) showed similar slopes but significantly different mean rates. 5. Analysis of the data for perciform fish provided no support for metabolic cold adaptation (the hypothesis that polar fish show a resting metabolic rate higher than predicted from the overall rate/temperature relationship established for temperate and tropical species). 6. Taxonomic variation in mean resting metabolic rate showed no relationship to phylogeny, although the robustness of this conclusion is constrained by our limited knowledge of fish evolutionary history.",Physiological and biochemical adaptations,Ecology,4827.0
7497,W1998479140,Implications of diadromy for the structuring and modelling of riverine fish communities in New Zealand,R. M. McDowall,"Abstract Development of models to describe the distribution and abundance of salmonid fishes depends on understanding the suitability of local, small‐scale, critical habitat parameters such as water depth, flow velocity, and substrate characteristics. The success of such models in predicting trout abundance depends on the populations not being limited by additional constraints on abundance by wider, large‐scale parameters, such as recruitment and migratory access. Extension of these models for use in evaluating the impacts of flow modifications to New Zealand rivers on indigenous fish populations would significantly advance biologically sustainable management of rivers. Many of New Zealand's native fish species are diadromous, requiring access to and from the sea to complete their life cycles. For this reason, abundance of native fishes is likely to be constrained by a combination of large‐scale variables such as: elevation, slope, distance from the sea, the presence or absence of explicit barriers to upstream migration, and individual species' ""urge"" to migrate upstream. Successful predictive models for describing abundance of diadromous native fish will need to incorporate factors that account for these variables, Although models for estimating areas of available habitat can be based on habitat suitability criteria, when models are used for predicting abundance, they must take into account factors relating to accessibility.",Fish Ecology and Management Studies,Nature and Landscape Conservation,4828.0
7498,W2026257972,Variation in demersal fish species richness in the oceans surrounding New Zealand: an analysis using boosted regression trees,"JR Leathwick, Jane Elith, MP Francis, Trevor Hastie, Paul Taylor","MEPS Marine Ecology Progress Series Contact the journal Facebook Twitter RSS Mailing List Subscribe to our mailing list via Mailchimp HomeLatest VolumeAbout the JournalEditorsTheme Sections MEPS 321:267-281 (2006) - doi:10.3354/meps321267 Variation in demersal fish species richness in the oceans surrounding New Zealand: an analysis using boosted regression trees J. R. Leathwick1,*, J. Elith2, M. P. Francis3, T. Hastie4, P. Taylor1 1National Institute of Water and Atmospheric Research, PO Box 11115, Hamilton, New Zealand 2School of Botany, University of Melbourne, Parkville 3010, Victoria, Australia 3National Institute of Water and Atmospheric Research, Private Bag 14901, Wellington, New Zealand 4Sequoia Hall, Stanford University, Stanford, California 94305-4065, USA *Email: j.leathwick@niwa.co.nz ABSTRACT: We analysed relationships between demersal fish species richness, environment and trawl characteristics using an extensive collection of trawl data from the oceans around New Zealand. Analyses were carried out using both generalised additive models and boosted regression trees (sometimes referred to as stochastic gradient boosting). Depth was the single most important environmental predictor of variation in species richness, with highest richness occurring at depths of 900 to 1000 m, and with a broad plateau of moderately high richness between 400 and 1100 m. Richness was higher both in waters with high surface concentrations of chlorophyll a and in zones of mixing of water bodies of contrasting origins. Local variation in temperature was also important, with lower richness occurring in waters that were cooler than expected given their depth. Variables describing trawl length, trawl speed, and cod-end mesh size made a substantial contribution to analysis outcomes, even though functions fitted for trawl distance and cod-end mesh size were constrained to reflect the known performance of trawl gear. Species richness declined with increasing cod-end mesh size and increasing trawl speed, but increased with increasing trawl distance, reaching a plateau once trawl distances exceed about 3 nautical miles. Boosted regression trees provided a powerful analysis tool, giving substantially superior predictive performance to generalized additive models, despite the fitting of interaction terms in the latter. KEY WORDS: Demersal fish · Species richness · Boosted regression trees · Statistical model Full text in pdf format PreviousNextExport citation RSS - Facebook - Tweet - linkedIn Cited by Published in MEPS Vol. 321. Online publication date: September 08, 2006 Print ISSN: 0171-8630; Online ISSN: 1616-1599 Copyright © 2006 Inter-Research.",Marine and fisheries research,Global and Planetary Change,4829.0
7499,W2084341220,Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author),Leo Breiman,"There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.",Bayesian Modeling and Causal Inference,Artificial Intelligence,4831.0
7500,W2125847307,Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation,"Alex Goldstein, Adam Kapelner, Justin Bleich, Emil Pitkin","This article presents individual conditional expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features. In the presence of substantial interaction effects, the partial response relationship can be heterogeneous. Thus, an average curve, such as the PDP, can obfuscate the complexity of the modeled relationship. Accordingly, ICE plots refine the PDP by graphing the functional relationship between the predicted response and the feature for individual observations. Specifically, ICE plots highlight the variation in the fitted values across the range of a covariate, suggesting where and to what extent heterogeneities might exist. In addition to providing a plotting suite for exploratory analysis, we include a visual test for additive structure in the data-generating model. Through simulated examples and real datasets, we demonstrate how ICE plots can shed light on estimated models in ways PDPs cannot. Procedures outlined are available in the R package ICEbox.",Data Analysis with R,Artificial Intelligence,4832.0
7501,W2884859931,Drinking Water Nitrate and Human Health: An Updated Review,"Mary H. Ward, Rena R. Jones, Jean D. Brender, Theo de Kok, Peter J. Weyer, Bernard T. Nolan, Cristina M. Villanueva, Simone G. van Breda","Nitrate levels in our water resources have increased in many areas of the world largely due to applications of inorganic fertilizer and animal manure in agricultural areas. The regulatory limit for nitrate in public drinking water supplies was set to protect against infant methemoglobinemia, but other health effects were not considered. Risk of specific cancers and birth defects may be increased when nitrate is ingested under conditions that increase formation of N-nitroso compounds. We previously reviewed epidemiologic studies before 2005 of nitrate intake from drinking water and cancer, adverse reproductive outcomes and other health effects. Since that review, more than 30 epidemiologic studies have evaluated drinking water nitrate and these outcomes. The most common endpoints studied were colorectal cancer, bladder, and breast cancer (three studies each), and thyroid disease (four studies). Considering all studies, the strongest evidence for a relationship between drinking water nitrate ingestion and adverse health outcomes (besides methemoglobinemia) is for colorectal cancer, thyroid disease, and neural tube defects. Many studies observed increased risk with ingestion of water nitrate levels that were below regulatory limits. Future studies of these and other health outcomes should include improved exposure assessment and accurate characterization of individual factors that affect endogenous nitrosation.",Water Treatment and Disinfection,"Health, Toxicology and Mutagenesis",4833.0
